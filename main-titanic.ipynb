{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/competitions/titanic\n",
    "\n",
    "To read a file csv (exls, ... ), we can use Pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use data as matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Futrelle, Mrs. Jacques Heath (Lily May Peel)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Name'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['Survived'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([549.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 342.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(data['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x16be658ea70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sex = data['Sex']\n",
    "age = data['Age']\n",
    "survived = data['Survived']\n",
    "\n",
    "# plot the data as (sex, age, survived) point\n",
    "# color the point with survived = 1 as red, survived = 0 as blue\n",
    "\n",
    "plt.scatter(age, sex, c = survived, cmap = 'bwr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sex, Age seems to be the significant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset = ['Age', 'Sex'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Sandstrom, Miss. Marguerite Rut</td>\n",
       "      <td>female</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PP 9549</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>G6</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PassengerId  Survived  Pclass  \\\n",
       "0             1         0       3   \n",
       "1             2         1       1   \n",
       "2             3         1       3   \n",
       "3             4         1       1   \n",
       "4             5         0       3   \n",
       "6             7         0       1   \n",
       "7             8         0       3   \n",
       "8             9         1       3   \n",
       "9            10         1       2   \n",
       "10           11         1       3   \n",
       "\n",
       "                                                 Name     Sex   Age  SibSp  \\\n",
       "0                             Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1   Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                              Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3        Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                            Allen, Mr. William Henry    male  35.0      0   \n",
       "6                             McCarthy, Mr. Timothy J    male  54.0      0   \n",
       "7                      Palsson, Master. Gosta Leonard    male   2.0      3   \n",
       "8   Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
       "9                 Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
       "10                    Sandstrom, Miss. Marguerite Rut  female   4.0      1   \n",
       "\n",
       "    Parch            Ticket     Fare Cabin Embarked  \n",
       "0       0         A/5 21171   7.2500   NaN        S  \n",
       "1       0          PC 17599  71.2833   C85        C  \n",
       "2       0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3       0            113803  53.1000  C123        S  \n",
       "4       0            373450   8.0500   NaN        S  \n",
       "6       0             17463  51.8625   E46        S  \n",
       "7       1            349909  21.0750   NaN        S  \n",
       "8       2            347742  11.1333   NaN        S  \n",
       "9       0            237736  30.0708   NaN        C  \n",
       "10      1           PP 9549  16.7000    G6        S  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sex = data['Sex']\n",
    "sex = np.array([1 if x == 'male' else 0 for x in sex])\n",
    "age = np.array(data['Age'])\n",
    "survived = np.array(data['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1ac12bd93f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(age, sex, c = survived, cmap = 'bwr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    P(survive | x, y) = \\sigma(ax+by+c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(a,b,c) = \\sum_{i = 1}^n (-z_i.\\ln(\\sigma(ax_i + by_i + c)) - (1 - z_i).\\ln(1 - \\sigma(ax_i + by_i +c)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial L}{\\partial a} = \\sum_{i = 1}^n -x_i(z_i - \\sigma(ax_i + by_i + c))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\sum_{i = 1}^n -y_i(z_i - \\sigma(ax_i + by_i + c))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial c} = \\sum_{i = 1}^n - (z_i - \\sigma(ax_i + by_i + c))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction:\n",
    "    def __init__(self, x, y, z):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def value(self, a, b, c):\n",
    "        predictions = self.sigmoid(a * self.x + b * self.y + c)\n",
    "        loss_values = -self.z * np.log(predictions) - (1 - self.z) * np.log(1 - predictions)\n",
    "        return np.nansum(loss_values)\n",
    "    \n",
    "    def accuracy(self, a, b, c):\n",
    "        predictions = self.sigmoid(a * self.x + b * self.y + c)\n",
    "        predictions = np.array([1 if x > 0.5 else 0 for x in predictions])\n",
    "        return np.sum(predictions == self.z) / len(self.z)\n",
    "    \n",
    "    def derivative(self, a, b, c):\n",
    "        predictions = self.sigmoid(a * self.x + b * self.y + c)\n",
    "        common_factor = - (self.z - predictions)\n",
    "        return np.array([np.nansum(common_factor * self.x), np.nansum(common_factor * self.y), np.nansum(common_factor)])\n",
    "    \n",
    "func = LossFunction(sex, age, survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Current point: [-0.5     0.      0.0012]\n",
      "Gradient: [ 78.15368974 684.49439503  11.73198973]\n",
      "Loss value: 442.1822427036061\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1\n",
      "Current point: [-5.07815369e-01 -6.84494395e-02  2.68010266e-05]\n",
      "Gradient: [  -50.24965119 -6576.20321682  -204.49567069]\n",
      "Loss value: 706.3848616886082\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 2\n",
      "Current point: [-0.5027904   0.58917088  0.02047637]\n",
      "Gradient: [  352.03432947 12963.53121626   412.14376153]\n",
      "Loss value: inf\n",
      "Accuracy: 0.40336134453781514\n",
      "====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_22024\\3244202866.py:12: RuntimeWarning: divide by zero encountered in log\n",
      "  loss_values = -self.z * np.log(predictions) - (1 - self.z) * np.log(1 - predictions)\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_22024\\3244202866.py:12: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss_values = -self.z * np.log(predictions) - (1 - self.z) * np.log(1 - predictions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3\n",
      "Current point: [-0.53799384 -0.70718224 -0.02073801]\n",
      "Gradient: [  -89.69308072 -8208.92723674  -283.51200522]\n",
      "Loss value: 5876.18982589471\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 4\n",
      "Current point: [-0.52902453  0.11371048  0.00761319]\n",
      "Gradient: [  312.84908205 11938.79260893   353.58151512]\n",
      "Loss value: 1373.2243236242007\n",
      "Accuracy: 0.3963585434173669\n",
      "====================================\n",
      "Iteration 5\n",
      "Current point: [-0.56030944 -1.08016878 -0.02774496]\n",
      "Gradient: [  -90.82943285 -8214.51725566  -285.96525789]\n",
      "Loss value: 8943.246515148163\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 6\n",
      "Current point: [-0.55122649 -0.25871705  0.00085157]\n",
      "Gradient: [  -85.79324261 -8151.1399774   -273.99315324]\n",
      "Loss value: 2196.2704009217473\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 7\n",
      "Current point: [-0.54264717  0.55639695  0.02825088]\n",
      "Gradient: [  351.54653267 12961.24929384   411.43877359]\n",
      "Loss value: inf\n",
      "Accuracy: 0.39915966386554624\n",
      "====================================\n",
      "Iteration 8\n",
      "Current point: [-0.57780182 -0.73972798 -0.012893  ]\n",
      "Gradient: [  -89.89859192 -8209.7702052   -283.86002853]\n",
      "Loss value: 6144.7185483549165\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 9\n",
      "Current point: [-0.56881196  0.08124904  0.01549301]\n",
      "Gradient: [  281.54235503 10835.50282146   310.56583   ]\n",
      "Loss value: 991.9523738064797\n",
      "Accuracy: 0.3949579831932773\n",
      "====================================\n",
      "Iteration 10\n",
      "Current point: [-0.5969662  -1.00230124 -0.01556358]\n",
      "Gradient: [  -90.70386871 -8213.81521434  -285.62268698]\n",
      "Loss value: 8303.47461378903\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 11\n",
      "Current point: [-0.58789581 -0.18091972  0.01299869]\n",
      "Gradient: [  -82.64473824 -8043.78375101  -266.51241608]\n",
      "Loss value: 1565.1597013902206\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 12\n",
      "Current point: [-0.57963134  0.62345865  0.03964993]\n",
      "Gradient: [  352.08137658 12964.9437704    412.47311743]\n",
      "Loss value: inf\n",
      "Accuracy: 0.4005602240896359\n",
      "====================================\n",
      "Iteration 13\n",
      "Current point: [-0.61483948 -0.67303572 -0.00159738]\n",
      "Gradient: [  -89.71011201 -8208.15779546  -283.29648718]\n",
      "Loss value: 5597.365027463178\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 14\n",
      "Current point: [-0.60586846  0.14778005  0.02673227]\n",
      "Gradient: [  327.5212274  12431.19978057   375.14977605]\n",
      "Loss value: 1771.736638333274\n",
      "Accuracy: 0.3949579831932773\n",
      "====================================\n",
      "Iteration 15\n",
      "Current point: [-0.63862059 -1.09533992 -0.01078271]\n",
      "Gradient: [  -90.96871722 -8214.72760219  -286.11475907]\n",
      "Loss value: 9070.138344070978\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 16\n",
      "Current point: [-0.62952372 -0.27386716  0.01782877]\n",
      "Gradient: [  -86.46122099 -8160.69078629  -275.04294377]\n",
      "Loss value: 2321.919848499048\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 17\n",
      "Current point: [-0.62087759  0.54220192  0.04533306]\n",
      "Gradient: [  351.08047387 12959.72598276   410.91509283]\n",
      "Loss value: inf\n",
      "Accuracy: 0.39775910364145656\n",
      "====================================\n",
      "Iteration 18\n",
      "Current point: [-0.65598564 -0.75377068  0.00424156]\n",
      "Gradient: [  -90.09702461 -8210.20336239  -284.08501008]\n",
      "Loss value: 6262.179654571802\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 19\n",
      "Current point: [-0.64697594  0.06724965  0.03265006]\n",
      "Gradient: [ 256.84517868 9933.89821956  278.35192373]\n",
      "Loss value: 830.1655492800489\n",
      "Accuracy: 0.39775910364145656\n",
      "====================================\n",
      "Iteration 20\n",
      "Current point: [-0.67266046 -0.92614017  0.00481486]\n",
      "Gradient: [  -90.62974027 -8213.0285964   -285.28657321]\n",
      "Loss value: 7678.976325851273\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 21\n",
      "Current point: [-0.66359748 -0.10483731  0.03334352]\n",
      "Gradient: [  -71.55405327 -7529.21342999  -241.93081543]\n",
      "Loss value: 968.0697879512473\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 22\n",
      "Current point: [-0.65644208  0.64808403  0.0575366 ]\n",
      "Gradient: [  352.02725324 12965.70161694   412.61596703]\n",
      "Loss value: inf\n",
      "Accuracy: 0.39915966386554624\n",
      "====================================\n",
      "Iteration 23\n",
      "Current point: [-0.6916448  -0.64848613  0.01627501]\n",
      "Gradient: [  -89.76642139 -8207.57985111  -283.16062935]\n",
      "Loss value: 5397.695416667237\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 24\n",
      "Current point: [-0.68266816  0.17227186  0.04459107]\n",
      "Gradient: [  333.30240815 12613.31010977   383.95065982]\n",
      "Loss value: 2060.0077732072505\n",
      "Accuracy: 0.3949579831932773\n",
      "====================================\n",
      "Iteration 25\n",
      "Current point: [-0.7159984  -1.08905915  0.006196  ]\n",
      "Gradient: [  -91.05855113 -8214.75327165  -286.16442477]\n",
      "Loss value: 9020.7276172343\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 26\n",
      "Current point: [-0.70689255 -0.26758383  0.03481245]\n",
      "Gradient: [  -86.63883786 -8158.07697253  -274.92126783]\n",
      "Loss value: 2272.677618533588\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 27\n",
      "Current point: [-0.69822866  0.54822387  0.06230457]\n",
      "Gradient: [  350.83618258 12959.60828356   410.77047532]\n",
      "Loss value: inf\n",
      "Accuracy: 0.39775910364145656\n",
      "====================================\n",
      "Iteration 28\n",
      "Current point: [-0.73331228 -0.74773696  0.02122752]\n",
      "Gradient: [  -90.21841149 -8210.19607004  -284.13817128]\n",
      "Loss value: 6214.7872593512275\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 29\n",
      "Current point: [-0.72429044  0.07328265  0.04964134]\n",
      "Gradient: [  262.76634635 10242.24427353   288.49083126]\n",
      "Loss value: 875.7744580030952\n",
      "Accuracy: 0.39775910364145656\n",
      "====================================\n",
      "Iteration 30\n",
      "Current point: [-0.75056707 -0.95094178  0.02079226]\n",
      "Gradient: [  -90.80739195 -8213.42109382  -285.51368361]\n",
      "Loss value: 7885.18527848174\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 31\n",
      "Current point: [-0.74148633 -0.12959967  0.04934363]\n",
      "Gradient: [  -78.12394933 -7808.65283406  -254.61159978]\n",
      "Loss value: 1160.1988132885951\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 32\n",
      "Current point: [-0.73367394  0.65126562  0.07480479]\n",
      "Gradient: [  351.77350819 12965.40592986   412.42535531]\n",
      "Loss value: inf\n",
      "Accuracy: 0.39775910364145656\n",
      "====================================\n",
      "Iteration 33\n",
      "Current point: [-0.76885129 -0.64527498  0.03356225]\n",
      "Gradient: [  -89.90874424 -8207.61539882  -283.23571075]\n",
      "Loss value: 5373.37994052996\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 34\n",
      "Current point: [-0.75986042  0.17548656  0.06188582]\n",
      "Gradient: [  332.99784149 12620.82253222   384.10681282]\n",
      "Loss value: 2081.4931678891603\n",
      "Accuracy: 0.3949579831932773\n",
      "====================================\n",
      "Iteration 35\n",
      "Current point: [-0.7931602  -1.08659569  0.02347514]\n",
      "Gradient: [  -91.15118742 -8214.80264502  -286.22488252]\n",
      "Loss value: 9002.575458177438\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 36\n",
      "Current point: [-0.78404508 -0.26511543  0.05209763]\n",
      "Gradient: [  -86.88552699 -8157.35318634  -274.97924492]\n",
      "Loss value: 2254.482487241368\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 37\n",
      "Current point: [-0.77535653  0.55061989  0.07959555]\n",
      "Gradient: [  350.55133812 12959.22156265   410.55664678]\n",
      "Loss value: inf\n",
      "Accuracy: 0.39775910364145656\n",
      "====================================\n",
      "Iteration 38\n",
      "Current point: [-0.81041166 -0.74530226  0.03853989]\n",
      "Gradient: [  -90.34582409 -8210.2513987   -284.21275694]\n",
      "Loss value: 6196.8388522782425\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 39\n",
      "Current point: [-0.80137708  0.07572288  0.06696117]\n",
      "Gradient: [  262.6929289  10306.09669351   290.26307847]\n",
      "Loss value: 885.6057139313284\n",
      "Accuracy: 0.39775910364145656\n",
      "====================================\n",
      "Iteration 40\n",
      "Current point: [-0.82764637 -0.95488679  0.03793486]\n",
      "Gradient: [  -90.92475859 -8213.5453408   -285.61243832]\n",
      "Loss value: 7919.696232781069\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 41\n",
      "Current point: [-0.8185539  -0.13353226  0.0664961 ]\n",
      "Gradient: [  -79.46419552 -7844.00437359  -256.47687588]\n",
      "Loss value: 1192.6679052203601\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 42\n",
      "Current point: [-0.81060748  0.65086818  0.09214379]\n",
      "Gradient: [  351.48551903 12964.93282407   412.17956469]\n",
      "Loss value: inf\n",
      "Accuracy: 0.39775910364145656\n",
      "====================================\n",
      "Iteration 43\n",
      "Current point: [-0.84575603 -0.6456251   0.05092583]\n",
      "Gradient: [  -90.05859738 -8207.74540974  -283.33961283]\n",
      "Loss value: 5378.2549824837915\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 44\n",
      "Current point: [-0.83675017  0.17514944  0.07925979]\n",
      "Gradient: [  331.86904096 12606.30569621   383.11385361]\n",
      "Loss value: 2058.3447115411245\n",
      "Accuracy: 0.3963585434173669\n",
      "====================================\n",
      "Iteration 45\n",
      "Current point: [-0.86993707 -1.08548113  0.04094841]\n",
      "Gradient: [  -91.24153707 -8214.85597532  -286.28560774]\n",
      "Loss value: 8995.419531522992\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 46\n",
      "Current point: [-0.86081292 -0.26399554  0.06957697]\n",
      "Gradient: [  -87.1485369  -8157.29872787  -275.09389349]\n",
      "Loss value: 2247.219858772708\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 47\n",
      "Current point: [-0.85209807  0.55173434  0.09708636]\n",
      "Gradient: [  350.25199613 12958.73368404   410.31835413]\n",
      "Loss value: inf\n",
      "Accuracy: 0.39775910364145656\n",
      "====================================\n",
      "Iteration 48\n",
      "Current point: [-0.88712327 -0.74413903  0.05605452]\n",
      "Gradient: [  -90.47104492 -8210.32146024  -284.29022448]\n",
      "Loss value: 6189.245215573466\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 49\n",
      "Current point: [-0.87807616  0.07689311  0.08448355]\n",
      "Gradient: [  260.63719062 10291.69090005   289.30587522]\n",
      "Loss value: 882.6662501996078\n",
      "Accuracy: 0.39915966386554624\n",
      "====================================\n",
      "Iteration 50\n",
      "Current point: [-0.90413988 -0.95227598  0.05555296]\n",
      "Gradient: [  -91.02145332 -8213.58482653  -285.66886448]\n",
      "Loss value: 7900.178529504521\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 51\n",
      "Current point: [-0.89503774 -0.13091749  0.08411984]\n",
      "Gradient: [  -79.75392279 -7830.23580778  -255.97840522]\n",
      "Loss value: 1173.7485293168916\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 52\n",
      "Current point: [-0.88706234  0.65210609  0.10971768]\n",
      "Gradient: [  351.21298907 12964.52989472   411.95853399]\n",
      "Loss value: inf\n",
      "Accuracy: 0.39775910364145656\n",
      "====================================\n",
      "Iteration 53\n",
      "Current point: [-0.92218364 -0.6443469   0.06852183]\n",
      "Gradient: [  -90.19500072 -8207.81082049  -283.41859373]\n",
      "Loss value: 5369.665634920013\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 54\n",
      "Current point: [-0.91316414  0.17643418  0.09686369]\n",
      "Gradient: [  331.12124468 12601.6122605    382.64208771]\n",
      "Loss value: 2055.9468675902776\n",
      "Accuracy: 0.3963585434173669\n",
      "====================================\n",
      "Iteration 55\n",
      "Current point: [-0.94627627 -1.08372705  0.05859948]\n",
      "Gradient: [  -91.32615035 -8214.89745666  -286.33836081]\n",
      "Loss value: 8982.924752730556\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 56\n",
      "Current point: [-0.93714365 -0.2622373   0.08723332]\n",
      "Gradient: [  -87.38817071 -8156.79997022  -275.1594183 ]\n",
      "Loss value: 2234.681387696277\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 57\n",
      "Current point: [-0.92840483  0.5534427   0.11474926]\n",
      "Gradient: [  349.96054433 12958.28148022   410.09205938]\n",
      "Loss value: inf\n",
      "Accuracy: 0.39775910364145656\n",
      "====================================\n",
      "Iteration 58\n",
      "Current point: [-0.96340089 -0.74238545  0.07374005]\n",
      "Gradient: [  -90.58845673 -8210.36750413  -284.35592643]\n",
      "Loss value: 6176.724724117164\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 59\n",
      "Current point: [-0.95434204  0.0786513   0.10217565]\n",
      "Gradient: [  259.55047997 10311.79551252   289.57809405]\n",
      "Loss value: 886.0635502436535\n",
      "Accuracy: 0.39915966386554624\n",
      "====================================\n",
      "Iteration 60\n",
      "Current point: [-0.98029709 -0.95252825  0.07321784]\n",
      "Gradient: [  -91.1193778  -8213.64932113  -285.73507681]\n",
      "Loss value: 7904.139427933524\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 61\n",
      "Current point: [-0.97118515 -0.13116332  0.10179134]\n",
      "Gradient: [  -80.43865528 -7836.72318064  -256.44097257]\n",
      "Loss value: 1177.2458910066734\n",
      "Accuracy: 0.5966386554621849\n",
      "====================================\n",
      "Iteration 62\n",
      "Current point: [-0.96314129  0.652509    0.12743544]\n",
      "Gradient: [  350.93212966 12964.07830331   411.72432107]\n",
      "Loss value: inf\n",
      "Accuracy: 0.39775910364145656\n",
      "====================================\n",
      "Iteration 63\n",
      "Current point: [-0.9982345  -0.64389883  0.08626301]\n",
      "Gradient: [  -90.32789777 -8207.88765681  -283.49836885]\n",
      "Loss value: 5367.8235442885525\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 64\n",
      "Current point: [-0.98920171  0.17688993  0.11461285]\n",
      "Gradient: [  330.16518112 12591.38490921   381.88837899]\n",
      "Loss value: 2043.3312721373097\n",
      "Accuracy: 0.3963585434173669\n",
      "====================================\n",
      "Iteration 65\n",
      "Current point: [-1.02221823 -1.08224856  0.07642401]\n",
      "Gradient: [  -91.40688847 -8214.93476876  -286.38723901]\n",
      "Loss value: 8972.613381045256\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 66\n",
      "Current point: [-1.01307754 -0.26075508  0.10506273]\n",
      "Gradient: [  -87.62270451 -8156.37638542  -275.22520903]\n",
      "Loss value: 2224.3296676509062\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 67\n",
      "Current point: [-1.00431527  0.55488256  0.13258525]\n",
      "Gradient: [  349.66686364 12957.80187856   409.86122437]\n",
      "Loss value: inf\n",
      "Accuracy: 0.39775910364145656\n",
      "====================================\n",
      "Iteration 68\n",
      "Current point: [-1.03928196 -0.74089763  0.09159913]\n",
      "Gradient: [  -90.70093502 -8210.40800122  -284.41695688]\n",
      "Loss value: 6166.308490489846\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 69\n",
      "Current point: [-1.03021186  0.08014317  0.12004083]\n",
      "Gradient: [  258.07180382 10315.29640476   289.27848189]\n",
      "Loss value: 886.9850108061175\n",
      "Accuracy: 0.4005602240896359\n",
      "====================================\n",
      "Iteration 70\n",
      "Current point: [-1.05601904 -0.95138647  0.09111298]\n",
      "Gradient: [  -91.20947039 -8213.69012421  -285.7881685 ]\n",
      "Loss value: 7896.550629606742\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 71\n",
      "Current point: [-1.0468981  -0.13001746  0.1196918 ]\n",
      "Gradient: [  -80.90019839 -7832.47631306  -256.37984125]\n",
      "Loss value: 1169.7863790374918\n",
      "Accuracy: 0.5966386554621849\n",
      "====================================\n",
      "Iteration 72\n",
      "Current point: [-1.03880808  0.65323017  0.14532978]\n",
      "Gradient: [  350.65468097 12963.63471972   411.49515626]\n",
      "Loss value: inf\n",
      "Accuracy: 0.39775910364145656\n",
      "====================================\n",
      "Iteration 73\n",
      "Current point: [-1.07387354 -0.6431333   0.10418026]\n",
      "Gradient: [  -90.45323602 -8207.9413853   -283.56744437]\n",
      "Loss value: 5363.297056224372\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 74\n",
      "Current point: [-1.06482822  0.17766084  0.13253701]\n",
      "Gradient: [  329.28433645 12582.9294794    381.23406153]\n",
      "Loss value: 2034.9379943302097\n",
      "Accuracy: 0.3963585434173669\n",
      "====================================\n",
      "Iteration 75\n",
      "Current point: [-1.09775665 -1.08063211  0.0944136 ]\n",
      "Gradient: [  -91.48326727 -8214.96486039  -286.43072783]\n",
      "Loss value: 8961.089579428099\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 76\n",
      "Current point: [-1.08860833 -0.25913562  0.12305668]\n",
      "Gradient: [  -87.84470548 -8155.79005028  -275.26925018]\n",
      "Loss value: 2212.795061573667\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 77\n",
      "Current point: [-1.07982386  0.55644338  0.1505836 ]\n",
      "Gradient: [  349.37602303 12957.32431169   409.63371983]\n",
      "Loss value: inf\n",
      "Accuracy: 0.39775910364145656\n",
      "====================================\n",
      "Iteration 78\n",
      "Current point: [-1.11476146 -0.73928905  0.10962023]\n",
      "Gradient: [  -90.80761194 -8210.43523544  -284.47051092]\n",
      "Loss value: 6154.825472614122\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 79\n",
      "Current point: [-1.1056807   0.08175448  0.13806728]\n",
      "Gradient: [  256.81842279 10325.11439082   289.2164002 ]\n",
      "Loss value: 889.3993228570441\n",
      "Accuracy: 0.4005602240896359\n",
      "====================================\n",
      "Iteration 80\n",
      "Current point: [-1.13136254 -0.95075696  0.10914564]\n",
      "Gradient: [  -91.29598697 -8213.72946368  -285.83866986]\n",
      "Loss value: 7893.101384331951\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 81\n",
      "Current point: [-1.12223294 -0.12938402  0.13772951]\n",
      "Gradient: [  -81.40893134 -7831.4473116   -256.46242854]\n",
      "Loss value: 1166.313865768061\n",
      "Accuracy: 0.5994397759103641\n",
      "====================================\n",
      "Iteration 82\n",
      "Current point: [-1.11409205  0.65376071  0.16337575]\n",
      "Gradient: [  350.37567522 12963.1740347    411.2632184 ]\n",
      "Loss value: inf\n",
      "Accuracy: 0.39775910364145656\n",
      "====================================\n",
      "Iteration 83\n",
      "Current point: [-1.14912962 -0.64255669  0.12224943]\n",
      "Gradient: [  -90.5731193  -8207.98801862  -283.63127411]\n",
      "Loss value: 5360.251554664357\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 84\n",
      "Current point: [-1.1400723   0.17824211  0.15061255]\n",
      "Gradient: [  328.34865637 12572.93299455   380.50719261]\n",
      "Loss value: 2024.3920962237726\n",
      "Accuracy: 0.3963585434173669\n",
      "====================================\n",
      "Iteration 85\n",
      "Current point: [-1.17290717 -1.07905119  0.11256184]\n",
      "Gradient: [  -91.55578556 -8214.98938334  -286.4697603 ]\n",
      "Loss value: 8949.781538336785\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 86\n",
      "Current point: [-1.16375159 -0.25755225  0.14120881]\n",
      "Gradient: [  -88.05775329 -8155.14164451  -275.30123446]\n",
      "Loss value: 2201.4938631893456\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 87\n",
      "Current point: [-1.15494582  0.55796192  0.16873893]\n",
      "Gradient: [  349.0862177  12956.83688854   409.40653824]\n",
      "Loss value: inf\n",
      "Accuracy: 0.39775910364145656\n",
      "====================================\n",
      "Iteration 88\n",
      "Current point: [-1.18985444 -0.73772177  0.12779828]\n",
      "Gradient: [  -90.90919921 -8210.45301913  -284.51812199]\n",
      "Loss value: 6143.6087428271585\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 89\n",
      "Current point: [-1.18076352  0.08332353  0.15625009]\n",
      "Gradient: [  255.53073932 10331.80540178   289.05685361]\n",
      "Loss value: 891.6284960867098\n",
      "Accuracy: 0.39915966386554624\n",
      "====================================\n",
      "Iteration 90\n",
      "Current point: [-1.20631659 -0.94985701  0.12734441]\n",
      "Gradient: [  -91.37759806 -8213.75871414  -285.88263168]\n",
      "Loss value: 7887.353174000688\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 91\n",
      "Current point: [-1.19717883 -0.12848114  0.15593267]\n",
      "Gradient: [  -81.86048179 -7827.89939873  -256.41565185]\n",
      "Loss value: 1160.694836306182\n",
      "Accuracy: 0.5994397759103641\n",
      "====================================\n",
      "Iteration 92\n",
      "Current point: [-1.18899278  0.6543088   0.18157424]\n",
      "Gradient: [  350.09749991 12962.70696409   411.03202182]\n",
      "Loss value: inf\n",
      "Accuracy: 0.39775910364145656\n",
      "====================================\n",
      "Iteration 93\n",
      "Current point: [-1.22400253 -0.6419619   0.14047103]\n",
      "Gradient: [  -90.68708976 -8208.02176362  -283.68803307]\n",
      "Loss value: 5356.986517387057\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 94\n",
      "Current point: [-1.21493382  0.17884028  0.16883984]\n",
      "Gradient: [  327.41193701 12562.7554787    379.77856433]\n",
      "Loss value: 2014.2930795859022\n",
      "Accuracy: 0.3963585434173669\n",
      "====================================\n",
      "Iteration 95\n",
      "Current point: [-1.24767502 -1.07743527  0.13086198]\n",
      "Gradient: [  -91.62453671 -8215.00801521  -286.50424555]\n",
      "Loss value: 8938.112023631353\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 96\n",
      "Current point: [-1.23851256 -0.25593447  0.15951241]\n",
      "Gradient: [  -88.26098081 -8154.38853335  -275.31756138]\n",
      "Loss value: 2189.852975325614\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 97\n",
      "Current point: [-1.22968647  0.55950439  0.18704416]\n",
      "Gradient: [  348.79853465 12956.34507964   409.18120198]\n",
      "Loss value: inf\n",
      "Accuracy: 0.39775910364145656\n",
      "====================================\n",
      "Iteration 98\n",
      "Current point: [-1.26456632 -0.73613012  0.14612604]\n",
      "Gradient: [  -91.00576573 -8210.46033829  -284.55951706]\n",
      "Loss value: 6132.121214750399\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 99\n",
      "Current point: [-1.25546574  0.08491591  0.17458199]\n",
      "Gradient: [  254.31422362 10339.21581415   288.93609815]\n",
      "Loss value: 894.3414374535259\n",
      "Accuracy: 0.39915966386554624\n",
      "====================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.28089717, -0.94900567,  0.14568838])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gradient_descent_process(starting_point, learning_rate, iterations):\n",
    "\n",
    "    # clear the graphs folder\n",
    "    import os\n",
    "    import shutil\n",
    "    shutil.rmtree('./graphs')\n",
    "    os.mkdir('./graphs')\n",
    "\n",
    "    x = np.linspace(-3, 0.001, 100)\n",
    "    y = np.linspace(-0.05, 0.06, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "    Z = [[func.value(x, y, 0.0012) for x in x] for y in y]\n",
    "\n",
    "    plt.contourf(X, Y, Z, 20, cmap = 'RdGy')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    cur_point = starting_point\n",
    "    for i in range(iterations):\n",
    "        gradient = func.derivative(cur_point[0], cur_point[1], cur_point[2])\n",
    "        print(\"Iteration {id}\".format(id = i))\n",
    "        print(\"Current point: {point}\".format(point = cur_point))\n",
    "        print(\"Gradient: {grad}\".format(grad = gradient))\n",
    "        print(\"Loss value: {loss}\".format(loss = func.value(cur_point[0], cur_point[1], cur_point[2])))\n",
    "        print(\"Accuracy: {acc}\".format(acc = func.accuracy(cur_point[0], cur_point[1], cur_point[2])))\n",
    "        print(\"====================================\")\n",
    "        next_point = cur_point - gradient * learning_rate\n",
    "\n",
    "        # plot the current point as grey point, the new point as red\n",
    "        plt.scatter(cur_point[0], cur_point[1], c = 'grey')\n",
    "        plt.scatter(next_point[0], next_point[1], c = 'red')\n",
    "        plt.plot([cur_point[0], next_point[0]], [cur_point[1], next_point[1]], c = 'yellow')\n",
    "        plt.savefig('./graphs/iteration_{id}.png'.format(id = i))\n",
    "\n",
    "        cur_point = next_point\n",
    "\n",
    "        if (np.linalg.norm(gradient) < 0.00001):\n",
    "            break\n",
    "\n",
    "    return cur_point\n",
    "\n",
    "starting_point = np.array([-0.5, 0.00, 0.0012])\n",
    "learning_rate = 0.00001\n",
    "iterations = 100\n",
    "gradient_descent_process(starting_point, learning_rate, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-3, 0.001, 100)\n",
    "y = np.linspace(-0.05, 0.06, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "n_layers = 30\n",
    "z = np.linspace(-0.1, 0.1, n_layers)\n",
    "\n",
    "id = 0\n",
    "import os\n",
    "import shutil\n",
    "shutil.rmtree('./loss_graphs')\n",
    "os.makedirs('./loss_graphs')\n",
    "for z_value in z:\n",
    "    Z = [[func.value(x, y, z_value) for x in x] for y in y]\n",
    "\n",
    "    plt.contourf(X, Y, Z, 100, cmap = 'RdGy')\n",
    "    # always fix the colorbar to start from 360\n",
    "    plt.colorbar()\n",
    "    plt.title('z = {z}'.format(z = z_value))\n",
    "    plt.savefig('./loss_graphs/loss_{z}.png'.format(z = id))\n",
    "    plt.clf()\n",
    "    id += 1\n",
    "# plt.contourf(X, Y, Z, 20, cmap = 'RdGy')\n",
    "# plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_3200\\2113232225.py:6: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images.append(imageio.imread('./loss_graphs/' + 'loss_{id}.png'.format(id = i)))\n"
     ]
    }
   ],
   "source": [
    "# save all images in ./loss_graphs as a gif\n",
    "import imageio\n",
    "import os\n",
    "images = []\n",
    "for i in range(n_layers):\n",
    "    images.append(imageio.imread('./loss_graphs/' + 'loss_{id}.png'.format(id = i)))\n",
    "imageio.mimsave('./titanic-loss.gif', images)\n",
    "shutil.rmtree('./loss_graphs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem arised!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the descent direction always go vertical? Because when calculating gradient, the y-direction is multiply with a large value of $y_i$. The age is always big (at least 20, 30 for example). But other values like $x_i$ is only 0 and 1. Therefore, other direction became insignificant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0\n",
      "count  714.000000\n",
      "mean     0.367921\n",
      "std      0.182540\n",
      "min      0.000000\n",
      "25%      0.247612\n",
      "50%      0.346569\n",
      "75%      0.472229\n",
      "max      1.000000\n"
     ]
    }
   ],
   "source": [
    "# normalize age between 0 and 1\n",
    "age = np.array(data['Age'])\n",
    "age = (age - np.min(age)) / (np.max(age) - np.min(age))\n",
    "print(pd.DataFrame(age).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is very important, as you may have seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = LossFunction(sex, age, survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that it is not moving in the perpendicular direction of the contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_22024\\2351286146.py:31: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images.append(imageio.imread('./loss_graphs/' + 'loss_{id}.png'.format(id = i)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-3, 0.3, 100)\n",
    "y = np.linspace(-0.1, 1.5, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "n_layers = 30\n",
    "z = np.linspace(-0.1, 0.9, n_layers)\n",
    "\n",
    "id = 0\n",
    "import os\n",
    "import shutil\n",
    "if (os.path.exists('./loss_graphs')):\n",
    "    shutil.rmtree('./loss_graphs')\n",
    "os.makedirs('./loss_graphs')\n",
    "for z_value in z:\n",
    "    Z = [[func.value(x, y, z_value) for x in x] for y in y]\n",
    "\n",
    "    plt.contourf(X, Y, Z, 50, cmap = 'RdGy')\n",
    "    # always fix the colorbar to start from 360\n",
    "    plt.colorbar()\n",
    "    plt.title('z = {z}'.format(z = z_value))\n",
    "    plt.savefig('./loss_graphs/loss_{z}.png'.format(z = id))\n",
    "    plt.clf()\n",
    "    id += 1\n",
    "# plt.contourf(X, Y, Z, 20, cmap = 'RdGy')\n",
    "# plt.colorbar()\n",
    "# save all images in ./loss_graphs as a gif\n",
    "import imageio\n",
    "import os\n",
    "images = []\n",
    "for i in range(n_layers):\n",
    "    images.append(imageio.imread('./loss_graphs/' + 'loss_{id}.png'.format(id = i)))\n",
    "imageio.mimsave('./titanic-loss-normalized.gif', images)\n",
    "shutil.rmtree('./loss_graphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Current point: [-0.5  0.  -0.1]\n",
      "Gradient: [67.51769328  2.20925533 -5.50187465]\n",
      "Loss value: 441.8695698493082\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 1\n",
      "Current point: [-0.63503539 -0.00441851 -0.08899625]\n",
      "Gradient: [ 54.73956854  -2.46783766 -17.6630191 ]\n",
      "Loss value: 433.49354255339847\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 2\n",
      "Current point: [-7.44514524e-01  5.17164649e-04 -5.36702125e-02]\n",
      "Gradient: [ 47.63662074  -4.31242023 -22.35286937]\n",
      "Loss value: 427.1670944693808\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 3\n",
      "Current point: [-0.83978777  0.00914201 -0.00896447]\n",
      "Gradient: [ 43.09297897  -4.92254789 -23.78584928]\n",
      "Loss value: 421.77420692972254\n",
      "Accuracy: 0.5938375350140056\n",
      "====================================\n",
      "Iteration 4\n",
      "Current point: [-0.92597372  0.0189871   0.03860722]\n",
      "Gradient: [ 39.79704311  -4.97768581 -23.7562892 ]\n",
      "Loss value: 417.02280846871895\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 5\n",
      "Current point: [-1.00556781  0.02894247  0.0861198 ]\n",
      "Gradient: [ 37.1574632   -4.78484626 -23.0755902 ]\n",
      "Loss value: 412.7992006081048\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 6\n",
      "Current point: [-1.07988274  0.03851217  0.13227098]\n",
      "Gradient: [ 34.89541069  -4.48727917 -22.12251844]\n",
      "Loss value: 409.03464931978937\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 7\n",
      "Current point: [-1.14967356  0.04748672  0.17651602]\n",
      "Gradient: [ 32.87472272  -4.15269803 -21.07615951]\n",
      "Loss value: 405.67543478975546\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 8\n",
      "Current point: [-1.215423    0.05579212  0.21866834]\n",
      "Gradient: [ 31.02637555  -3.81301675 -20.0209209 ]\n",
      "Loss value: 402.67553842692456\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 9\n",
      "Current point: [-1.27747575  0.06341815  0.25871018]\n",
      "Gradient: [ 29.31357748  -3.48292454 -18.99563504]\n",
      "Loss value: 399.99451957081993\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 10\n",
      "Current point: [-1.33610291  0.070384    0.29670145]\n",
      "Gradient: [ 27.71525944  -3.16876301 -18.01703282]\n",
      "Loss value: 397.5966268173534\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 11\n",
      "Current point: [-1.39153343  0.07672153  0.33273552]\n",
      "Gradient: [ 26.21815387  -2.87282876 -17.09112374]\n",
      "Loss value: 395.4502262512716\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 12\n",
      "Current point: [-1.44396974  0.08246719  0.36691776]\n",
      "Gradient: [ 24.81295763  -2.59547833 -16.21876595]\n",
      "Loss value: 393.52733290583217\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 13\n",
      "Current point: [-1.49359565  0.08765814  0.3993553 ]\n",
      "Gradient: [ 23.49246035  -2.33616429 -15.39840978]\n",
      "Loss value: 391.80319624026407\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 14\n",
      "Current point: [-1.54058057  0.09233047  0.43015212]\n",
      "Gradient: [ 22.25062605  -2.09394631 -14.62745299]\n",
      "Loss value: 390.25592801392065\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 15\n",
      "Current point: [-1.58508182  0.09651836  0.45940702]\n",
      "Gradient: [ 21.08213961  -1.86774249 -13.90290872]\n",
      "Loss value: 388.86616878394346\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 16\n",
      "Current point: [-1.6272461   0.10025385  0.48721284]\n",
      "Gradient: [ 19.98218021  -1.65645131 -13.22173088]\n",
      "Loss value: 387.6167904213793\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 17\n",
      "Current point: [-1.66721046  0.10356675  0.5136563 ]\n",
      "Gradient: [ 18.94630569  -1.45900888 -12.58096811]\n",
      "Loss value: 386.49263193824123\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 18\n",
      "Current point: [-1.70510307  0.10648477  0.53881824]\n",
      "Gradient: [ 17.97039073  -1.27441378 -11.97783167]\n",
      "Loss value: 385.4802657385863\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 19\n",
      "Current point: [-1.74104386  0.1090336   0.5627739 ]\n",
      "Gradient: [ 17.05059093  -1.10173583 -11.4097203 ]\n",
      "Loss value: 384.56779135676686\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 20\n",
      "Current point: [-1.77514504  0.11123707  0.58559334]\n",
      "Gradient: [ 16.18331929  -0.94011692 -10.8742239 ]\n",
      "Loss value: 383.74465381851905\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 21\n",
      "Current point: [-1.80751168  0.1131173   0.60734179]\n",
      "Gradient: [ 15.36522843  -0.78876808 -10.36911697]\n",
      "Loss value: 383.00148391510857\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 22\n",
      "Current point: [-1.83824213  0.11469484  0.62808002]\n",
      "Gradient: [14.59319545 -0.64696499 -9.89234762]\n",
      "Loss value: 382.3299578818567\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 23\n",
      "Current point: [-1.86742852  0.11598877  0.64786472]\n",
      "Gradient: [13.86430819 -0.51404292 -9.4420248 ]\n",
      "Loss value: 381.722674194599\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 24\n",
      "Current point: [-1.89515714  0.11701685  0.66674877]\n",
      "Gradient: [13.17585207 -0.38939159 -9.01640543]\n",
      "Loss value: 381.1730454240684\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 25\n",
      "Current point: [-1.92150884  0.11779564  0.68478158]\n",
      "Gradient: [12.52529758 -0.2724504  -8.61388193]\n",
      "Loss value: 380.675203308228\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 26\n",
      "Current point: [-1.94655944  0.11834054  0.70200934]\n",
      "Gradient: [11.91028814 -0.16270386 -8.23297059]\n",
      "Loss value: 380.2239154098527\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 27\n",
      "Current point: [-1.97038002  0.11866595  0.71847528]\n",
      "Gradient: [11.32862851 -0.05967749 -7.87230076]\n",
      "Loss value: 379.8145119178389\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 28\n",
      "Current point: [-1.99303727  0.1187853   0.73421989]\n",
      "Gradient: [10.7782737   0.03706594 -7.53060508]\n",
      "Loss value: 379.44282132434984\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 29\n",
      "Current point: [-2.01459382  0.11871117  0.7492811 ]\n",
      "Gradient: [10.25731852  0.12792979 -7.20671044]\n",
      "Loss value: 379.1051138658447\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 30\n",
      "Current point: [-2.03510846  0.11845531  0.76369452]\n",
      "Gradient: [ 9.76398763  0.21328665 -6.89952996]\n",
      "Loss value: 378.79805175489474\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 31\n",
      "Current point: [-2.05463643  0.11802874  0.77749358]\n",
      "Gradient: [ 9.29662623  0.29348103 -6.60805554]\n",
      "Loss value: 378.51864535253446\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 32\n",
      "Current point: [-2.07322969  0.11744177  0.79070969]\n",
      "Gradient: [ 8.85369141  0.36883192 -6.3313512 ]\n",
      "Loss value: 378.26421453905016\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 33\n",
      "Current point: [-2.09093707  0.11670411  0.80337239]\n",
      "Gradient: [ 8.43374392  0.43963499 -6.06854709]\n",
      "Loss value: 378.0323546359484\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 34\n",
      "Current point: [-2.10780456  0.11582484  0.81550948]\n",
      "Gradient: [ 8.03544072  0.5061646  -5.81883395]\n",
      "Loss value: 377.82090631479383\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 35\n",
      "Current point: [-2.12387544  0.11481251  0.82714715]\n",
      "Gradient: [ 7.65752786  0.56867565 -5.58145816]\n",
      "Loss value: 377.62792900097645\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 36\n",
      "Current point: [-2.13919049  0.11367516  0.83831007]\n",
      "Gradient: [ 7.29883405  0.62740519 -5.35571728]\n",
      "Loss value: 377.4516773435231\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 37\n",
      "Current point: [-2.15378816  0.11242035  0.8490215 ]\n",
      "Gradient: [ 6.95826456  0.68257393 -5.1409559 ]\n",
      "Loss value: 377.29058037693403\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 38\n",
      "Current point: [-2.16770469  0.1110552   0.85930341]\n",
      "Gradient: [ 6.63479572  0.73438759 -4.93656195]\n",
      "Loss value: 377.1432230487412\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 39\n",
      "Current point: [-2.18097428  0.10958643  0.86917654]\n",
      "Gradient: [ 6.32746969  0.78303808 -4.74196335]\n",
      "Loss value: 377.0083298279542\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 40\n",
      "Current point: [-2.19362922  0.10802035  0.87866047]\n",
      "Gradient: [ 6.03538979  0.82870461 -4.55662487]\n",
      "Loss value: 376.88475014559947\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 41\n",
      "Current point: [-2.2057      0.10636294  0.88777371]\n",
      "Gradient: [ 5.75771605  0.87155473 -4.38004538]\n",
      "Loss value: 376.7714454498847\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 42\n",
      "Current point: [-2.21721543  0.10461983  0.89653381]\n",
      "Gradient: [ 5.49366116  0.91174518 -4.21175529]\n",
      "Loss value: 376.6674776857421\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 43\n",
      "Current point: [-2.22820275  0.10279634  0.90495732]\n",
      "Gradient: [ 5.24248678  0.94942278 -4.05131416]\n",
      "Loss value: 376.57199903218486\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 44\n",
      "Current point: [-2.23868773  0.1008975   0.91305994]\n",
      "Gradient: [ 5.00350003  0.9847251  -3.89830864]\n",
      "Loss value: 376.48424275150944\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 45\n",
      "Current point: [-2.24869473  0.09892804  0.92085656]\n",
      "Gradient: [ 4.77605035  1.01778124 -3.75235045]\n",
      "Loss value: 376.4035150223064\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 46\n",
      "Current point: [-2.25824683  0.09689248  0.92836126]\n",
      "Gradient: [ 4.55952653  1.04871237 -3.61307463]\n",
      "Loss value: 376.32918764386795\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 47\n",
      "Current point: [-2.26736588  0.09479506  0.93558741]\n",
      "Gradient: [ 4.35335403  1.07763237 -3.48013789]\n",
      "Loss value: 376.26069151319416\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 48\n",
      "Current point: [-2.27607259  0.09263979  0.94254769]\n",
      "Gradient: [ 4.15699244  1.1046483  -3.3532171 ]\n",
      "Loss value: 376.19751078768553\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 49\n",
      "Current point: [-2.28438658  0.0904305   0.94925412]\n",
      "Gradient: [ 3.96993319  1.12986093 -3.2320079 ]\n",
      "Loss value: 376.1391776569843\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 50\n",
      "Current point: [-2.29232644  0.08817077  0.95571814]\n",
      "Gradient: [ 3.79169739  1.15336512 -3.11622342]\n",
      "Loss value: 376.08526765649447\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 51\n",
      "Current point: [-2.29990984  0.08586404  0.96195058]\n",
      "Gradient: [ 3.62183387  1.17525027 -3.00559314]\n",
      "Loss value: 376.0353954630475\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 52\n",
      "Current point: [-2.3071535   0.08351354  0.96796177]\n",
      "Gradient: [ 3.45991733  1.1956007  -2.89986176]\n",
      "Loss value: 375.98921112012476\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 53\n",
      "Current point: [-2.31407334  0.08112234  0.97376149]\n",
      "Gradient: [ 3.30554668  1.21449593 -2.79878826]\n",
      "Loss value: 375.9463966461407\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 54\n",
      "Current point: [-2.32068443  0.07869335  0.97935907]\n",
      "Gradient: [ 3.15834342  1.23201106 -2.70214491]\n",
      "Loss value: 375.90666298463225\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 55\n",
      "Current point: [-2.32700112  0.07622933  0.98476336]\n",
      "Gradient: [ 3.01795022  1.24821702 -2.60971647]\n",
      "Loss value: 375.86974725989364\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 56\n",
      "Current point: [-2.33303702  0.07373289  0.98998279]\n",
      "Gradient: [ 2.88402958  1.26318085 -2.5212994 ]\n",
      "Loss value: 375.83541030572155\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 57\n",
      "Current point: [-2.33880508  0.07120653  0.99502539]\n",
      "Gradient: [ 2.75626253  1.27696596 -2.43670108]\n",
      "Loss value: 375.8034344385673\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 58\n",
      "Current point: [-2.3443176   0.0686526   0.99989879]\n",
      "Gradient: [ 2.63434753  1.28963234 -2.35573919]\n",
      "Loss value: 375.7736214495899\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 59\n",
      "Current point: [-2.3495863   0.06607334  1.00461027]\n",
      "Gradient: [ 2.51799935  1.30123676 -2.27824106]\n",
      "Loss value: 375.7457907929272\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 60\n",
      "Current point: [-2.3546223   0.06347086  1.00916676]\n",
      "Gradient: [ 2.40694808  1.31183301 -2.20404307]\n",
      "Loss value: 375.71977794999043\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 61\n",
      "Current point: [-2.35943619  0.0608472   1.01357484]\n",
      "Gradient: [ 2.3009382   1.32147204 -2.13299014]\n",
      "Loss value: 375.6954329517879\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 62\n",
      "Current point: [-2.36403807  0.05820425  1.01784082]\n",
      "Gradient: [ 2.1997277   1.33020217 -2.06493519]\n",
      "Loss value: 375.6726190432314\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 63\n",
      "Current point: [-2.36843753  0.05554385  1.02197069]\n",
      "Gradient: [ 2.10308731  1.33806918 -1.9997387 ]\n",
      "Loss value: 375.65121147509944\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 64\n",
      "Current point: [-2.3726437   0.05286771  1.02597017]\n",
      "Gradient: [ 2.01079967  1.34511655 -1.93726825]\n",
      "Loss value: 375.63109641085964\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 65\n",
      "Current point: [-2.3766653   0.05017748  1.02984471]\n",
      "Gradient: [ 1.92265871  1.3513855  -1.87739812]\n",
      "Loss value: 375.61216993690823\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 66\n",
      "Current point: [-2.38051062  0.04747471  1.0335995 ]\n",
      "Gradient: [ 1.83846896  1.3569152  -1.82000891]\n",
      "Loss value: 375.5943371659841\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 67\n",
      "Current point: [-2.38418755  0.04476088  1.03723952]\n",
      "Gradient: [ 1.75804492  1.36174282 -1.76498719]\n",
      "Loss value: 375.5775114245871\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 68\n",
      "Current point: [-2.38770364  0.04203739  1.04076949]\n",
      "Gradient: [ 1.68121051  1.3659037  -1.71222514]\n",
      "Loss value: 375.5616135161787\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 69\n",
      "Current point: [-2.39106607  0.03930558  1.04419394]\n",
      "Gradient: [ 1.60779855  1.3694314  -1.66162028]\n",
      "Loss value: 375.5465710527908\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 70\n",
      "Current point: [-2.39428166  0.03656672  1.04751719]\n",
      "Gradient: [ 1.53765022  1.37235785 -1.61307512]\n",
      "Loss value: 375.53231784842274\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 71\n",
      "Current point: [-2.39735696  0.033822    1.05074334]\n",
      "Gradient: [ 1.47061463  1.37471338 -1.56649696]\n",
      "Loss value: 375.51879336827807\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 72\n",
      "Current point: [-2.40029819  0.03107258  1.05387633]\n",
      "Gradient: [ 1.40654835  1.37652687 -1.52179757]\n",
      "Loss value: 375.50594222849645\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 73\n",
      "Current point: [-2.40311129  0.02831952  1.05691992]\n",
      "Gradient: [ 1.34531504  1.37782577 -1.47889297]\n",
      "Loss value: 375.4937137415684\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 74\n",
      "Current point: [-2.40580192  0.02556387  1.05987771]\n",
      "Gradient: [ 1.28678503  1.37863622 -1.43770322]\n",
      "Loss value: 375.4820615031033\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 75\n",
      "Current point: [-2.40837549  0.0228066   1.06275312]\n",
      "Gradient: [ 1.23083495  1.37898309 -1.3981522 ]\n",
      "Loss value: 375.47094301605165\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 76\n",
      "Current point: [-2.41083716  0.02004863  1.06554942]\n",
      "Gradient: [ 1.17734744  1.37889006 -1.36016738]\n",
      "Loss value: 375.4603193488616\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 77\n",
      "Current point: [-2.41319185  0.01729085  1.06826976]\n",
      "Gradient: [ 1.12621079  1.37837969 -1.32367968]\n",
      "Loss value: 375.45015482440164\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 78\n",
      "Current point: [-2.41544428  0.01453409  1.07091712]\n",
      "Gradient: [ 1.07731865  1.37747346 -1.28862326]\n",
      "Loss value: 375.4404167367851\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 79\n",
      "Current point: [-2.41759891  0.01177915  1.07349436]\n",
      "Gradient: [ 1.03056976  1.37619182 -1.25493537]\n",
      "Loss value: 375.4310750935115\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 80\n",
      "Current point: [-2.41966005  0.00902676  1.07600423]\n",
      "Gradient: [ 0.98586766  1.37455427 -1.22255619]\n",
      "Loss value: 375.4221023805901\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 81\n",
      "Current point: [-2.42163179  0.00627765  1.07844934]\n",
      "Gradient: [ 0.94312048  1.37257939 -1.19142866]\n",
      "Loss value: 375.4134733485325\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 82\n",
      "Current point: [-2.42351803  0.0035325   1.0808322 ]\n",
      "Gradient: [ 0.90224065  1.37028489 -1.16149838]\n",
      "Loss value: 375.40516481730583\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 83\n",
      "Current point: [-2.42532251e+00  7.91925895e-04  1.08315520e+00]\n",
      "Gradient: [ 0.86314476  1.36768764 -1.13271345]\n",
      "Loss value: 375.3971554985164\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 84\n",
      "Current point: [-2.42704880e+00 -1.94344938e-03  1.08542063e+00]\n",
      "Gradient: [ 0.82575325  1.36480373 -1.10502436]\n",
      "Loss value: 375.3894258332592\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 85\n",
      "Current point: [-2.42870031 -0.00467306  1.08763067]\n",
      "Gradient: [ 0.78999029  1.3616485  -1.07838384]\n",
      "Loss value: 375.38195784421583\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 86\n",
      "Current point: [-2.43028029 -0.00739635  1.08978744]\n",
      "Gradient: [ 0.75578357  1.35823659 -1.05274681]\n",
      "Loss value: 375.3747350007137\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 87\n",
      "Current point: [-2.43179185 -0.01011283  1.09189294]\n",
      "Gradient: [ 0.72306412  1.35458194 -1.02807023]\n",
      "Loss value: 375.3677420955837\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 88\n",
      "Current point: [-2.43323798 -0.01282199  1.09394908]\n",
      "Gradient: [ 0.69176616  1.35069786 -1.00431299]\n",
      "Loss value: 375.36096513275714\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 89\n",
      "Current point: [-2.43462151 -0.01552339  1.0959577 ]\n",
      "Gradient: [ 0.6618269   1.34659703 -0.98143585]\n",
      "Loss value: 375.3543912246428\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 90\n",
      "Current point: [-2.43594517 -0.01821658  1.09792057]\n",
      "Gradient: [ 0.63318646  1.34229158 -0.95940134]\n",
      "Loss value: 375.34800849841486\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 91\n",
      "Current point: [-2.43721154 -0.02090116  1.09983938]\n",
      "Gradient: [ 0.60578767  1.33779304 -0.93817368]\n",
      "Loss value: 375.3418060104169\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 92\n",
      "Current point: [-2.43842312 -0.02357675  1.10171572]\n",
      "Gradient: [ 0.57957597  1.33311244 -0.91771866]\n",
      "Loss value: 375.3357736679674\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 93\n",
      "Current point: [-2.43958227 -0.02624297  1.10355116]\n",
      "Gradient: [ 0.55449927  1.32826028 -0.89800364]\n",
      "Loss value: 375.3299021579093\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 94\n",
      "Current point: [-2.44069127 -0.0288995   1.10534717]\n",
      "Gradient: [ 0.53050784  1.3232466  -0.87899739]\n",
      "Loss value: 375.3241828813121\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 95\n",
      "Current point: [-2.44175228 -0.03154599  1.10710516]\n",
      "Gradient: [ 0.50755419  1.31808096 -0.86067011]\n",
      "Loss value: 375.31860789378493\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 96\n",
      "Current point: [-2.44276739 -0.03418215  1.1088265 ]\n",
      "Gradient: [ 0.48559298  1.31277251 -0.84299329]\n",
      "Loss value: 375.31316985090893\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 97\n",
      "Current point: [-2.44373858 -0.0368077   1.11051249]\n",
      "Gradient: [ 0.46458088  1.30732994 -0.82593969]\n",
      "Loss value: 375.30786195834116\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 98\n",
      "Current point: [-2.44466774 -0.03942236  1.11216437]\n",
      "Gradient: [ 0.44447653  1.30176158 -0.80948328]\n",
      "Loss value: 375.30267792618315\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 99\n",
      "Current point: [-2.44555669 -0.04202588  1.11378334]\n",
      "Gradient: [ 0.42524039  1.29607536 -0.79359915]\n",
      "Loss value: 375.29761192724203\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 100\n",
      "Current point: [-2.44640717 -0.04461803  1.11537053]\n",
      "Gradient: [ 0.40683469  1.29027885 -0.77826352]\n",
      "Loss value: 375.29265855884637\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 101\n",
      "Current point: [-2.44722084 -0.04719859  1.11692706]\n",
      "Gradient: [ 0.38922336  1.28437927 -0.76345362]\n",
      "Loss value: 375.2878128079095\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 102\n",
      "Current point: [-2.44799929 -0.04976735  1.11845397]\n",
      "Gradient: [ 0.37237191  1.27838352 -0.7491477 ]\n",
      "Loss value: 375.28307001895735\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 103\n",
      "Current point: [-2.44874403 -0.05232411  1.11995226]\n",
      "Gradient: [ 0.35624738  1.27229817 -0.73532494]\n",
      "Loss value: 375.27842586486736\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 104\n",
      "Current point: [-2.44945653 -0.05486871  1.12142291]\n",
      "Gradient: [ 0.34081827  1.26612949 -0.72196543]\n",
      "Loss value: 375.27387632008237\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 105\n",
      "Current point: [-2.45013816 -0.05740097  1.12286684]\n",
      "Gradient: [ 0.32605447  1.25988348 -0.70905015]\n",
      "Loss value: 375.2694176360883\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 106\n",
      "Current point: [-2.45079027 -0.05992073  1.12428495]\n",
      "Gradient: [ 0.3119272   1.25356584 -0.69656087]\n",
      "Loss value: 375.26504631896046\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 107\n",
      "Current point: [-2.45141413 -0.06242787  1.12567807]\n",
      "Gradient: [ 0.29840894  1.24718202 -0.68448017]\n",
      "Loss value: 375.26075910880087\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 108\n",
      "Current point: [-2.45201094 -0.06492223  1.12704703]\n",
      "Gradient: [ 0.28547339  1.24073722 -0.67279139]\n",
      "Loss value: 375.2565529609054\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 109\n",
      "Current point: [-2.45258189 -0.0674037   1.12839261]\n",
      "Gradient: [ 0.27309538  1.23423639 -0.66147858]\n",
      "Loss value: 375.25242502851233\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 110\n",
      "Current point: [-2.45312808 -0.06987218  1.12971557]\n",
      "Gradient: [ 0.26125087  1.22768427 -0.65052646]\n",
      "Loss value: 375.24837264699715\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 111\n",
      "Current point: [-2.45365058 -0.07232755  1.13101662]\n",
      "Gradient: [ 0.24991685  1.22108535 -0.63992045]\n",
      "Loss value: 375.2443933193913\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 112\n",
      "Current point: [-2.45415042 -0.07476972  1.13229646]\n",
      "Gradient: [ 0.23907132  1.21444394 -0.62964656]\n",
      "Loss value: 375.2404847031113\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 113\n",
      "Current point: [-2.45462856 -0.0771986   1.13355575]\n",
      "Gradient: [ 0.22869325  1.20776415 -0.61969141]\n",
      "Loss value: 375.236644597795\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 114\n",
      "Current point: [-2.45508595 -0.07961413  1.13479514]\n",
      "Gradient: [ 0.21876251  1.20104986 -0.6100422 ]\n",
      "Loss value: 375.2328709341529\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 115\n",
      "Current point: [-2.45552347 -0.08201623  1.13601522]\n",
      "Gradient: [ 0.20925985  1.19430482 -0.60068669]\n",
      "Loss value: 375.229161763746\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 116\n",
      "Current point: [-2.45594199 -0.08440484  1.13721659]\n",
      "Gradient: [ 0.20016687  1.18753256 -0.59161312]\n",
      "Loss value: 375.2255152496136\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 117\n",
      "Current point: [-2.45634233 -0.08677991  1.13839982]\n",
      "Gradient: [ 0.19146594  1.18073648 -0.58281028]\n",
      "Loss value: 375.22192965767806\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 118\n",
      "Current point: [-2.45672526 -0.08914138  1.13956544]\n",
      "Gradient: [ 0.18314023  1.17391979 -0.57426741]\n",
      "Loss value: 375.21840334886167\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 119\n",
      "Current point: [-2.45709154 -0.09148922  1.14071398]\n",
      "Gradient: [ 0.1751736   1.16708556 -0.56597422]\n",
      "Loss value: 375.21493477185516\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 120\n",
      "Current point: [-2.45744188 -0.09382339  1.14184592]\n",
      "Gradient: [ 0.16755065  1.16023672 -0.55792085]\n",
      "Loss value: 375.2115224564824\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 121\n",
      "Current point: [-2.45777699 -0.09614386  1.14296177]\n",
      "Gradient: [ 0.16025661  1.15337605 -0.55009787]\n",
      "Loss value: 375.2081650076125\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 122\n",
      "Current point: [-2.4580975  -0.09845062  1.14406196]\n",
      "Gradient: [ 0.15327737  1.14650621 -0.54249623]\n",
      "Loss value: 375.20486109957164\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 123\n",
      "Current point: [-2.45840405 -0.10074363  1.14514695]\n",
      "Gradient: [ 0.14659942  1.13962971 -0.53510728]\n",
      "Loss value: 375.2016094710141\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 124\n",
      "Current point: [-2.45869725 -0.10302289  1.14621717]\n",
      "Gradient: [ 0.14020983  1.13274895 -0.52792274]\n",
      "Loss value: 375.1984089202125\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 125\n",
      "Current point: [-2.45897767 -0.10528839  1.14727301]\n",
      "Gradient: [ 0.13409623  1.12586623 -0.52093465]\n",
      "Loss value: 375.19525830073366\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 126\n",
      "Current point: [-2.45924587 -0.10754012  1.14831488]\n",
      "Gradient: [ 0.12824679  1.11898371 -0.51413543]\n",
      "Loss value: 375.1921565174663\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 127\n",
      "Current point: [-2.45950236 -0.10977809  1.14934315]\n",
      "Gradient: [ 0.12265017  1.11210345 -0.50751779]\n",
      "Loss value: 375.1891025229722\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 128\n",
      "Current point: [-2.45974766 -0.11200229  1.15035819]\n",
      "Gradient: [ 0.11729555  1.10522742 -0.50107474]\n",
      "Loss value: 375.1860953141335\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 129\n",
      "Current point: [-2.45998225 -0.11421275  1.15136034]\n",
      "Gradient: [ 0.11217254  1.09835749 -0.49479961]\n",
      "Loss value: 375.1831339290711\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 130\n",
      "Current point: [-2.4602066  -0.11640946  1.15234994]\n",
      "Gradient: [ 0.10727122  1.09149543 -0.488686  ]\n",
      "Loss value: 375.1802174443113\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 131\n",
      "Current point: [-2.46042114 -0.11859245  1.15332731]\n",
      "Gradient: [ 0.10258208  1.08464291 -0.48272777]\n",
      "Loss value: 375.1773449721816\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 132\n",
      "Current point: [-2.4606263  -0.12076174  1.15429277]\n",
      "Gradient: [ 0.09809603  1.07780154 -0.47691904]\n",
      "Loss value: 375.17451565841407\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 133\n",
      "Current point: [-2.46082249 -0.12291734  1.1552466 ]\n",
      "Gradient: [ 0.09380437  1.07097283 -0.47125419]\n",
      "Loss value: 375.1717286799415\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 134\n",
      "Current point: [-2.4610101  -0.12505929  1.15618911]\n",
      "Gradient: [ 0.08969876  1.06415823 -0.46572782]\n",
      "Loss value: 375.1689832428685\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 135\n",
      "Current point: [-2.4611895  -0.12718761  1.15712057]\n",
      "Gradient: [ 0.08577122  1.05735908 -0.46033475]\n",
      "Loss value: 375.16627858060355\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 136\n",
      "Current point: [-2.46136104 -0.12930232  1.15804124]\n",
      "Gradient: [ 0.08201414  1.05057669 -0.45507003]\n",
      "Loss value: 375.1636139521397\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 137\n",
      "Current point: [-2.46152507 -0.13140348  1.15895138]\n",
      "Gradient: [ 0.07842019  1.04381227 -0.44992892]\n",
      "Loss value: 375.16098864046967\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 138\n",
      "Current point: [-2.46168191 -0.1334911   1.15985124]\n",
      "Gradient: [ 0.07498239  1.03706699 -0.44490687]\n",
      "Loss value: 375.1584019511258\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 139\n",
      "Current point: [-2.46183188 -0.13556524  1.16074105]\n",
      "Gradient: [ 0.07169404  1.03034193 -0.43999951]\n",
      "Loss value: 375.15585321083404\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 140\n",
      "Current point: [-2.46197526 -0.13762592  1.16162105]\n",
      "Gradient: [ 0.06854873  1.02363813 -0.43520265]\n",
      "Loss value: 375.1533417662721\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 141\n",
      "Current point: [-2.46211236 -0.1396732   1.16249145]\n",
      "Gradient: [ 0.06554033  1.01695658 -0.4305123 ]\n",
      "Loss value: 375.15086698292413\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 142\n",
      "Current point: [-2.46224344 -0.14170711  1.16335248]\n",
      "Gradient: [ 0.06266296  1.01029818 -0.42592462]\n",
      "Loss value: 375.14842824402297\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 143\n",
      "Current point: [-2.46236877 -0.1437277   1.16420433]\n",
      "Gradient: [ 0.05991102  1.00366383 -0.42143591]\n",
      "Loss value: 375.1460249495733\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 144\n",
      "Current point: [-2.46248859 -0.14573503  1.1650472 ]\n",
      "Gradient: [ 0.05727911  0.99705432 -0.41704264]\n",
      "Loss value: 375.1436565154494\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 145\n",
      "Current point: [-2.46260315 -0.14772914  1.16588129]\n",
      "Gradient: [ 0.05476208  0.99047045 -0.41274144]\n",
      "Loss value: 375.1413223725604\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 146\n",
      "Current point: [-2.46271267 -0.14971008  1.16670677]\n",
      "Gradient: [ 0.052355    0.98391293 -0.40852906]\n",
      "Loss value: 375.1390219660784\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 147\n",
      "Current point: [-2.46281738 -0.15167791  1.16752383]\n",
      "Gradient: [ 0.05005316  0.97738245 -0.40440238]\n",
      "Loss value: 375.13675475472365\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 148\n",
      "Current point: [-2.46291749 -0.15363267  1.16833263]\n",
      "Gradient: [ 0.04785203  0.97087966 -0.40035841]\n",
      "Loss value: 375.13452021010335\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 149\n",
      "Current point: [-2.46301319 -0.15557443  1.16913335]\n",
      "Gradient: [ 0.04574729  0.96440515 -0.3963943 ]\n",
      "Loss value: 375.1323178160973\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 150\n",
      "Current point: [-2.46310469 -0.15750324  1.16992614]\n",
      "Gradient: [ 0.0437348   0.95795949 -0.3925073 ]\n",
      "Loss value: 375.1301470682897\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 151\n",
      "Current point: [-2.46319216 -0.15941916  1.17071115]\n",
      "Gradient: [ 0.04181058  0.9515432  -0.38869478]\n",
      "Loss value: 375.12800747344085\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 152\n",
      "Current point: [-2.46327578 -0.16132225  1.17148854]\n",
      "Gradient: [ 0.03997086  0.94515679 -0.3849542 ]\n",
      "Loss value: 375.12589854899613\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 153\n",
      "Current point: [-2.46335572 -0.16321256  1.17225845]\n",
      "Gradient: [ 0.03821198  0.93880071 -0.38128315]\n",
      "Loss value: 375.1238198226308\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 154\n",
      "Current point: [-2.46343214 -0.16509016  1.17302102]\n",
      "Gradient: [ 0.03653048  0.93247539 -0.3776793 ]\n",
      "Loss value: 375.12177083182524\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 155\n",
      "Current point: [-2.4635052  -0.16695511  1.17377637]\n",
      "Gradient: [ 0.03492302  0.92618122 -0.37414042]\n",
      "Loss value: 375.11975112346977\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 156\n",
      "Current point: [-2.46357505 -0.16880748  1.17452465]\n",
      "Gradient: [ 0.03338642  0.91991859 -0.37066437]\n",
      "Loss value: 375.11776025349667\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 157\n",
      "Current point: [-2.46364182 -0.17064731  1.17526598]\n",
      "Gradient: [ 0.03191762  0.91368782 -0.3672491 ]\n",
      "Loss value: 375.1157977865365\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 158\n",
      "Current point: [-2.46370566 -0.17247469  1.17600048]\n",
      "Gradient: [ 0.03051371  0.90748923 -0.36389263]\n",
      "Loss value: 375.1138632955974\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 159\n",
      "Current point: [-2.46376669 -0.17428967  1.17672827]\n",
      "Gradient: [ 0.02917187  0.90132312 -0.36059308]\n",
      "Loss value: 375.111956361765\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 160\n",
      "Current point: [-2.46382503 -0.17609231  1.17744945]\n",
      "Gradient: [ 0.02788945  0.89518975 -0.35734863]\n",
      "Loss value: 375.1100765739217\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 161\n",
      "Current point: [-2.46388081 -0.17788269  1.17816415]\n",
      "Gradient: [ 0.02666387  0.88908936 -0.35415753]\n",
      "Loss value: 375.1082235284839\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 162\n",
      "Current point: [-2.46393414 -0.17966087  1.17887247]\n",
      "Gradient: [ 0.02549267  0.88302217 -0.35101812]\n",
      "Loss value: 375.10639682915564\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 163\n",
      "Current point: [-2.46398512 -0.18142692  1.1795745 ]\n",
      "Gradient: [ 0.02437352  0.87698839 -0.34792878]\n",
      "Loss value: 375.10459608669663\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 164\n",
      "Current point: [-2.46403387 -0.18318089  1.18027036]\n",
      "Gradient: [ 0.02330416  0.87098818 -0.34488798]\n",
      "Loss value: 375.1028209187049\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 165\n",
      "Current point: [-2.46408048 -0.18492287  1.18096014]\n",
      "Gradient: [ 0.02228243  0.86502173 -0.34189424]\n",
      "Loss value: 375.10107094941196\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 166\n",
      "Current point: [-2.46412504 -0.18665291  1.18164392]\n",
      "Gradient: [ 0.02130628  0.85908916 -0.33894612]\n",
      "Loss value: 375.09934580948976\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 167\n",
      "Current point: [-2.46416765 -0.18837109  1.18232182]\n",
      "Gradient: [ 0.02037373  0.85319061 -0.33604228]\n",
      "Loss value: 375.0976451358686\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 168\n",
      "Current point: [-2.4642084  -0.19007747  1.1829939 ]\n",
      "Gradient: [ 0.0194829   0.84732617 -0.33318139]\n",
      "Loss value: 375.09596857156527\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 169\n",
      "Current point: [-2.46424737 -0.19177212  1.18366026]\n",
      "Gradient: [ 0.01863197  0.84149596 -0.33036219]\n",
      "Loss value: 375.09431576552004\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 170\n",
      "Current point: [-2.46428463 -0.19345512  1.18432099]\n",
      "Gradient: [ 0.01781921  0.83570004 -0.32758348]\n",
      "Loss value: 375.0926863724428\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 171\n",
      "Current point: [-2.46432027 -0.19512652  1.18497615]\n",
      "Gradient: [ 0.01704297  0.82993847 -0.32484409]\n",
      "Loss value: 375.09108005266705\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 172\n",
      "Current point: [-2.46435436 -0.19678639  1.18562584]\n",
      "Gradient: [ 0.01630166  0.82421132 -0.3221429 ]\n",
      "Loss value: 375.0894964720111\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 173\n",
      "Current point: [-2.46438696 -0.19843482  1.18627013]\n",
      "Gradient: [ 0.01559376  0.81851862 -0.31947884]\n",
      "Loss value: 375.08793530164564\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 174\n",
      "Current point: [-2.46441815 -0.20007185  1.18690909]\n",
      "Gradient: [ 0.01491782  0.81286039 -0.31685088]\n",
      "Loss value: 375.0863962179689\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 175\n",
      "Current point: [-2.46444798 -0.20169757  1.18754279]\n",
      "Gradient: [ 0.01427245  0.80723664 -0.31425803]\n",
      "Loss value: 375.0848789024866\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 176\n",
      "Current point: [-2.46447653 -0.20331205  1.1881713 ]\n",
      "Gradient: [ 0.01365631  0.80164739 -0.31169932]\n",
      "Loss value: 375.08338304169763\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 177\n",
      "Current point: [-2.46450384 -0.20491534  1.1887947 ]\n",
      "Gradient: [ 0.01306813  0.79609261 -0.30917384]\n",
      "Loss value: 375.0819083269847\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 178\n",
      "Current point: [-2.46452998 -0.20650753  1.18941305]\n",
      "Gradient: [ 0.01250668  0.7905723  -0.30668071]\n",
      "Loss value: 375.0804544545101\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 179\n",
      "Current point: [-2.46455499 -0.20808867  1.19002641]\n",
      "Gradient: [ 0.01197081  0.78508643 -0.30421909]\n",
      "Loss value: 375.0790211251155\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 180\n",
      "Current point: [-2.46457893 -0.20965884  1.19063485]\n",
      "Gradient: [ 0.01145939  0.77963495 -0.30178814]\n",
      "Loss value: 375.0776080442257\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 181\n",
      "Current point: [-2.46460185 -0.21121811  1.19123843]\n",
      "Gradient: [ 0.01097135  0.77421783 -0.2993871 ]\n",
      "Loss value: 375.07621492175645\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 182\n",
      "Current point: [-2.46462379 -0.21276655  1.1918372 ]\n",
      "Gradient: [ 0.01050567  0.76883501 -0.2970152 ]\n",
      "Loss value: 375.0748414720257\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 183\n",
      "Current point: [-2.4646448  -0.21430422  1.19243123]\n",
      "Gradient: [ 0.01006136  0.76348642 -0.29467171]\n",
      "Loss value: 375.07348741366815\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 184\n",
      "Current point: [-2.46466493 -0.21583119  1.19302057]\n",
      "Gradient: [ 0.00963749  0.758172   -0.29235594]\n",
      "Loss value: 375.07215246955286\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 185\n",
      "Current point: [-2.4646842  -0.21734754  1.19360529]\n",
      "Gradient: [ 0.00923316  0.75289168 -0.29006721]\n",
      "Loss value: 375.070836366704\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 186\n",
      "Current point: [-2.46470267 -0.21885332  1.19418542]\n",
      "Gradient: [ 0.00884751  0.74764537 -0.28780487]\n",
      "Loss value: 375.0695388362236\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 187\n",
      "Current point: [-2.46472036 -0.22034861  1.19476103]\n",
      "Gradient: [ 0.00847971  0.74243298 -0.2855683 ]\n",
      "Loss value: 375.06825961321783\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 188\n",
      "Current point: [-2.46473732 -0.22183348  1.19533217]\n",
      "Gradient: [ 0.00812899  0.73725441 -0.2833569 ]\n",
      "Loss value: 375.066998436725\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 189\n",
      "Current point: [-2.46475358 -0.22330799  1.19589888]\n",
      "Gradient: [ 0.00779457  0.73210957 -0.28117008]\n",
      "Loss value: 375.0657550496458\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 190\n",
      "Current point: [-2.46476917 -0.22477221  1.19646122]\n",
      "Gradient: [ 0.00747575  0.72699836 -0.27900729]\n",
      "Loss value: 375.0645291986759\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 191\n",
      "Current point: [-2.46478412 -0.2262262   1.19701924]\n",
      "Gradient: [ 0.00717184  0.72192065 -0.27686799]\n",
      "Loss value: 375.0633206342405\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 192\n",
      "Current point: [-2.46479847 -0.22767004  1.19757297]\n",
      "Gradient: [ 0.00688216  0.71687634 -0.27475165]\n",
      "Loss value: 375.06212911043053\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 193\n",
      "Current point: [-2.46481223 -0.2291038   1.19812247]\n",
      "Gradient: [ 0.0066061   0.7118653  -0.27265779]\n",
      "Loss value: 375.06095438494106\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 194\n",
      "Current point: [-2.46482544 -0.23052753  1.19866779]\n",
      "Gradient: [ 0.00634304  0.70688741 -0.27058591]\n",
      "Loss value: 375.0597962190105\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 195\n",
      "Current point: [-2.46483813 -0.2319413   1.19920896]\n",
      "Gradient: [ 0.00609241  0.70194254 -0.26853555]\n",
      "Loss value: 375.0586543773627\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 196\n",
      "Current point: [-2.46485031 -0.23334519  1.19974603]\n",
      "Gradient: [ 0.00585365  0.69703056 -0.26650627]\n",
      "Loss value: 375.0575286281494\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 197\n",
      "Current point: [-2.46486202 -0.23473925  1.20027905]\n",
      "Gradient: [ 0.00562623  0.69215134 -0.26449762]\n",
      "Loss value: 375.05641874289427\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 198\n",
      "Current point: [-2.46487327 -0.23612355  1.20080804]\n",
      "Gradient: [ 0.00540964  0.68730473 -0.2625092 ]\n",
      "Loss value: 375.05532449643925\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 199\n",
      "Current point: [-2.46488409 -0.23749816  1.20133306]\n",
      "Gradient: [ 0.00520341  0.68249059 -0.26054059]\n",
      "Loss value: 375.0542456668909\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 200\n",
      "Current point: [-2.4648945  -0.23886314  1.20185414]\n",
      "Gradient: [ 0.00500705  0.67770878 -0.25859142]\n",
      "Loss value: 375.0531820355687\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 201\n",
      "Current point: [-2.46490451 -0.24021856  1.20237132]\n",
      "Gradient: [ 0.00482014  0.67295916 -0.25666129]\n",
      "Loss value: 375.0521333869545\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 202\n",
      "Current point: [-2.46491415 -0.24156448  1.20288465]\n",
      "Gradient: [ 0.00464224  0.66824156 -0.25474986]\n",
      "Loss value: 375.05109950864284\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 203\n",
      "Current point: [-2.46492344 -0.24290096  1.20339415]\n",
      "Gradient: [ 0.00447295  0.66355584 -0.25285676]\n",
      "Loss value: 375.0500801912924\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 204\n",
      "Current point: [-2.46493238 -0.24422807  1.20389986]\n",
      "Gradient: [ 0.00431188  0.65890184 -0.25098166]\n",
      "Loss value: 375.0490752285788\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 205\n",
      "Current point: [-2.46494101 -0.24554588  1.20440182]\n",
      "Gradient: [ 0.00415866  0.65427941 -0.24912423]\n",
      "Loss value: 375.0480844171478\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 206\n",
      "Current point: [-2.46494932 -0.24685443  1.20490007]\n",
      "Gradient: [ 0.00401293  0.64968838 -0.24728416]\n",
      "Loss value: 375.04710755657\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 207\n",
      "Current point: [-2.46495735 -0.24815381  1.20539464]\n",
      "Gradient: [ 0.00387435  0.64512861 -0.24546113]\n",
      "Loss value: 375.046144449296\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 208\n",
      "Current point: [-2.4649651  -0.24944407  1.20588556]\n",
      "Gradient: [ 0.00374259  0.64059991 -0.24365485]\n",
      "Loss value: 375.0451949006129\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 209\n",
      "Current point: [-2.46497258 -0.25072527  1.20637287]\n",
      "Gradient: [ 0.00361734  0.63610214 -0.24186504]\n",
      "Loss value: 375.0442587186011\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 210\n",
      "Current point: [-2.46497982 -0.25199747  1.2068566 ]\n",
      "Gradient: [ 0.00349831  0.63163513 -0.24009141]\n",
      "Loss value: 375.04333571409256\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 211\n",
      "Current point: [-2.46498682 -0.25326074  1.20733678]\n",
      "Gradient: [ 0.00338521  0.6271987  -0.2383337 ]\n",
      "Loss value: 375.042425700629\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 212\n",
      "Current point: [-2.46499359 -0.25451514  1.20781345]\n",
      "Gradient: [ 0.00327776  0.62279269 -0.23659164]\n",
      "Loss value: 375.04152849442164\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 213\n",
      "Current point: [-2.46500014 -0.25576073  1.20828663]\n",
      "Gradient: [ 0.00317571  0.61841694 -0.23486499]\n",
      "Loss value: 375.0406439143114\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 214\n",
      "Current point: [-2.46500649 -0.25699756  1.20875636]\n",
      "Gradient: [ 0.00307881  0.61407128 -0.23315349]\n",
      "Loss value: 375.03977178172966\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 215\n",
      "Current point: [-2.46501265 -0.2582257   1.20922267]\n",
      "Gradient: [ 0.00298681  0.60975553 -0.23145692]\n",
      "Loss value: 375.03891192065976\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 216\n",
      "Current point: [-2.46501862 -0.25944521  1.20968559]\n",
      "Gradient: [ 0.00289949  0.60546952 -0.22977503]\n",
      "Loss value: 375.03806415759965\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 217\n",
      "Current point: [-2.46502442 -0.26065615  1.21014514]\n",
      "Gradient: [ 0.00281664  0.60121308 -0.22810762]\n",
      "Loss value: 375.037228321524\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 218\n",
      "Current point: [-2.46503006 -0.26185858  1.21060135]\n",
      "Gradient: [ 0.00273803  0.59698604 -0.22645446]\n",
      "Loss value: 375.03640424384866\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 219\n",
      "Current point: [-2.46503553 -0.26305255  1.21105426]\n",
      "Gradient: [ 0.00266349  0.59278823 -0.22481534]\n",
      "Loss value: 375.0355917583939\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 220\n",
      "Current point: [-2.46504086 -0.26423813  1.21150389]\n",
      "Gradient: [ 0.0025928   0.58861948 -0.22319005]\n",
      "Loss value: 375.0347907013497\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 221\n",
      "Current point: [-2.46504604 -0.26541537  1.21195027]\n",
      "Gradient: [ 0.00252579  0.5844796  -0.22157841]\n",
      "Loss value: 375.03400091124104\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 222\n",
      "Current point: [-2.4650511  -0.26658432  1.21239343]\n",
      "Gradient: [ 0.00246229  0.58036843 -0.21998022]\n",
      "Loss value: 375.03322222889335\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 223\n",
      "Current point: [-2.46505602 -0.26774506  1.21283339]\n",
      "Gradient: [ 0.00240213  0.5762858  -0.21839529]\n",
      "Loss value: 375.0324544973994\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 224\n",
      "Current point: [-2.46506083 -0.26889763  1.21327018]\n",
      "Gradient: [ 0.00234516  0.57223152 -0.21682344]\n",
      "Loss value: 375.03169756208604\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 225\n",
      "Current point: [-2.46506552 -0.2700421   1.21370383]\n",
      "Gradient: [ 0.00229121  0.56820543 -0.21526449]\n",
      "Loss value: 375.03095127048186\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 226\n",
      "Current point: [-2.4650701  -0.27117851  1.21413435]\n",
      "Gradient: [ 0.00224014  0.56420734 -0.21371828]\n",
      "Loss value: 375.0302154722849\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 227\n",
      "Current point: [-2.46507458 -0.27230692  1.21456179]\n",
      "Gradient: [ 0.00219182  0.5602371  -0.21218463]\n",
      "Loss value: 375.02949001933155\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 228\n",
      "Current point: [-2.46507896 -0.2734274   1.21498616]\n",
      "Gradient: [ 0.00214611  0.55629452 -0.21066338]\n",
      "Loss value: 375.0287747655652\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 229\n",
      "Current point: [-2.46508325 -0.27453998  1.21540749]\n",
      "Gradient: [ 0.00210288  0.55237942 -0.20915438]\n",
      "Loss value: 375.0280695670059\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 230\n",
      "Current point: [-2.46508746 -0.27564474  1.2158258 ]\n",
      "Gradient: [ 0.00206202  0.54849165 -0.20765747]\n",
      "Loss value: 375.02737428172054\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 231\n",
      "Current point: [-2.46509158 -0.27674173  1.21624111]\n",
      "Gradient: [ 0.0020234   0.54463101 -0.20617249]\n",
      "Loss value: 375.0266887697929\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 232\n",
      "Current point: [-2.46509563 -0.27783099  1.21665346]\n",
      "Gradient: [ 0.00198692  0.54079734 -0.20469932]\n",
      "Loss value: 375.0260128932947\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 233\n",
      "Current point: [-2.4650996  -0.27891258  1.21706285]\n",
      "Gradient: [ 0.00195248  0.53699047 -0.20323779]\n",
      "Loss value: 375.0253465162569\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 234\n",
      "Current point: [-2.46510351 -0.27998656  1.21746933]\n",
      "Gradient: [ 0.00191996  0.53321022 -0.20178777]\n",
      "Loss value: 375.0246895046416\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 235\n",
      "Current point: [-2.46510735 -0.28105299  1.21787291]\n",
      "Gradient: [ 0.00188928  0.52945643 -0.20034913]\n",
      "Loss value: 375.024041726314\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 236\n",
      "Current point: [-2.46511113 -0.2821119   1.2182736 ]\n",
      "Gradient: [ 0.00186034  0.52572891 -0.19892174]\n",
      "Loss value: 375.0234030510153\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 237\n",
      "Current point: [-2.46511485 -0.28316336  1.21867145]\n",
      "Gradient: [ 0.00183305  0.5220275  -0.19750546]\n",
      "Loss value: 375.02277335033534\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 238\n",
      "Current point: [-2.46511851 -0.28420741  1.21906646]\n",
      "Gradient: [ 0.00180733  0.51835203 -0.19610017]\n",
      "Loss value: 375.02215249768653\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 239\n",
      "Current point: [-2.46512213 -0.28524411  1.21945866]\n",
      "Gradient: [ 0.00178311  0.51470233 -0.19470575]\n",
      "Loss value: 375.0215403682771\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 240\n",
      "Current point: [-2.4651257  -0.28627352  1.21984807]\n",
      "Gradient: [ 0.00176029  0.51107823 -0.19332207]\n",
      "Loss value: 375.02093683908583\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 241\n",
      "Current point: [-2.46512922 -0.28729568  1.22023471]\n",
      "Gradient: [ 0.00173882  0.50747955 -0.19194902]\n",
      "Loss value: 375.0203417888363\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 242\n",
      "Current point: [-2.46513269 -0.28831064  1.22061861]\n",
      "Gradient: [ 0.00171861  0.50390614 -0.19058648]\n",
      "Loss value: 375.01975509797205\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 243\n",
      "Current point: [-2.46513613 -0.28931845  1.22099979]\n",
      "Gradient: [ 0.00169961  0.50035781 -0.18923434]\n",
      "Loss value: 375.0191766486319\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 244\n",
      "Current point: [-2.46513953 -0.29031916  1.22137825]\n",
      "Gradient: [ 0.00168176  0.49683442 -0.1878925 ]\n",
      "Loss value: 375.0186063246257\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 245\n",
      "Current point: [-2.46514289 -0.29131283  1.22175404]\n",
      "Gradient: [ 0.00166498  0.49333578 -0.18656083]\n",
      "Loss value: 375.0180440114102\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 246\n",
      "Current point: [-2.46514622 -0.2922995   1.22212716]\n",
      "Gradient: [ 0.00164922  0.48986173 -0.18523925]\n",
      "Loss value: 375.01748959606573\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 247\n",
      "Current point: [-2.46514952 -0.29327923  1.22249764]\n",
      "Gradient: [ 0.00163443  0.48641212 -0.18392763]\n",
      "Loss value: 375.0169429672728\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 248\n",
      "Current point: [-2.46515279 -0.29425205  1.22286549]\n",
      "Gradient: [ 0.00162055  0.48298676 -0.18262589]\n",
      "Loss value: 375.01640401528937\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 249\n",
      "Current point: [-2.46515603 -0.29521802  1.22323075]\n",
      "Gradient: [ 0.00160754  0.4795855  -0.18133393]\n",
      "Loss value: 375.015872631928\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 250\n",
      "Current point: [-2.46515925 -0.2961772   1.22359341]\n",
      "Gradient: [ 0.00159535  0.47620818 -0.18005164]\n",
      "Loss value: 375.015348710534\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 251\n",
      "Current point: [-2.46516244 -0.29712961  1.22395352]\n",
      "Gradient: [ 0.00158392  0.47285464 -0.17877893]\n",
      "Loss value: 375.0148321459633\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 252\n",
      "Current point: [-2.46516561 -0.29807532  1.22431108]\n",
      "Gradient: [ 0.00157323  0.4695247  -0.17751571]\n",
      "Loss value: 375.0143228345609\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 253\n",
      "Current point: [-2.46516875 -0.29901437  1.22466611]\n",
      "Gradient: [ 0.00156322  0.46621822 -0.17626189]\n",
      "Loss value: 375.01382067413965\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 254\n",
      "Current point: [-2.46517188 -0.29994681  1.22501863]\n",
      "Gradient: [ 0.00155385  0.46293503 -0.17501737]\n",
      "Loss value: 375.01332556395926\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 255\n",
      "Current point: [-2.46517499 -0.30087268  1.22536866]\n",
      "Gradient: [ 0.0015451   0.45967497 -0.17378208]\n",
      "Loss value: 375.0128374047056\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 256\n",
      "Current point: [-2.46517808 -0.30179203  1.22571623]\n",
      "Gradient: [ 0.00153692  0.45643789 -0.17255591]\n",
      "Loss value: 375.0123560984705\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 257\n",
      "Current point: [-2.46518115 -0.3027049   1.22606134]\n",
      "Gradient: [ 0.00152928  0.45322362 -0.1713388 ]\n",
      "Loss value: 375.0118815487315\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 258\n",
      "Current point: [-2.46518421 -0.30361135  1.22640402]\n",
      "Gradient: [ 0.00152214  0.45003201 -0.17013064]\n",
      "Loss value: 375.01141366033227\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 259\n",
      "Current point: [-2.46518725 -0.30451141  1.22674428]\n",
      "Gradient: [ 0.00151548  0.4468629  -0.16893137]\n",
      "Loss value: 375.0109523394631\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 260\n",
      "Current point: [-2.46519028 -0.30540514  1.22708214]\n",
      "Gradient: [ 0.00150927  0.44371615 -0.1677409 ]\n",
      "Loss value: 375.0104974936414\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 261\n",
      "Current point: [-2.4651933  -0.30629257  1.22741762]\n",
      "Gradient: [ 0.00150347  0.44059159 -0.16655915]\n",
      "Loss value: 375.0100490316933\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 262\n",
      "Current point: [-2.46519631 -0.30717376  1.22775074]\n",
      "Gradient: [ 0.00149807  0.43748907 -0.16538604]\n",
      "Loss value: 375.0096068637347\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 263\n",
      "Current point: [-2.46519931 -0.30804873  1.22808151]\n",
      "Gradient: [ 0.00149303  0.43440844 -0.1642215 ]\n",
      "Loss value: 375.00917090115274\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 264\n",
      "Current point: [-2.46520229 -0.30891755  1.22840996]\n",
      "Gradient: [ 0.00148833  0.43134954 -0.16306545]\n",
      "Loss value: 375.008741056588\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 265\n",
      "Current point: [-2.46520527 -0.30978025  1.22873609]\n",
      "Gradient: [ 0.00148396  0.42831224 -0.16191781]\n",
      "Loss value: 375.0083172439165\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 266\n",
      "Current point: [-2.46520824 -0.31063687  1.22905992]\n",
      "Gradient: [ 0.00147988  0.42529637 -0.16077852]\n",
      "Loss value: 375.0078993782323\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 267\n",
      "Current point: [-2.4652112  -0.31148747  1.22938148]\n",
      "Gradient: [ 0.00147608  0.42230178 -0.15964751]\n",
      "Loss value: 375.00748737582967\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 268\n",
      "Current point: [-2.46521415 -0.31233207  1.22970078]\n",
      "Gradient: [ 0.00147253  0.41932834 -0.15852469]\n",
      "Loss value: 375.0070811541866\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 269\n",
      "Current point: [-2.46521709 -0.31317073  1.23001783]\n",
      "Gradient: [ 0.00146923  0.41637589 -0.15741   ]\n",
      "Loss value: 375.0066806319476\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 270\n",
      "Current point: [-2.46522003 -0.31400348  1.23033265]\n",
      "Gradient: [ 0.00146614  0.41344429 -0.15630337]\n",
      "Loss value: 375.00628572890696\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 271\n",
      "Current point: [-2.46522296 -0.31483037  1.23064525]\n",
      "Gradient: [ 0.00146326  0.41053338 -0.15520474]\n",
      "Loss value: 375.0058963659931\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 272\n",
      "Current point: [-2.46522589 -0.31565143  1.23095566]\n",
      "Gradient: [ 0.00146057  0.40764303 -0.15411404]\n",
      "Loss value: 375.00551246525157\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 273\n",
      "Current point: [-2.46522881 -0.31646672  1.23126389]\n",
      "Gradient: [ 0.00145805  0.4047731  -0.15303119]\n",
      "Loss value: 375.00513394982954\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 274\n",
      "Current point: [-2.46523173 -0.31727627  1.23156995]\n",
      "Gradient: [ 0.00145569  0.40192343 -0.15195614]\n",
      "Loss value: 375.00476074396033\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 275\n",
      "Current point: [-2.46523464 -0.31808011  1.23187386]\n",
      "Gradient: [ 0.00145347  0.39909389 -0.15088882]\n",
      "Loss value: 375.00439277294777\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 276\n",
      "Current point: [-2.46523755 -0.3188783   1.23217564]\n",
      "Gradient: [ 0.00145138  0.39628434 -0.14982917]\n",
      "Loss value: 375.00402996315074\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 277\n",
      "Current point: [-2.46524045 -0.31967087  1.2324753 ]\n",
      "Gradient: [ 0.00144942  0.39349464 -0.14877713]\n",
      "Loss value: 375.0036722419685\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 278\n",
      "Current point: [-2.46524335 -0.32045786  1.23277285]\n",
      "Gradient: [ 0.00144756  0.39072464 -0.14773263]\n",
      "Loss value: 375.00331953782614\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 279\n",
      "Current point: [-2.46524624 -0.32123931  1.23306832]\n",
      "Gradient: [ 0.0014458   0.38797421 -0.14669562]\n",
      "Loss value: 375.0029717801592\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 280\n",
      "Current point: [-2.46524913 -0.32201526  1.23336171]\n",
      "Gradient: [ 0.00144413  0.38524321 -0.14566602]\n",
      "Loss value: 375.0026288994001\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 281\n",
      "Current point: [-2.46525202 -0.32278574  1.23365304]\n",
      "Gradient: [ 0.00144253  0.38253151 -0.14464379]\n",
      "Loss value: 375.00229082696364\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 282\n",
      "Current point: [-2.46525491 -0.32355081  1.23394233]\n",
      "Gradient: [ 0.001441    0.37983896 -0.14362887]\n",
      "Loss value: 375.0019574952329\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 283\n",
      "Current point: [-2.46525779 -0.32431048  1.23422959]\n",
      "Gradient: [ 0.00143953  0.37716544 -0.14262119]\n",
      "Loss value: 375.0016288375459\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 284\n",
      "Current point: [-2.46526067 -0.32506481  1.23451483]\n",
      "Gradient: [ 0.00143811  0.37451082 -0.1416207 ]\n",
      "Loss value: 375.00130478818164\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 285\n",
      "Current point: [-2.46526355 -0.32581384  1.23479807]\n",
      "Gradient: [ 0.00143674  0.37187494 -0.14062735]\n",
      "Loss value: 375.0009852823469\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 286\n",
      "Current point: [-2.46526642 -0.32655759  1.23507933]\n",
      "Gradient: [ 0.0014354   0.3692577  -0.13964108]\n",
      "Loss value: 375.0006702561632\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 287\n",
      "Current point: [-2.46526929 -0.3272961   1.23535861]\n",
      "Gradient: [ 0.00143409  0.36665894 -0.13866182]\n",
      "Loss value: 375.0003596466535\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 288\n",
      "Current point: [-2.46527216 -0.32802942  1.23563593]\n",
      "Gradient: [ 0.0014328   0.36407855 -0.13768954]\n",
      "Loss value: 375.0000533917298\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 289\n",
      "Current point: [-2.46527502 -0.32875758  1.23591131]\n",
      "Gradient: [ 0.00143153  0.36151639 -0.13672416]\n",
      "Loss value: 374.99975143018\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 290\n",
      "Current point: [-2.46527789 -0.32948061  1.23618476]\n",
      "Gradient: [ 0.00143026  0.35897234 -0.13576565]\n",
      "Loss value: 374.9994537016561\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 291\n",
      "Current point: [-2.46528075 -0.33019855  1.23645629]\n",
      "Gradient: [ 0.00142901  0.35644627 -0.13481395]\n",
      "Loss value: 374.9991601466614\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 292\n",
      "Current point: [-2.46528361 -0.33091145  1.23672592]\n",
      "Gradient: [ 0.00142775  0.35393804 -0.133869  ]\n",
      "Loss value: 374.9988707065387\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 293\n",
      "Current point: [-2.46528646 -0.33161932  1.23699366]\n",
      "Gradient: [ 0.00142649  0.35144754 -0.13293076]\n",
      "Loss value: 374.99858532345854\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 294\n",
      "Current point: [-2.46528931 -0.33232222  1.23725952]\n",
      "Gradient: [ 0.00142522  0.34897464 -0.13199917]\n",
      "Loss value: 374.99830394040674\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 295\n",
      "Current point: [-2.46529216 -0.33302017  1.23752352]\n",
      "Gradient: [ 0.00142394  0.34651921 -0.13107418]\n",
      "Loss value: 374.99802650117385\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 296\n",
      "Current point: [-2.46529501 -0.33371321  1.23778567]\n",
      "Gradient: [ 0.00142264  0.34408113 -0.13015574]\n",
      "Loss value: 374.99775295034294\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 297\n",
      "Current point: [-2.46529786 -0.33440137  1.23804598]\n",
      "Gradient: [ 0.00142132  0.34166028 -0.12924381]\n",
      "Loss value: 374.9974832332788\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 298\n",
      "Current point: [-2.4653007  -0.33508469  1.23830447]\n",
      "Gradient: [ 0.00141998  0.33925654 -0.12833833]\n",
      "Loss value: 374.9972172961166\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 299\n",
      "Current point: [-2.46530354 -0.3357632   1.23856114]\n",
      "Gradient: [ 0.00141861  0.33686978 -0.12743925]\n",
      "Loss value: 374.9969550857512\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 300\n",
      "Current point: [-2.46530638 -0.33643694  1.23881602]\n",
      "Gradient: [ 0.00141721  0.33449988 -0.12654654]\n",
      "Loss value: 374.9966965498262\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 301\n",
      "Current point: [-2.46530921 -0.33710594  1.23906911]\n",
      "Gradient: [ 0.00141577  0.33214673 -0.12566013]\n",
      "Loss value: 374.9964416367234\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 302\n",
      "Current point: [-2.46531204 -0.33777023  1.23932043]\n",
      "Gradient: [ 0.0014143   0.32981021 -0.12477998]\n",
      "Loss value: 374.9961902955521\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 303\n",
      "Current point: [-2.46531487 -0.33842985  1.23956999]\n",
      "Gradient: [ 0.0014128   0.32749019 -0.12390605]\n",
      "Loss value: 374.9959424761395\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 304\n",
      "Current point: [-2.4653177  -0.33908484  1.23981781]\n",
      "Gradient: [ 0.00141126  0.32518657 -0.1230383 ]\n",
      "Loss value: 374.9956981290196\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 305\n",
      "Current point: [-2.46532052 -0.33973521  1.24006388]\n",
      "Gradient: [ 0.00140967  0.32289922 -0.12217666]\n",
      "Loss value: 374.99545720542386\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 306\n",
      "Current point: [-2.46532334 -0.34038101  1.24030824]\n",
      "Gradient: [ 0.00140804  0.32062803 -0.1213211 ]\n",
      "Loss value: 374.995219657271\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 307\n",
      "Current point: [-2.46532615 -0.34102226  1.24055088]\n",
      "Gradient: [ 0.00140637  0.31837288 -0.12047158]\n",
      "Loss value: 374.9949854371573\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 308\n",
      "Current point: [-2.46532897 -0.34165901  1.24079182]\n",
      "Gradient: [ 0.00140465  0.31613367 -0.11962805]\n",
      "Loss value: 374.99475449834745\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 309\n",
      "Current point: [-2.46533178 -0.34229128  1.24103108]\n",
      "Gradient: [ 0.00140288  0.31391027 -0.11879046]\n",
      "Loss value: 374.99452679476406\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 310\n",
      "Current point: [-2.46533458 -0.3429191   1.24126866]\n",
      "Gradient: [ 0.00140107  0.31170259 -0.11795878]\n",
      "Loss value: 374.99430228097935\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 311\n",
      "Current point: [-2.46533738 -0.3435425   1.24150458]\n",
      "Gradient: [ 0.00139921  0.30951049 -0.11713295]\n",
      "Loss value: 374.9940809122057\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 312\n",
      "Current point: [-2.46534018 -0.34416152  1.24173884]\n",
      "Gradient: [ 0.00139729  0.30733388 -0.11631294]\n",
      "Loss value: 374.9938626442861\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 313\n",
      "Current point: [-2.46534298 -0.34477619  1.24197147]\n",
      "Gradient: [ 0.00139533  0.30517265 -0.1154987 ]\n",
      "Loss value: 374.9936474336859\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 314\n",
      "Current point: [-2.46534577 -0.34538654  1.24220246]\n",
      "Gradient: [ 0.00139331  0.30302668 -0.11469019]\n",
      "Loss value: 374.9934352374836\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 315\n",
      "Current point: [-2.46534856 -0.34599259  1.24243184]\n",
      "Gradient: [ 0.00139124  0.30089586 -0.11388737]\n",
      "Loss value: 374.9932260133622\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 316\n",
      "Current point: [-2.46535134 -0.34659438  1.24265962]\n",
      "Gradient: [ 0.00138912  0.2987801  -0.1130902 ]\n",
      "Loss value: 374.9930197196007\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 317\n",
      "Current point: [-2.46535412 -0.34719194  1.2428858 ]\n",
      "Gradient: [ 0.00138694  0.29667928 -0.11229864]\n",
      "Loss value: 374.99281631506574\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 318\n",
      "Current point: [-2.46535689 -0.3477853   1.2431104 ]\n",
      "Gradient: [ 0.00138472  0.29459329 -0.11151264]\n",
      "Loss value: 374.99261575920315\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 319\n",
      "Current point: [-2.46535966 -0.34837449  1.24333342]\n",
      "Gradient: [ 0.00138243  0.29252204 -0.11073217]\n",
      "Loss value: 374.99241801202993\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 320\n",
      "Current point: [-2.46536242 -0.34895953  1.24355489]\n",
      "Gradient: [ 0.0013801   0.29046541 -0.10995719]\n",
      "Loss value: 374.992223034126\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 321\n",
      "Current point: [-2.46536518 -0.34954046  1.2437748 ]\n",
      "Gradient: [ 0.00137771  0.2884233  -0.10918765]\n",
      "Loss value: 374.9920307866262\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 322\n",
      "Current point: [-2.46536794 -0.35011731  1.24399318]\n",
      "Gradient: [ 0.00137526  0.28639562 -0.10842352]\n",
      "Loss value: 374.9918412312128\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 323\n",
      "Current point: [-2.46537069 -0.3506901   1.24421002]\n",
      "Gradient: [ 0.00137276  0.28438225 -0.10766475]\n",
      "Loss value: 374.9916543301071\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 324\n",
      "Current point: [-2.46537344 -0.35125886  1.24442535]\n",
      "Gradient: [ 0.00137021  0.28238309 -0.10691132]\n",
      "Loss value: 374.99147004606266\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 325\n",
      "Current point: [-2.46537618 -0.35182363  1.24463918]\n",
      "Gradient: [ 0.0013676   0.28039805 -0.10616317]\n",
      "Loss value: 374.99128834235694\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 326\n",
      "Current point: [-2.46537891 -0.35238443  1.2448515 ]\n",
      "Gradient: [ 0.00136494  0.27842703 -0.10542028]\n",
      "Loss value: 374.99110918278456\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 327\n",
      "Current point: [-2.46538164 -0.35294128  1.24506234]\n",
      "Gradient: [ 0.00136223  0.27646992 -0.10468261]\n",
      "Loss value: 374.9909325316495\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 328\n",
      "Current point: [-2.46538437 -0.35349422  1.24527171]\n",
      "Gradient: [ 0.00135946  0.27452662 -0.10395011]\n",
      "Loss value: 374.9907583537582\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 329\n",
      "Current point: [-2.46538708 -0.35404327  1.24547961]\n",
      "Gradient: [ 0.00135665  0.27259705 -0.10322276]\n",
      "Loss value: 374.9905866144124\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 330\n",
      "Current point: [-2.4653898  -0.35458847  1.24568605]\n",
      "Gradient: [ 0.00135378  0.27068109 -0.10250051]\n",
      "Loss value: 374.99041727940187\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 331\n",
      "Current point: [-2.46539251 -0.35512983  1.24589105]\n",
      "Gradient: [ 0.00135085  0.26877865 -0.10178332]\n",
      "Loss value: 374.9902503149981\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 332\n",
      "Current point: [-2.46539521 -0.35566739  1.24609462]\n",
      "Gradient: [ 0.00134788  0.26688965 -0.10107117]\n",
      "Loss value: 374.9900856879468\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 333\n",
      "Current point: [-2.4653979  -0.35620117  1.24629676]\n",
      "Gradient: [ 0.00134486  0.26501397 -0.10036401]\n",
      "Loss value: 374.98992336546166\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 334\n",
      "Current point: [-2.46540059 -0.35673119  1.24649749]\n",
      "Gradient: [ 0.00134178  0.26315153 -0.09966182]\n",
      "Loss value: 374.9897633152178\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 335\n",
      "Current point: [-2.46540328 -0.3572575   1.24669682]\n",
      "Gradient: [ 0.00133866  0.26130224 -0.09896454]\n",
      "Loss value: 374.9896055053447\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 336\n",
      "Current point: [-2.46540595 -0.3577801   1.24689474]\n",
      "Gradient: [ 0.00133548  0.259466   -0.09827216]\n",
      "Loss value: 374.9894499044203\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 337\n",
      "Current point: [-2.46540862 -0.35829903  1.24709129]\n",
      "Gradient: [ 0.00133226  0.25764271 -0.09758463]\n",
      "Loss value: 374.98929648146475\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 338\n",
      "Current point: [-2.46541129 -0.35881432  1.24728646]\n",
      "Gradient: [ 0.00132899  0.25583229 -0.09690192]\n",
      "Loss value: 374.9891452059336\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 339\n",
      "Current point: [-2.46541395 -0.35932598  1.24748026]\n",
      "Gradient: [ 0.00132568  0.25403464 -0.096224  ]\n",
      "Loss value: 374.988996047712\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 340\n",
      "Current point: [-2.4654166  -0.35983405  1.24767271]\n",
      "Gradient: [ 0.00132231  0.25224968 -0.09555083]\n",
      "Loss value: 374.9888489771089\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 341\n",
      "Current point: [-2.46541924 -0.36033855  1.24786381]\n",
      "Gradient: [ 0.00131891  0.25047731 -0.09488237]\n",
      "Loss value: 374.9887039648504\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 342\n",
      "Current point: [-2.46542188 -0.36083951  1.24805358]\n",
      "Gradient: [ 0.00131545  0.24871744 -0.0942186 ]\n",
      "Loss value: 374.9885609820744\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 343\n",
      "Current point: [-2.46542451 -0.36133694  1.24824201]\n",
      "Gradient: [ 0.00131195  0.24696999 -0.09355949]\n",
      "Loss value: 374.9884200003247\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 344\n",
      "Current point: [-2.46542714 -0.36183088  1.24842913]\n",
      "Gradient: [ 0.00130841  0.24523486 -0.09290499]\n",
      "Loss value: 374.988280991545\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 345\n",
      "Current point: [-2.46542975 -0.36232135  1.24861494]\n",
      "Gradient: [ 0.00130483  0.24351197 -0.09225507]\n",
      "Loss value: 374.9881439280738\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 346\n",
      "Current point: [-2.46543236 -0.36280837  1.24879945]\n",
      "Gradient: [ 0.0013012   0.24180124 -0.09160971]\n",
      "Loss value: 374.98800878263796\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 347\n",
      "Current point: [-2.46543496 -0.36329198  1.24898267]\n",
      "Gradient: [ 0.00129753  0.24010257 -0.09096887]\n",
      "Loss value: 374.98787552834824\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 348\n",
      "Current point: [-2.46543756 -0.36377218  1.24916461]\n",
      "Gradient: [ 0.00129383  0.23841588 -0.09033252]\n",
      "Loss value: 374.9877441386929\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 349\n",
      "Current point: [-2.46544015 -0.36424901  1.24934527]\n",
      "Gradient: [ 0.00129008  0.23674109 -0.08970063]\n",
      "Loss value: 374.98761458753324\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 350\n",
      "Current point: [-2.46544273 -0.3647225   1.24952468]\n",
      "Gradient: [ 0.00128629  0.23507811 -0.08907316]\n",
      "Loss value: 374.98748684909776\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 351\n",
      "Current point: [-2.4654453  -0.36519265  1.24970282]\n",
      "Gradient: [ 0.00128247  0.23342685 -0.08845009]\n",
      "Loss value: 374.98736089797717\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 352\n",
      "Current point: [-2.46544786 -0.36565951  1.24987972]\n",
      "Gradient: [ 0.00127861  0.23178724 -0.08783138]\n",
      "Loss value: 374.9872367091193\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 353\n",
      "Current point: [-2.46545042 -0.36612308  1.25005539]\n",
      "Gradient: [ 0.00127471  0.23015919 -0.087217  ]\n",
      "Loss value: 374.9871142578241\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 354\n",
      "Current point: [-2.46545297 -0.3665834   1.25022982]\n",
      "Gradient: [ 0.00127077  0.22854262 -0.08660692]\n",
      "Loss value: 374.98699351973835\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 355\n",
      "Current point: [-2.46545551 -0.36704048  1.25040303]\n",
      "Gradient: [ 0.0012668   0.22693745 -0.08600112]\n",
      "Loss value: 374.9868744708514\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 356\n",
      "Current point: [-2.46545805 -0.36749436  1.25057504]\n",
      "Gradient: [ 0.0012628   0.2253436  -0.08539956]\n",
      "Loss value: 374.9867570874897\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 357\n",
      "Current point: [-2.46546057 -0.36794505  1.25074583]\n",
      "Gradient: [ 0.00125876  0.22376098 -0.08480221]\n",
      "Loss value: 374.9866413463124\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 358\n",
      "Current point: [-2.46546309 -0.36839257  1.25091544]\n",
      "Gradient: [ 0.00125469  0.22218951 -0.08420904]\n",
      "Loss value: 374.98652722430654\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 359\n",
      "Current point: [-2.4654656  -0.36883695  1.25108386]\n",
      "Gradient: [ 0.00125059  0.22062913 -0.08362002]\n",
      "Loss value: 374.98641469878254\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 360\n",
      "Current point: [-2.4654681  -0.36927821  1.2512511 ]\n",
      "Gradient: [ 0.00124646  0.21907974 -0.08303513]\n",
      "Loss value: 374.9863037473694\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 361\n",
      "Current point: [-2.46547059 -0.36971637  1.25141717]\n",
      "Gradient: [ 0.0012423   0.21754128 -0.08245433]\n",
      "Loss value: 374.98619434801054\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 362\n",
      "Current point: [-2.46547308 -0.37015145  1.25158208]\n",
      "Gradient: [ 0.00123811  0.21601365 -0.0818776 ]\n",
      "Loss value: 374.986086478959\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 363\n",
      "Current point: [-2.46547555 -0.37058348  1.25174583]\n",
      "Gradient: [ 0.00123389  0.2144968  -0.0813049 ]\n",
      "Loss value: 374.9859801187735\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 364\n",
      "Current point: [-2.46547802 -0.37101247  1.25190844]\n",
      "Gradient: [ 0.00122964  0.21299063 -0.08073621]\n",
      "Loss value: 374.9858752463137\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 365\n",
      "Current point: [-2.46548048 -0.37143845  1.25206991]\n",
      "Gradient: [ 0.00122537  0.21149508 -0.0801715 ]\n",
      "Loss value: 374.9857718407363\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 366\n",
      "Current point: [-2.46548293 -0.37186144  1.25223026]\n",
      "Gradient: [ 0.00122107  0.21001007 -0.07961074]\n",
      "Loss value: 374.9856698814905\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 367\n",
      "Current point: [-2.46548537 -0.37228146  1.25238948]\n",
      "Gradient: [ 0.00121674  0.20853552 -0.07905391]\n",
      "Loss value: 374.9855693483144\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 368\n",
      "Current point: [-2.46548781 -0.37269853  1.25254759]\n",
      "Gradient: [ 0.00121239  0.20707137 -0.07850098]\n",
      "Loss value: 374.98547022123046\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 369\n",
      "Current point: [-2.46549023 -0.37311267  1.25270459]\n",
      "Gradient: [ 0.00120802  0.20561752 -0.07795191]\n",
      "Loss value: 374.9853724805415\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 370\n",
      "Current point: [-2.46549265 -0.37352391  1.25286049]\n",
      "Gradient: [ 0.00120362  0.20417393 -0.07740668]\n",
      "Loss value: 374.98527610682714\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 371\n",
      "Current point: [-2.46549506 -0.37393226  1.2530153 ]\n",
      "Gradient: [ 0.0011992   0.2027405  -0.07686528]\n",
      "Loss value: 374.9851810809396\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 372\n",
      "Current point: [-2.46549745 -0.37433774  1.25316904]\n",
      "Gradient: [ 0.00119476  0.20131717 -0.07632766]\n",
      "Loss value: 374.98508738399994\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 373\n",
      "Current point: [-2.46549984 -0.37474037  1.25332169]\n",
      "Gradient: [ 0.00119029  0.19990387 -0.0757938 ]\n",
      "Loss value: 374.9849949973941\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 374\n",
      "Current point: [-2.46550222 -0.37514018  1.25347328]\n",
      "Gradient: [ 0.00118581  0.19850052 -0.07526367]\n",
      "Loss value: 374.9849039027694\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 375\n",
      "Current point: [-2.4655046  -0.37553718  1.25362381]\n",
      "Gradient: [ 0.00118131  0.19710706 -0.07473726]\n",
      "Loss value: 374.98481408203105\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 376\n",
      "Current point: [-2.46550696 -0.3759314   1.25377328]\n",
      "Gradient: [ 0.00117679  0.19572342 -0.07421453]\n",
      "Loss value: 374.98472551733784\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 377\n",
      "Current point: [-2.46550931 -0.37632284  1.25392171]\n",
      "Gradient: [ 0.00117225  0.19434952 -0.07369545]\n",
      "Loss value: 374.9846381910992\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 378\n",
      "Current point: [-2.46551166 -0.37671154  1.2540691 ]\n",
      "Gradient: [ 0.00116769  0.19298529 -0.07318001]\n",
      "Loss value: 374.9845520859714\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 379\n",
      "Current point: [-2.46551399 -0.37709751  1.25421546]\n",
      "Gradient: [ 0.00116312  0.19163068 -0.07266817]\n",
      "Loss value: 374.9844671848543\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 380\n",
      "Current point: [-2.46551632 -0.37748077  1.2543608 ]\n",
      "Gradient: [ 0.00115853  0.1902856  -0.07215992]\n",
      "Loss value: 374.98438347088734\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 381\n",
      "Current point: [-2.46551864 -0.37786134  1.25450512]\n",
      "Gradient: [ 0.00115393  0.18895    -0.07165522]\n",
      "Loss value: 374.98430092744707\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 382\n",
      "Current point: [-2.46552094 -0.37823924  1.25464843]\n",
      "Gradient: [ 0.00114931  0.1876238  -0.07115405]\n",
      "Loss value: 374.9842195381429\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 383\n",
      "Current point: [-2.46552324 -0.37861449  1.25479073]\n",
      "Gradient: [ 0.00114468  0.18630694 -0.07065639]\n",
      "Loss value: 374.98413928681447\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 384\n",
      "Current point: [-2.46552553 -0.37898711  1.25493205]\n",
      "Gradient: [ 0.00114003  0.18499935 -0.0701622 ]\n",
      "Loss value: 374.9840601575281\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 385\n",
      "Current point: [-2.46552781 -0.3793571   1.25507237]\n",
      "Gradient: [ 0.00113538  0.18370097 -0.06967148]\n",
      "Loss value: 374.98398213457375\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 386\n",
      "Current point: [-2.46553008 -0.37972451  1.25521171]\n",
      "Gradient: [ 0.00113071  0.18241173 -0.06918419]\n",
      "Loss value: 374.98390520246164\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 387\n",
      "Current point: [-2.46553234 -0.38008933  1.25535008]\n",
      "Gradient: [ 0.00112602  0.18113157 -0.06870031]\n",
      "Loss value: 374.9838293459195\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 388\n",
      "Current point: [-2.4655346  -0.38045159  1.25548748]\n",
      "Gradient: [ 0.00112133  0.17986042 -0.06821981]\n",
      "Loss value: 374.9837545498892\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 389\n",
      "Current point: [-2.46553684 -0.38081131  1.25562392]\n",
      "Gradient: [ 0.00111663  0.17859822 -0.06774267]\n",
      "Loss value: 374.9836807995239\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 390\n",
      "Current point: [-2.46553907 -0.38116851  1.25575941]\n",
      "Gradient: [ 0.00111192  0.17734491 -0.06726887]\n",
      "Loss value: 374.9836080801849\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 391\n",
      "Current point: [-2.4655413  -0.3815232   1.25589395]\n",
      "Gradient: [ 0.0011072   0.17610041 -0.06679838]\n",
      "Loss value: 374.9835363774391\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 392\n",
      "Current point: [-2.46554351 -0.3818754   1.25602754]\n",
      "Gradient: [ 0.00110247  0.17486468 -0.06633119]\n",
      "Loss value: 374.98346567705573\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 393\n",
      "Current point: [-2.46554571 -0.38222513  1.25616021]\n",
      "Gradient: [ 0.00109773  0.17363764 -0.06586726]\n",
      "Loss value: 374.9833959650035\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 394\n",
      "Current point: [-2.46554791 -0.38257241  1.25629194]\n",
      "Gradient: [ 0.00109299  0.17241924 -0.06540658]\n",
      "Loss value: 374.98332722744806\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 395\n",
      "Current point: [-2.4655501  -0.38291724  1.25642275]\n",
      "Gradient: [ 0.00108824  0.17120942 -0.06494912]\n",
      "Loss value: 374.983259450749\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 396\n",
      "Current point: [-2.46555227 -0.38325966  1.25655265]\n",
      "Gradient: [ 0.00108348  0.17000811 -0.06449486]\n",
      "Loss value: 374.9831926214573\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 397\n",
      "Current point: [-2.46555444 -0.38359968  1.25668164]\n",
      "Gradient: [ 0.00107872  0.16881526 -0.06404378]\n",
      "Loss value: 374.98312672631266\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 398\n",
      "Current point: [-2.4655566  -0.38393731  1.25680973]\n",
      "Gradient: [ 0.00107395  0.1676308  -0.06359585]\n",
      "Loss value: 374.98306175224036\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 399\n",
      "Current point: [-2.46555874 -0.38427257  1.25693692]\n",
      "Gradient: [ 0.00106918  0.16645467 -0.06315106]\n",
      "Loss value: 374.98299768634945\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 400\n",
      "Current point: [-2.46556088 -0.38460548  1.25706322]\n",
      "Gradient: [ 0.0010644   0.16528682 -0.06270937]\n",
      "Loss value: 374.98293451592934\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 401\n",
      "Current point: [-2.46556301 -0.38493605  1.25718864]\n",
      "Gradient: [ 0.00105962  0.16412719 -0.06227078]\n",
      "Loss value: 374.982872228448\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 402\n",
      "Current point: [-2.46556513 -0.38526431  1.25731318]\n",
      "Gradient: [ 0.00105484  0.16297572 -0.06183525]\n",
      "Loss value: 374.98281081154903\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 403\n",
      "Current point: [-2.46556724 -0.38559026  1.25743685]\n",
      "Gradient: [ 0.00105006  0.16183234 -0.06140277]\n",
      "Loss value: 374.98275025304906\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 404\n",
      "Current point: [-2.46556934 -0.38591392  1.25755966]\n",
      "Gradient: [ 0.00104527  0.16069702 -0.06097332]\n",
      "Loss value: 374.9826905409359\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 405\n",
      "Current point: [-2.46557143 -0.38623532  1.25768161]\n",
      "Gradient: [ 0.00104048  0.15956968 -0.06054686]\n",
      "Loss value: 374.9826316633654\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 406\n",
      "Current point: [-2.46557351 -0.38655446  1.2578027 ]\n",
      "Gradient: [ 0.00103569  0.15845027 -0.06012339]\n",
      "Loss value: 374.98257360865966\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 407\n",
      "Current point: [-2.46557558 -0.38687136  1.25792295]\n",
      "Gradient: [ 0.0010309   0.15733874 -0.05970289]\n",
      "Loss value: 374.98251636530443\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 408\n",
      "Current point: [-2.46557765 -0.38718604  1.25804235]\n",
      "Gradient: [ 0.00102611  0.15623502 -0.05928532]\n",
      "Loss value: 374.982459921947\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 409\n",
      "Current point: [-2.4655797  -0.38749851  1.25816092]\n",
      "Gradient: [ 0.00102132  0.15513907 -0.05887067]\n",
      "Loss value: 374.9824042673936\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 410\n",
      "Current point: [-2.46558174 -0.38780878  1.25827866]\n",
      "Gradient: [ 0.00101654  0.15405083 -0.05845893]\n",
      "Loss value: 374.9823493906074\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 411\n",
      "Current point: [-2.46558377 -0.38811689  1.25839558]\n",
      "Gradient: [ 0.00101175  0.15297024 -0.05805006]\n",
      "Loss value: 374.98229528070635\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 412\n",
      "Current point: [-2.4655858  -0.38842283  1.25851168]\n",
      "Gradient: [ 0.00100696  0.15189725 -0.05764405]\n",
      "Loss value: 374.9822419269609\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 413\n",
      "Current point: [-2.46558781 -0.38872662  1.25862697]\n",
      "Gradient: [ 0.00100218  0.15083181 -0.05724088]\n",
      "Loss value: 374.9821893187918\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 414\n",
      "Current point: [-2.46558982 -0.38902828  1.25874145]\n",
      "Gradient: [ 0.0009974   0.14977386 -0.05684054]\n",
      "Loss value: 374.98213744576816\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 415\n",
      "Current point: [-2.46559181 -0.38932783  1.25885513]\n",
      "Gradient: [ 0.00099262  0.14872336 -0.05644299]\n",
      "Loss value: 374.9820862976053\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 416\n",
      "Current point: [-2.4655938  -0.38962528  1.25896802]\n",
      "Gradient: [ 0.00098784  0.14768024 -0.05604822]\n",
      "Loss value: 374.9820358641624\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 417\n",
      "Current point: [-2.46559577 -0.38992064  1.25908012]\n",
      "Gradient: [ 0.00098307  0.14664445 -0.05565621]\n",
      "Loss value: 374.9819861354413\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 418\n",
      "Current point: [-2.46559774 -0.39021393  1.25919143]\n",
      "Gradient: [ 0.0009783   0.14561595 -0.05526695]\n",
      "Loss value: 374.9819371015833\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 419\n",
      "Current point: [-2.46559969 -0.39050516  1.25930196]\n",
      "Gradient: [ 0.00097353  0.14459468 -0.05488041]\n",
      "Loss value: 374.98188875286814\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 420\n",
      "Current point: [-2.46560164 -0.39079435  1.25941172]\n",
      "Gradient: [ 0.00096878  0.1435806  -0.05449657]\n",
      "Loss value: 374.981841079712\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 421\n",
      "Current point: [-2.46560358 -0.39108151  1.25952072]\n",
      "Gradient: [ 0.00096402  0.14257364 -0.05411541]\n",
      "Loss value: 374.9817940726649\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 422\n",
      "Current point: [-2.46560551 -0.39136666  1.25962895]\n",
      "Gradient: [ 0.00095927  0.14157376 -0.05373692]\n",
      "Loss value: 374.9817477224095\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 423\n",
      "Current point: [-2.46560742 -0.39164981  1.25973642]\n",
      "Gradient: [ 0.00095453  0.14058091 -0.05336108]\n",
      "Loss value: 374.98170201975904\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 424\n",
      "Current point: [-2.46560933 -0.39193097  1.25984314]\n",
      "Gradient: [ 0.00094979  0.13959505 -0.05298786]\n",
      "Loss value: 374.98165695565524\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 425\n",
      "Current point: [-2.46561123 -0.39221016  1.25994912]\n",
      "Gradient: [ 0.00094506  0.13861611 -0.05261726]\n",
      "Loss value: 374.9816125211671\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 426\n",
      "Current point: [-2.46561312 -0.39248739  1.26005435]\n",
      "Gradient: [ 0.00094033  0.13764405 -0.05224925]\n",
      "Loss value: 374.98156870748835\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 427\n",
      "Current point: [-2.465615   -0.39276268  1.26015885]\n",
      "Gradient: [ 0.00093562  0.13667883 -0.05188381]\n",
      "Loss value: 374.98152550593625\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 428\n",
      "Current point: [-2.46561688 -0.39303604  1.26026262]\n",
      "Gradient: [ 0.00093091  0.1357204  -0.05152093]\n",
      "Loss value: 374.98148290794967\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 429\n",
      "Current point: [-2.46561874 -0.39330748  1.26036566]\n",
      "Gradient: [ 0.0009262   0.1347687  -0.05116058]\n",
      "Loss value: 374.98144090508754\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 430\n",
      "Current point: [-2.46562059 -0.39357701  1.26046798]\n",
      "Gradient: [ 0.00092151  0.13382369 -0.05080276]\n",
      "Loss value: 374.9813994890268\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 431\n",
      "Current point: [-2.46562243 -0.39384466  1.26056959]\n",
      "Gradient: [ 0.00091682  0.13288532 -0.05044744]\n",
      "Loss value: 374.981358651561\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 432\n",
      "Current point: [-2.46562427 -0.39411043  1.26067048]\n",
      "Gradient: [ 0.00091214  0.13195355 -0.0500946 ]\n",
      "Loss value: 374.9813183845988\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 433\n",
      "Current point: [-2.46562609 -0.39437434  1.26077067]\n",
      "Gradient: [ 0.00090747  0.13102832 -0.04974423]\n",
      "Loss value: 374.9812786801621\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 434\n",
      "Current point: [-2.46562791 -0.3946364   1.26087016]\n",
      "Gradient: [ 0.00090281  0.1301096  -0.04939631]\n",
      "Loss value: 374.9812395303845\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 435\n",
      "Current point: [-2.46562971 -0.39489661  1.26096895]\n",
      "Gradient: [ 0.00089816  0.12919734 -0.04905082]\n",
      "Loss value: 374.9812009275097\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 436\n",
      "Current point: [-2.46563151 -0.39515501  1.26106705]\n",
      "Gradient: [ 0.00089352  0.12829149 -0.04870775]\n",
      "Loss value: 374.9811628638903\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 437\n",
      "Current point: [-2.46563329 -0.39541159  1.26116447]\n",
      "Gradient: [ 0.00088889  0.127392   -0.04836708]\n",
      "Loss value: 374.9811253319856\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 438\n",
      "Current point: [-2.46563507 -0.39566638  1.2612612 ]\n",
      "Gradient: [ 0.00088426  0.12649883 -0.04802879]\n",
      "Loss value: 374.98108832436094\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 439\n",
      "Current point: [-2.46563684 -0.39591937  1.26135726]\n",
      "Gradient: [ 0.00087965  0.12561195 -0.04769287]\n",
      "Loss value: 374.9810518336853\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 440\n",
      "Current point: [-2.4656386  -0.3961706   1.26145265]\n",
      "Gradient: [ 0.00087505  0.12473129 -0.0473593 ]\n",
      "Loss value: 374.9810158527307\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 441\n",
      "Current point: [-2.46564035 -0.39642006  1.26154737]\n",
      "Gradient: [ 0.00087046  0.12385682 -0.04702806]\n",
      "Loss value: 374.9809803743701\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 442\n",
      "Current point: [-2.46564209 -0.39666777  1.26164142]\n",
      "Gradient: [ 0.00086588  0.12298849 -0.04669913]\n",
      "Loss value: 374.98094539157654\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 443\n",
      "Current point: [-2.46564382 -0.39691375  1.26173482]\n",
      "Gradient: [ 0.00086131  0.12212627 -0.04637251]\n",
      "Loss value: 374.98091089742115\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 444\n",
      "Current point: [-2.46564555 -0.397158    1.26182757]\n",
      "Gradient: [ 0.00085675  0.1212701  -0.04604817]\n",
      "Loss value: 374.98087688507223\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 445\n",
      "Current point: [-2.46564726 -0.39740054  1.26191966]\n",
      "Gradient: [ 0.00085221  0.12041995 -0.0457261 ]\n",
      "Loss value: 374.9808433477938\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 446\n",
      "Current point: [-2.46564896 -0.39764138  1.26201111]\n",
      "Gradient: [ 0.00084767  0.11957577 -0.04540628]\n",
      "Loss value: 374.9808102789439\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 447\n",
      "Current point: [-2.46565066 -0.39788054  1.26210193]\n",
      "Gradient: [ 0.00084315  0.11873752 -0.0450887 ]\n",
      "Loss value: 374.9807776719739\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 448\n",
      "Current point: [-2.46565235 -0.39811801  1.2621921 ]\n",
      "Gradient: [ 0.00083864  0.11790516 -0.04477334]\n",
      "Loss value: 374.9807455204266\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 449\n",
      "Current point: [-2.46565402 -0.39835382  1.26228165]\n",
      "Gradient: [ 0.00083414  0.11707865 -0.04446018]\n",
      "Loss value: 374.98071381793534\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 450\n",
      "Current point: [-2.46565569 -0.39858798  1.26237057]\n",
      "Gradient: [ 0.00082965  0.11625795 -0.04414922]\n",
      "Loss value: 374.9806825582225\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 451\n",
      "Current point: [-2.46565735 -0.39882049  1.26245887]\n",
      "Gradient: [ 0.00082518  0.11544301 -0.04384043]\n",
      "Loss value: 374.9806517350984\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 452\n",
      "Current point: [-2.465659   -0.39905138  1.26254655]\n",
      "Gradient: [ 0.00082072  0.11463379 -0.0435338 ]\n",
      "Loss value: 374.9806213424599\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 453\n",
      "Current point: [-2.46566064 -0.39928065  1.26263362]\n",
      "Gradient: [ 0.00081627  0.11383026 -0.04322931]\n",
      "Loss value: 374.9805913742895\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 454\n",
      "Current point: [-2.46566227 -0.39950831  1.26272008]\n",
      "Gradient: [ 0.00081184  0.11303237 -0.04292695]\n",
      "Loss value: 374.9805618246536\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 455\n",
      "Current point: [-2.4656639  -0.39973437  1.26280593]\n",
      "Gradient: [ 0.00080741  0.11224008 -0.04262671]\n",
      "Loss value: 374.98053268770195\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 456\n",
      "Current point: [-2.46566551 -0.39995885  1.26289118]\n",
      "Gradient: [ 0.00080301  0.11145336 -0.04232857]\n",
      "Loss value: 374.9805039576661\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 457\n",
      "Current point: [-2.46566712 -0.40018176  1.26297584]\n",
      "Gradient: [ 0.00079861  0.11067217 -0.04203251]\n",
      "Loss value: 374.98047562885824\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 458\n",
      "Current point: [-2.46566872 -0.4004031   1.26305991]\n",
      "Gradient: [ 0.00079423  0.10989646 -0.04173852]\n",
      "Loss value: 374.98044769567025\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 459\n",
      "Current point: [-2.4656703  -0.4006229   1.26314338]\n",
      "Gradient: [ 0.00078986  0.1091262  -0.04144659]\n",
      "Loss value: 374.9804201525724\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 460\n",
      "Current point: [-2.46567188 -0.40084115  1.26322628]\n",
      "Gradient: [ 0.00078551  0.10836135 -0.0411567 ]\n",
      "Loss value: 374.9803929941126\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 461\n",
      "Current point: [-2.46567346 -0.40105787  1.26330859]\n",
      "Gradient: [ 0.00078117  0.10760187 -0.04086884]\n",
      "Loss value: 374.9803662149147\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 462\n",
      "Current point: [-2.46567502 -0.40127308  1.26339033]\n",
      "Gradient: [ 0.00077685  0.10684772 -0.04058299]\n",
      "Loss value: 374.98033980967807\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 463\n",
      "Current point: [-2.46567657 -0.40148677  1.26347149]\n",
      "Gradient: [ 0.00077254  0.10609887 -0.04029914]\n",
      "Loss value: 374.980313773176\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 464\n",
      "Current point: [-2.46567812 -0.40169897  1.26355209]\n",
      "Gradient: [ 0.00076824  0.10535528 -0.04001728]\n",
      "Loss value: 374.9802881002554\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 465\n",
      "Current point: [-2.46567965 -0.40190968  1.26363213]\n",
      "Gradient: [ 0.00076396  0.10461691 -0.03973738]\n",
      "Loss value: 374.9802627858346\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 466\n",
      "Current point: [-2.46568118 -0.40211891  1.2637116 ]\n",
      "Gradient: [ 0.0007597   0.10388372 -0.03945945]\n",
      "Loss value: 374.98023782490367\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 467\n",
      "Current point: [-2.4656827  -0.40232668  1.26379052]\n",
      "Gradient: [ 0.00075544  0.10315569 -0.03918346]\n",
      "Loss value: 374.9802132125223\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 468\n",
      "Current point: [-2.46568421 -0.40253299  1.26386889]\n",
      "Gradient: [ 0.00075121  0.10243276 -0.03890939]\n",
      "Loss value: 374.9801889438196\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 469\n",
      "Current point: [-2.46568571 -0.40273786  1.26394671]\n",
      "Gradient: [ 0.00074699  0.10171491 -0.03863725]\n",
      "Loss value: 374.98016501399275\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 470\n",
      "Current point: [-2.46568721 -0.40294129  1.26402398]\n",
      "Gradient: [ 0.00074278  0.1010021  -0.03836701]\n",
      "Loss value: 374.9801414183062\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 471\n",
      "Current point: [-2.46568869 -0.40314329  1.26410071]\n",
      "Gradient: [ 0.00073859  0.10029429 -0.03809865]\n",
      "Loss value: 374.9801181520904\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 472\n",
      "Current point: [-2.46569017 -0.40334388  1.26417691]\n",
      "Gradient: [ 0.00073441  0.09959145 -0.03783218]\n",
      "Loss value: 374.98009521074135\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 473\n",
      "Current point: [-2.46569164 -0.40354306  1.26425258]\n",
      "Gradient: [ 0.00073025  0.09889355 -0.03756757]\n",
      "Loss value: 374.98007258971955\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 474\n",
      "Current point: [-2.4656931  -0.40374085  1.26432771]\n",
      "Gradient: [ 0.00072611  0.09820055 -0.03730481]\n",
      "Loss value: 374.9800502845487\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 475\n",
      "Current point: [-2.46569455 -0.40393725  1.26440232]\n",
      "Gradient: [ 0.00072198  0.09751241 -0.03704388]\n",
      "Loss value: 374.98002829081537\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 476\n",
      "Current point: [-2.465696   -0.40413228  1.26447641]\n",
      "Gradient: [ 0.00071786  0.0968291  -0.03678478]\n",
      "Loss value: 374.9800066041678\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 477\n",
      "Current point: [-2.46569743 -0.40432593  1.26454998]\n",
      "Gradient: [ 0.00071377  0.09615059 -0.0365275 ]\n",
      "Loss value: 374.97998522031503\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 478\n",
      "Current point: [-2.46569886 -0.40451824  1.26462303]\n",
      "Gradient: [ 0.00070968  0.09547684 -0.03627201]\n",
      "Loss value: 374.9799641350263\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 479\n",
      "Current point: [-2.46570028 -0.40470919  1.26469558]\n",
      "Gradient: [ 0.00070562  0.09480782 -0.03601831]\n",
      "Loss value: 374.9799433441299\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 480\n",
      "Current point: [-2.46570169 -0.4048988   1.26476761]\n",
      "Gradient: [ 0.00070157  0.09414349 -0.03576638]\n",
      "Loss value: 374.97992284351244\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 481\n",
      "Current point: [-2.46570309 -0.40508709  1.26483915]\n",
      "Gradient: [ 0.00069753  0.09348383 -0.03551622]\n",
      "Loss value: 374.9799026291182\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 482\n",
      "Current point: [-2.46570449 -0.40527406  1.26491018]\n",
      "Gradient: [ 0.00069351  0.0928288  -0.03526781]\n",
      "Loss value: 374.97988269694804\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 483\n",
      "Current point: [-2.46570587 -0.40545972  1.26498071]\n",
      "Gradient: [ 0.00068951  0.09217837 -0.03502113]\n",
      "Loss value: 374.9798630430589\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 484\n",
      "Current point: [-2.46570725 -0.40564407  1.26505076]\n",
      "Gradient: [ 0.00068552  0.0915325  -0.03477618]\n",
      "Loss value: 374.9798436635627\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 485\n",
      "Current point: [-2.46570862 -0.40582714  1.26512031]\n",
      "Gradient: [ 0.00068155  0.09089116 -0.03453294]\n",
      "Loss value: 374.9798245546259\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 486\n",
      "Current point: [-2.46570999 -0.40600892  1.26518937]\n",
      "Gradient: [ 0.0006776   0.09025433 -0.0342914 ]\n",
      "Loss value: 374.9798057124686\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 487\n",
      "Current point: [-2.46571134 -0.40618943  1.26525796]\n",
      "Gradient: [ 0.00067366  0.08962196 -0.03405155]\n",
      "Loss value: 374.9797871333635\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 488\n",
      "Current point: [-2.46571269 -0.40636867  1.26532606]\n",
      "Gradient: [ 0.00066974  0.08899404 -0.03381338]\n",
      "Loss value: 374.9797688136357\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 489\n",
      "Current point: [-2.46571403 -0.40654666  1.26539369]\n",
      "Gradient: [ 0.00066584  0.08837051 -0.03357688]\n",
      "Loss value: 374.97975074966166\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 490\n",
      "Current point: [-2.46571536 -0.4067234   1.26546084]\n",
      "Gradient: [ 0.00066195  0.08775137 -0.03334203]\n",
      "Loss value: 374.9797329378684\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 491\n",
      "Current point: [-2.46571669 -0.40689891  1.26552752]\n",
      "Gradient: [ 0.00065808  0.08713657 -0.03310882]\n",
      "Loss value: 374.9797153747329\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 492\n",
      "Current point: [-2.465718   -0.40707318  1.26559374]\n",
      "Gradient: [ 0.00065422  0.08652608 -0.03287724]\n",
      "Loss value: 374.97969805678156\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 493\n",
      "Current point: [-2.46571931 -0.40724623  1.2656595 ]\n",
      "Gradient: [ 0.00065038  0.08591988 -0.03264728]\n",
      "Loss value: 374.97968098058914\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 494\n",
      "Current point: [-2.46572061 -0.40741807  1.26572479]\n",
      "Gradient: [ 0.00064656  0.08531793 -0.03241893]\n",
      "Loss value: 374.9796641427786\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 495\n",
      "Current point: [-2.4657219  -0.40758871  1.26578963]\n",
      "Gradient: [ 0.00064275  0.08472021 -0.03219218]\n",
      "Loss value: 374.97964754001987\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 496\n",
      "Current point: [-2.46572319 -0.40775815  1.26585401]\n",
      "Gradient: [ 0.00063896  0.08412667 -0.03196701]\n",
      "Loss value: 374.97963116902935\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 497\n",
      "Current point: [-2.46572447 -0.4079264   1.26591795]\n",
      "Gradient: [ 0.00063519  0.08353731 -0.03174342]\n",
      "Loss value: 374.9796150265697\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 498\n",
      "Current point: [-2.46572574 -0.40809347  1.26598143]\n",
      "Gradient: [ 0.00063144  0.08295208 -0.03152139]\n",
      "Loss value: 374.9795991094487\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 499\n",
      "Current point: [-2.465727   -0.40825938  1.26604448]\n",
      "Gradient: [ 0.0006277   0.08237095 -0.03130092]\n",
      "Loss value: 374.9795834145187\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 500\n",
      "Current point: [-2.46572826 -0.40842412  1.26610708]\n",
      "Gradient: [ 0.00062397  0.0817939  -0.03108198]\n",
      "Loss value: 374.9795679386759\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 501\n",
      "Current point: [-2.4657295  -0.40858771  1.26616924]\n",
      "Gradient: [ 0.00062027  0.0812209  -0.03086458]\n",
      "Loss value: 374.9795526788603\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 502\n",
      "Current point: [-2.46573074 -0.40875015  1.26623097]\n",
      "Gradient: [ 0.00061658  0.08065192 -0.0306487 ]\n",
      "Loss value: 374.9795376320543\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 503\n",
      "Current point: [-2.46573198 -0.40891145  1.26629227]\n",
      "Gradient: [ 0.00061291  0.08008694 -0.03043433]\n",
      "Loss value: 374.97952279528283\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 504\n",
      "Current point: [-2.4657332  -0.40907163  1.26635314]\n",
      "Gradient: [ 0.00060925  0.07952591 -0.03022146]\n",
      "Loss value: 374.9795081656121\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 505\n",
      "Current point: [-2.46573442 -0.40923068  1.26641358]\n",
      "Gradient: [ 0.00060561  0.07896882 -0.03001007]\n",
      "Loss value: 374.97949374014956\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 506\n",
      "Current point: [-2.46573563 -0.40938862  1.2664736 ]\n",
      "Gradient: [ 0.00060199  0.07841564 -0.02980017]\n",
      "Loss value: 374.97947951604306\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 507\n",
      "Current point: [-2.46573684 -0.40954545  1.2665332 ]\n",
      "Gradient: [ 0.00059839  0.07786634 -0.02959173]\n",
      "Loss value: 374.9794654904801\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 508\n",
      "Current point: [-2.46573803 -0.40970118  1.26659239]\n",
      "Gradient: [ 0.0005948   0.07732089 -0.02938475]\n",
      "Loss value: 374.9794516606878\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 509\n",
      "Current point: [-2.46573922 -0.40985582  1.26665115]\n",
      "Gradient: [ 0.00059122  0.07677927 -0.02917922]\n",
      "Loss value: 374.9794380239319\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 510\n",
      "Current point: [-2.46574041 -0.41000938  1.26670951]\n",
      "Gradient: [ 0.00058767  0.07624145 -0.02897512]\n",
      "Loss value: 374.97942457751634\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 511\n",
      "Current point: [-2.46574158 -0.41016186  1.26676746]\n",
      "Gradient: [ 0.00058413  0.0757074  -0.02877246]\n",
      "Loss value: 374.9794113187828\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 512\n",
      "Current point: [-2.46574275 -0.41031328  1.26682501]\n",
      "Gradient: [ 0.00058061  0.0751771  -0.02857121]\n",
      "Loss value: 374.97939824511025\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 513\n",
      "Current point: [-2.46574391 -0.41046363  1.26688215]\n",
      "Gradient: [ 0.00057711  0.07465052 -0.02837137]\n",
      "Loss value: 374.97938535391387\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 514\n",
      "Current point: [-2.46574507 -0.41061293  1.26693889]\n",
      "Gradient: [ 0.00057362  0.07412763 -0.02817292]\n",
      "Loss value: 374.97937264264556\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 515\n",
      "Current point: [-2.46574621 -0.41076119  1.26699524]\n",
      "Gradient: [ 0.00057015  0.07360841 -0.02797587]\n",
      "Loss value: 374.9793601087923\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 516\n",
      "Current point: [-2.46574735 -0.41090841  1.26705119]\n",
      "Gradient: [ 0.00056669  0.07309283 -0.02778019]\n",
      "Loss value: 374.9793477498765\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 517\n",
      "Current point: [-2.46574849 -0.41105459  1.26710675]\n",
      "Gradient: [ 0.00056325  0.07258086 -0.02758588]\n",
      "Loss value: 374.97933556345527\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 518\n",
      "Current point: [-2.46574961 -0.41119975  1.26716192]\n",
      "Gradient: [ 0.00055983  0.07207249 -0.02739293]\n",
      "Loss value: 374.9793235471194\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 519\n",
      "Current point: [-2.46575073 -0.4113439   1.26721671]\n",
      "Gradient: [ 0.00055643  0.07156768 -0.02720133]\n",
      "Loss value: 374.979311698494\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 520\n",
      "Current point: [-2.46575185 -0.41148703  1.26727111]\n",
      "Gradient: [ 0.00055304  0.07106641 -0.02701107]\n",
      "Loss value: 374.9793000152369\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 521\n",
      "Current point: [-2.46575295 -0.41162917  1.26732513]\n",
      "Gradient: [ 0.00054967  0.07056866 -0.02682214]\n",
      "Loss value: 374.9792884950388\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 522\n",
      "Current point: [-2.46575405 -0.4117703   1.26737878]\n",
      "Gradient: [ 0.00054631  0.0700744  -0.02663453]\n",
      "Loss value: 374.97927713562274\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 523\n",
      "Current point: [-2.46575514 -0.41191045  1.26743205]\n",
      "Gradient: [ 0.00054298  0.0695836  -0.02644823]\n",
      "Loss value: 374.9792659347435\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 524\n",
      "Current point: [-2.46575623 -0.41204962  1.26748494]\n",
      "Gradient: [ 0.00053965  0.06909625 -0.02626324]\n",
      "Loss value: 374.9792548901873\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 525\n",
      "Current point: [-2.46575731 -0.41218781  1.26753747]\n",
      "Gradient: [ 0.00053635  0.06861231 -0.02607954]\n",
      "Loss value: 374.9792439997712\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 526\n",
      "Current point: [-2.46575838 -0.41232504  1.26758963]\n",
      "Gradient: [ 0.00053306  0.06813177 -0.02589712]\n",
      "Loss value: 374.9792332613429\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 527\n",
      "Current point: [-2.46575945 -0.4124613   1.26764142]\n",
      "Gradient: [ 0.00052979  0.0676546  -0.02571598]\n",
      "Loss value: 374.9792226727801\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 528\n",
      "Current point: [-2.46576051 -0.41259661  1.26769286]\n",
      "Gradient: [ 0.00052654  0.06718077 -0.02553611]\n",
      "Loss value: 374.9792122319901\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 529\n",
      "Current point: [-2.46576156 -0.41273097  1.26774393]\n",
      "Gradient: [ 0.0005233   0.06671027 -0.0253575 ]\n",
      "Loss value: 374.9792019369096\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 530\n",
      "Current point: [-2.46576261 -0.41286439  1.26779464]\n",
      "Gradient: [ 0.00052007  0.06624306 -0.02518013]\n",
      "Loss value: 374.9791917855041\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 531\n",
      "Current point: [-2.46576365 -0.41299688  1.267845  ]\n",
      "Gradient: [ 0.00051687  0.06577913 -0.02500401]\n",
      "Loss value: 374.9791817757673\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 532\n",
      "Current point: [-2.46576468 -0.41312844  1.26789501]\n",
      "Gradient: [ 0.00051368  0.06531846 -0.02482912]\n",
      "Loss value: 374.9791719057213\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 533\n",
      "Current point: [-2.46576571 -0.41325907  1.26794467]\n",
      "Gradient: [ 0.00051051  0.06486101 -0.02465545]\n",
      "Loss value: 374.97916217341543\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 534\n",
      "Current point: [-2.46576673 -0.4133888   1.26799398]\n",
      "Gradient: [ 0.00050735  0.06440677 -0.02448299]\n",
      "Loss value: 374.9791525769267\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 535\n",
      "Current point: [-2.46576774 -0.41351761  1.26804295]\n",
      "Gradient: [ 0.00050421  0.06395572 -0.02431174]\n",
      "Loss value: 374.9791431143587\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 536\n",
      "Current point: [-2.46576875 -0.41364552  1.26809157]\n",
      "Gradient: [ 0.00050108  0.06350783 -0.02414169]\n",
      "Loss value: 374.9791337838416\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 537\n",
      "Current point: [-2.46576975 -0.41377254  1.26813985]\n",
      "Gradient: [ 0.00049798  0.06306308 -0.02397283]\n",
      "Loss value: 374.97912458353176\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 538\n",
      "Current point: [-2.46577075 -0.41389866  1.2681878 ]\n",
      "Gradient: [ 0.00049488  0.06262144 -0.02380515]\n",
      "Loss value: 374.9791155116111\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 539\n",
      "Current point: [-2.46577174 -0.41402391  1.26823541]\n",
      "Gradient: [ 0.00049181  0.06218291 -0.02363864]\n",
      "Loss value: 374.97910656628727\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 540\n",
      "Current point: [-2.46577272 -0.41414827  1.26828269]\n",
      "Gradient: [ 0.00048875  0.06174745 -0.0234733 ]\n",
      "Loss value: 374.9790977457927\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 541\n",
      "Current point: [-2.4657737  -0.41427177  1.26832963]\n",
      "Gradient: [ 0.00048571  0.06131504 -0.02330911]\n",
      "Loss value: 374.9790890483846\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 542\n",
      "Current point: [-2.46577467 -0.4143944   1.26837625]\n",
      "Gradient: [ 0.00048268  0.06088566 -0.02314608]\n",
      "Loss value: 374.97908047234444\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 543\n",
      "Current point: [-2.46577564 -0.41451617  1.26842254]\n",
      "Gradient: [ 0.00047967  0.06045929 -0.02298418]\n",
      "Loss value: 374.9790720159779\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 544\n",
      "Current point: [-2.4657766  -0.41463709  1.26846851]\n",
      "Gradient: [ 0.00047667  0.06003592 -0.02282341]\n",
      "Loss value: 374.97906367761414\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 545\n",
      "Current point: [-2.46577755 -0.41475716  1.26851416]\n",
      "Gradient: [ 0.00047369  0.05961551 -0.02266377]\n",
      "Loss value: 374.9790554556058\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 546\n",
      "Current point: [-2.4657785  -0.41487639  1.26855949]\n",
      "Gradient: [ 0.00047073  0.05919804 -0.02250525]\n",
      "Loss value: 374.97904734832844\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 547\n",
      "Current point: [-2.46577944 -0.41499479  1.2686045 ]\n",
      "Gradient: [ 0.00046778  0.05878351 -0.02234783]\n",
      "Loss value: 374.97903935418026\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 548\n",
      "Current point: [-2.46578037 -0.41511235  1.26864919]\n",
      "Gradient: [ 0.00046485  0.05837188 -0.02219152]\n",
      "Loss value: 374.979031471582\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 549\n",
      "Current point: [-2.4657813  -0.4152291   1.26869358]\n",
      "Gradient: [ 0.00046193  0.05796313 -0.0220363 ]\n",
      "Loss value: 374.97902369897645\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 550\n",
      "Current point: [-2.46578223 -0.41534502  1.26873765]\n",
      "Gradient: [ 0.00045903  0.05755725 -0.02188216]\n",
      "Loss value: 374.97901603482796\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 551\n",
      "Current point: [-2.46578315 -0.41546014  1.26878141]\n",
      "Gradient: [ 0.00045615  0.05715422 -0.0217291 ]\n",
      "Loss value: 374.97900847762264\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 552\n",
      "Current point: [-2.46578406 -0.41557445  1.26882487]\n",
      "Gradient: [ 0.00045328  0.05675401 -0.02157711]\n",
      "Loss value: 374.9790010258674\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 553\n",
      "Current point: [-2.46578497 -0.41568795  1.26886802]\n",
      "Gradient: [ 0.00045043  0.0563566  -0.02142619]\n",
      "Loss value: 374.9789936780902\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 554\n",
      "Current point: [-2.46578587 -0.41580067  1.26891088]\n",
      "Gradient: [ 0.00044759  0.05596198 -0.02127632]\n",
      "Loss value: 374.97898643283963\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 555\n",
      "Current point: [-2.46578676 -0.41591259  1.26895343]\n",
      "Gradient: [ 0.00044476  0.05557013 -0.0211275 ]\n",
      "Loss value: 374.9789792886844\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 556\n",
      "Current point: [-2.46578765 -0.41602373  1.26899568]\n",
      "Gradient: [ 0.00044196  0.05518103 -0.02097972]\n",
      "Loss value: 374.97897224421325\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 557\n",
      "Current point: [-2.46578853 -0.41613409  1.26903764]\n",
      "Gradient: [ 0.00043917  0.05479465 -0.02083298]\n",
      "Loss value: 374.9789652980347\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 558\n",
      "Current point: [-2.46578941 -0.41624368  1.26907931]\n",
      "Gradient: [ 0.00043639  0.05441098 -0.02068726]\n",
      "Loss value: 374.9789584487768\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 559\n",
      "Current point: [-2.46579029 -0.4163525   1.26912068]\n",
      "Gradient: [ 0.00043363  0.05402999 -0.02054256]\n",
      "Loss value: 374.9789516950864\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 560\n",
      "Current point: [-2.46579115 -0.41646056  1.26916177]\n",
      "Gradient: [ 0.00043088  0.05365168 -0.02039887]\n",
      "Loss value: 374.9789450356295\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 561\n",
      "Current point: [-2.46579201 -0.41656787  1.26920257]\n",
      "Gradient: [ 0.00042815  0.05327602 -0.02025619]\n",
      "Loss value: 374.97893846909096\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 562\n",
      "Current point: [-2.46579287 -0.41667442  1.26924308]\n",
      "Gradient: [ 0.00042544  0.052903   -0.0201145 ]\n",
      "Loss value: 374.97893199417354\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 563\n",
      "Current point: [-2.46579372 -0.41678023  1.26928331]\n",
      "Gradient: [ 0.00042274  0.05253258 -0.01997381]\n",
      "Loss value: 374.97892560959843\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 564\n",
      "Current point: [-2.46579457 -0.41688529  1.26932326]\n",
      "Gradient: [ 0.00042005  0.05216477 -0.0198341 ]\n",
      "Loss value: 374.97891931410453\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 565\n",
      "Current point: [-2.46579541 -0.41698962  1.26936292]\n",
      "Gradient: [ 0.00041738  0.05179953 -0.01969536]\n",
      "Loss value: 374.9789131064485\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 566\n",
      "Current point: [-2.46579624 -0.41709322  1.26940232]\n",
      "Gradient: [ 0.00041472  0.05143685 -0.0195576 ]\n",
      "Loss value: 374.97890698540425\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 567\n",
      "Current point: [-2.46579707 -0.41719609  1.26944143]\n",
      "Gradient: [ 0.00041208  0.05107671 -0.0194208 ]\n",
      "Loss value: 374.9789009497628\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 568\n",
      "Current point: [-2.4657979  -0.41729825  1.26948027]\n",
      "Gradient: [ 0.00040946  0.0507191  -0.01928496]\n",
      "Loss value: 374.97889499833224\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 569\n",
      "Current point: [-2.46579871 -0.41739968  1.26951884]\n",
      "Gradient: [ 0.00040685  0.05036399 -0.01915007]\n",
      "Loss value: 374.978889129937\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 570\n",
      "Current point: [-2.46579953 -0.41750041  1.26955714]\n",
      "Gradient: [ 0.00040425  0.05001137 -0.01901612]\n",
      "Loss value: 374.97888334341815\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 571\n",
      "Current point: [-2.46580034 -0.41760044  1.26959517]\n",
      "Gradient: [ 0.00040167  0.04966122 -0.01888311]\n",
      "Loss value: 374.978877637633\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 572\n",
      "Current point: [-2.46580114 -0.41769976  1.26963294]\n",
      "Gradient: [ 0.0003991   0.04931353 -0.01875103]\n",
      "Loss value: 374.97887201145454\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 573\n",
      "Current point: [-2.46580194 -0.41779838  1.26967044]\n",
      "Gradient: [ 0.00039655  0.04896827 -0.01861987]\n",
      "Loss value: 374.97886646377185\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 574\n",
      "Current point: [-2.46580273 -0.41789632  1.26970768]\n",
      "Gradient: [ 0.00039401  0.04862543 -0.01848963]\n",
      "Loss value: 374.97886099348926\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 575\n",
      "Current point: [-2.46580352 -0.41799357  1.26974466]\n",
      "Gradient: [ 0.00039149  0.048285   -0.0183603 ]\n",
      "Loss value: 374.9788555995264\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 576\n",
      "Current point: [-2.4658043  -0.41809014  1.26978138]\n",
      "Gradient: [ 0.00038898  0.04794695 -0.01823187]\n",
      "Loss value: 374.9788502808183\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 577\n",
      "Current point: [-2.46580508 -0.41818604  1.26981785]\n",
      "Gradient: [ 0.00038648  0.04761127 -0.01810435]\n",
      "Loss value: 374.9788450363144\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 578\n",
      "Current point: [-2.46580585 -0.41828126  1.26985405]\n",
      "Gradient: [ 0.000384    0.04727794 -0.01797771]\n",
      "Loss value: 374.9788398649792\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 579\n",
      "Current point: [-2.46580662 -0.41837581  1.26989001]\n",
      "Gradient: [ 0.00038154  0.04694694 -0.01785197]\n",
      "Loss value: 374.97883476579136\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 580\n",
      "Current point: [-2.46580738 -0.41846971  1.26992571]\n",
      "Gradient: [ 0.00037909  0.04661827 -0.0177271 ]\n",
      "Loss value: 374.97882973774415\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 581\n",
      "Current point: [-2.46580814 -0.41856294  1.26996117]\n",
      "Gradient: [ 0.00037665  0.0462919  -0.0176031 ]\n",
      "Loss value: 374.97882477984456\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 582\n",
      "Current point: [-2.4658089  -0.41865553  1.26999637]\n",
      "Gradient: [ 0.00037422  0.04596781 -0.01747997]\n",
      "Loss value: 374.97881989111363\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 583\n",
      "Current point: [-2.46580964 -0.41874746  1.27003133]\n",
      "Gradient: [ 0.00037181  0.045646   -0.0173577 ]\n",
      "Loss value: 374.97881507058605\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 584\n",
      "Current point: [-2.46581039 -0.41883876  1.27006605]\n",
      "Gradient: [ 0.00036942  0.04532644 -0.01723629]\n",
      "Loss value: 374.9788103173099\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 585\n",
      "Current point: [-2.46581113 -0.41892941  1.27010052]\n",
      "Gradient: [ 0.00036704  0.04500912 -0.01711573]\n",
      "Loss value: 374.97880563034664\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 586\n",
      "Current point: [-2.46581186 -0.41901943  1.27013475]\n",
      "Gradient: [ 0.00036467  0.04469403 -0.01699601]\n",
      "Loss value: 374.9788010087708\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 587\n",
      "Current point: [-2.46581259 -0.41910882  1.27016875]\n",
      "Gradient: [ 0.00036231  0.04438114 -0.01687713]\n",
      "Loss value: 374.9787964516699\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 588\n",
      "Current point: [-2.46581332 -0.41919758  1.2702025 ]\n",
      "Gradient: [ 0.00035997  0.04407044 -0.01675908]\n",
      "Loss value: 374.97879195814403\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 589\n",
      "Current point: [-2.46581404 -0.41928572  1.27023602]\n",
      "Gradient: [ 0.00035765  0.04376192 -0.01664185]\n",
      "Loss value: 374.97878752730617\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 590\n",
      "Current point: [-2.46581475 -0.41937324  1.2702693 ]\n",
      "Gradient: [ 0.00035533  0.04345556 -0.01652545]\n",
      "Loss value: 374.9787831582813\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 591\n",
      "Current point: [-2.46581546 -0.41946015  1.27030235]\n",
      "Gradient: [ 0.00035303  0.04315135 -0.01640986]\n",
      "Loss value: 374.9787788502068\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 592\n",
      "Current point: [-2.46581617 -0.41954646  1.27033517]\n",
      "Gradient: [ 0.00035075  0.04284927 -0.01629507]\n",
      "Loss value: 374.9787746022322\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 593\n",
      "Current point: [-2.46581687 -0.41963215  1.27036776]\n",
      "Gradient: [ 0.00034848  0.04254931 -0.01618109]\n",
      "Loss value: 374.9787704135187\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 594\n",
      "Current point: [-2.46581757 -0.41971725  1.27040012]\n",
      "Gradient: [ 0.00034622  0.04225144 -0.01606791]\n",
      "Loss value: 374.97876628323934\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 595\n",
      "Current point: [-2.46581826 -0.41980176  1.27043226]\n",
      "Gradient: [ 0.00034397  0.04195567 -0.01595552]\n",
      "Loss value: 374.97876221057857\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 596\n",
      "Current point: [-2.46581895 -0.41988567  1.27046417]\n",
      "Gradient: [ 0.00034174  0.04166196 -0.01584392]\n",
      "Loss value: 374.9787581947325\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 597\n",
      "Current point: [-2.46581963 -0.41996899  1.27049586]\n",
      "Gradient: [ 0.00033952  0.04137032 -0.01573309]\n",
      "Loss value: 374.9787542349082\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 598\n",
      "Current point: [-2.46582031 -0.42005173  1.27052733]\n",
      "Gradient: [ 0.00033731  0.04108072 -0.01562304]\n",
      "Loss value: 374.97875033032386\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 599\n",
      "Current point: [-2.46582098 -0.42013389  1.27055857]\n",
      "Gradient: [ 0.00033512  0.04079314 -0.01551376]\n",
      "Loss value: 374.97874648020877\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 600\n",
      "Current point: [-2.46582165 -0.42021548  1.2705896 ]\n",
      "Gradient: [ 0.00033294  0.04050758 -0.01540525]\n",
      "Loss value: 374.9787426838027\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 601\n",
      "Current point: [-2.46582232 -0.4202965   1.27062041]\n",
      "Gradient: [ 0.00033077  0.04022402 -0.01529749]\n",
      "Loss value: 374.97873894035615\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 602\n",
      "Current point: [-2.46582298 -0.42037694  1.270651  ]\n",
      "Gradient: [ 0.00032861  0.03994245 -0.01519049]\n",
      "Loss value: 374.9787352491303\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 603\n",
      "Current point: [-2.46582364 -0.42045683  1.27068139]\n",
      "Gradient: [ 0.00032647  0.03966285 -0.01508424]\n",
      "Loss value: 374.97873160939616\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 604\n",
      "Current point: [-2.46582429 -0.42053615  1.27071155]\n",
      "Gradient: [ 0.00032434  0.03938521 -0.01497873]\n",
      "Loss value: 374.9787280204355\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 605\n",
      "Current point: [-2.46582494 -0.42061492  1.27074151]\n",
      "Gradient: [ 0.00032223  0.03910951 -0.01487396]\n",
      "Loss value: 374.97872448153964\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 606\n",
      "Current point: [-2.46582558 -0.42069314  1.27077126]\n",
      "Gradient: [ 0.00032012  0.03883575 -0.01476992]\n",
      "Loss value: 374.97872099201004\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 607\n",
      "Current point: [-2.46582622 -0.42077081  1.2708008 ]\n",
      "Gradient: [ 0.00031803  0.0385639  -0.0146666 ]\n",
      "Loss value: 374.97871755115784\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 608\n",
      "Current point: [-2.46582686 -0.42084794  1.27083013]\n",
      "Gradient: [ 0.00031596  0.03829396 -0.01456402]\n",
      "Loss value: 374.9787141583038\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 609\n",
      "Current point: [-2.46582749 -0.42092453  1.27085926]\n",
      "Gradient: [ 0.00031389  0.0380259  -0.01446214]\n",
      "Loss value: 374.9787108127781\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 610\n",
      "Current point: [-2.46582812 -0.42100058  1.27088819]\n",
      "Gradient: [ 0.00031184  0.03775973 -0.01436098]\n",
      "Loss value: 374.9787075139205\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 611\n",
      "Current point: [-2.46582874 -0.4210761   1.27091691]\n",
      "Gradient: [ 0.00030979  0.03749542 -0.01426053]\n",
      "Loss value: 374.97870426107966\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 612\n",
      "Current point: [-2.46582936 -0.42115109  1.27094543]\n",
      "Gradient: [ 0.00030777  0.03723296 -0.01416078]\n",
      "Loss value: 374.9787010536135\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 613\n",
      "Current point: [-2.46582998 -0.42122556  1.27097375]\n",
      "Gradient: [ 0.00030575  0.03697234 -0.01406173]\n",
      "Loss value: 374.97869789088907\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 614\n",
      "Current point: [-2.46583059 -0.4212995   1.27100187]\n",
      "Gradient: [ 0.00030375  0.03671354 -0.01396337]\n",
      "Loss value: 374.9786947722818\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 615\n",
      "Current point: [-2.4658312  -0.42137293  1.2710298 ]\n",
      "Gradient: [ 0.00030175  0.03645656 -0.0138657 ]\n",
      "Loss value: 374.9786916971764\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 616\n",
      "Current point: [-2.4658318  -0.42144584  1.27105753]\n",
      "Gradient: [ 0.00029977  0.03620138 -0.01376872]\n",
      "Loss value: 374.9786886649657\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 617\n",
      "Current point: [-2.4658324  -0.42151825  1.27108507]\n",
      "Gradient: [ 0.00029781  0.03594798 -0.01367241]\n",
      "Loss value: 374.9786856750512\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 618\n",
      "Current point: [-2.465833   -0.42159014  1.27111241]\n",
      "Gradient: [ 0.00029585  0.03569636 -0.01357677]\n",
      "Loss value: 374.97868272684286\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 619\n",
      "Current point: [-2.46583359 -0.42166153  1.27113957]\n",
      "Gradient: [ 0.00029391  0.0354465  -0.01348181]\n",
      "Loss value: 374.97867981975867\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 620\n",
      "Current point: [-2.46583418 -0.42173243  1.27116653]\n",
      "Gradient: [ 0.00029197  0.0351984  -0.0133875 ]\n",
      "Loss value: 374.97867695322475\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 621\n",
      "Current point: [-2.46583476 -0.42180282  1.27119331]\n",
      "Gradient: [ 0.00029005  0.03495203 -0.01329386]\n",
      "Loss value: 374.9786741266754\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 622\n",
      "Current point: [-2.46583534 -0.42187273  1.27121989]\n",
      "Gradient: [ 0.00028814  0.03470738 -0.01320087]\n",
      "Loss value: 374.97867133955293\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 623\n",
      "Current point: [-2.46583592 -0.42194214  1.2712463 ]\n",
      "Gradient: [ 0.00028625  0.03446445 -0.01310854]\n",
      "Loss value: 374.9786685913069\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 624\n",
      "Current point: [-2.46583649 -0.42201107  1.27127251]\n",
      "Gradient: [ 0.00028436  0.03422322 -0.01301685]\n",
      "Loss value: 374.97866588139505\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 625\n",
      "Current point: [-2.46583706 -0.42207952  1.27129855]\n",
      "Gradient: [ 0.00028249  0.03398368 -0.0129258 ]\n",
      "Loss value: 374.97866320928256\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 626\n",
      "Current point: [-2.46583762 -0.42214749  1.2713244 ]\n",
      "Gradient: [ 0.00028062  0.03374582 -0.01283538]\n",
      "Loss value: 374.97866057444196\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 627\n",
      "Current point: [-2.46583818 -0.42221498  1.27135007]\n",
      "Gradient: [ 0.00027877  0.03350962 -0.0127456 ]\n",
      "Loss value: 374.97865797635325\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 628\n",
      "Current point: [-2.46583874 -0.422282    1.27137556]\n",
      "Gradient: [ 0.00027693  0.03327508 -0.01265645]\n",
      "Loss value: 374.9786554145037\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 629\n",
      "Current point: [-2.4658393  -0.42234855  1.27140087]\n",
      "Gradient: [ 0.00027511  0.03304218 -0.01256792]\n",
      "Loss value: 374.9786528883876\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 630\n",
      "Current point: [-2.46583985 -0.42241463  1.27142601]\n",
      "Gradient: [ 0.00027329  0.03281091 -0.01248001]\n",
      "Loss value: 374.9786503975066\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 631\n",
      "Current point: [-2.46584039 -0.42248025  1.27145097]\n",
      "Gradient: [ 0.00027148  0.03258126 -0.01239272]\n",
      "Loss value: 374.978647941369\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 632\n",
      "Current point: [-2.46584093 -0.42254542  1.27147575]\n",
      "Gradient: [ 0.00026969  0.03235322 -0.01230603]\n",
      "Loss value: 374.97864551948993\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 633\n",
      "Current point: [-2.46584147 -0.42261012  1.27150037]\n",
      "Gradient: [ 0.00026791  0.03212678 -0.01221995]\n",
      "Loss value: 374.97864313139155\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 634\n",
      "Current point: [-2.46584201 -0.42267438  1.27152481]\n",
      "Gradient: [ 0.00026613  0.03190192 -0.01213448]\n",
      "Loss value: 374.97864077660256\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 635\n",
      "Current point: [-2.46584254 -0.42273818  1.27154907]\n",
      "Gradient: [ 0.00026437  0.03167864 -0.0120496 ]\n",
      "Loss value: 374.97863845465827\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 636\n",
      "Current point: [-2.46584307 -0.42280154  1.27157317]\n",
      "Gradient: [ 0.00026262  0.03145692 -0.01196532]\n",
      "Loss value: 374.9786361651004\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 637\n",
      "Current point: [-2.4658436  -0.42286445  1.2715971 ]\n",
      "Gradient: [ 0.00026088  0.03123675 -0.01188162]\n",
      "Loss value: 374.9786339074771\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 638\n",
      "Current point: [-2.46584412 -0.42292692  1.27162087]\n",
      "Gradient: [ 0.00025915  0.03101813 -0.01179851]\n",
      "Loss value: 374.9786316813429\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 639\n",
      "Current point: [-2.46584464 -0.42298896  1.27164446]\n",
      "Gradient: [ 0.00025743  0.03080103 -0.01171598]\n",
      "Loss value: 374.9786294862584\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 640\n",
      "Current point: [-2.46584515 -0.42305056  1.2716679 ]\n",
      "Gradient: [ 0.00025573  0.03058546 -0.01163403]\n",
      "Loss value: 374.97862732179044\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 641\n",
      "Current point: [-2.46584566 -0.42311173  1.27169116]\n",
      "Gradient: [ 0.00025403  0.0303714  -0.01155265]\n",
      "Loss value: 374.978625187512\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 642\n",
      "Current point: [-2.46584617 -0.42317248  1.27171427]\n",
      "Gradient: [ 0.00025234  0.03015883 -0.01147185]\n",
      "Loss value: 374.9786230830018\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 643\n",
      "Current point: [-2.46584668 -0.42323279  1.27173721]\n",
      "Gradient: [ 0.00025067  0.02994775 -0.0113916 ]\n",
      "Loss value: 374.9786210078445\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 644\n",
      "Current point: [-2.46584718 -0.42329269  1.27176   ]\n",
      "Gradient: [ 0.000249    0.02973816 -0.01131192]\n",
      "Loss value: 374.9786189616307\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 645\n",
      "Current point: [-2.46584767 -0.42335217  1.27178262]\n",
      "Gradient: [ 0.00024735  0.02953003 -0.0112328 ]\n",
      "Loss value: 374.9786169439566\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 646\n",
      "Current point: [-2.46584817 -0.42341123  1.27180509]\n",
      "Gradient: [ 0.0002457   0.02932335 -0.01115422]\n",
      "Loss value: 374.97861495442396\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 647\n",
      "Current point: [-2.46584866 -0.42346987  1.27182739]\n",
      "Gradient: [ 0.00024407  0.02911813 -0.0110762 ]\n",
      "Loss value: 374.9786129926402\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 648\n",
      "Current point: [-2.46584915 -0.42352811  1.27184955]\n",
      "Gradient: [ 0.00024245  0.02891434 -0.01099873]\n",
      "Loss value: 374.97861105821835\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 649\n",
      "Current point: [-2.46584963 -0.42358594  1.27187154]\n",
      "Gradient: [ 0.00024083  0.02871198 -0.01092179]\n",
      "Loss value: 374.97860915077655\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 650\n",
      "Current point: [-2.46585012 -0.42364336  1.27189339]\n",
      "Gradient: [ 0.00023923  0.02851103 -0.0108454 ]\n",
      "Loss value: 374.97860726993844\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 651\n",
      "Current point: [-2.46585059 -0.42370038  1.27191508]\n",
      "Gradient: [ 0.00023763  0.02831149 -0.01076954]\n",
      "Loss value: 374.97860541533294\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 652\n",
      "Current point: [-2.46585107 -0.42375701  1.27193662]\n",
      "Gradient: [ 0.00023605  0.02811335 -0.01069421]\n",
      "Loss value: 374.978603586594\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 653\n",
      "Current point: [-2.46585154 -0.42381323  1.27195801]\n",
      "Gradient: [ 0.00023448  0.0279166  -0.0106194 ]\n",
      "Loss value: 374.9786017833609\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 654\n",
      "Current point: [-2.46585201 -0.42386907  1.27197925]\n",
      "Gradient: [ 0.00023291  0.02772123 -0.01054512]\n",
      "Loss value: 374.9786000052777\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 655\n",
      "Current point: [-2.46585248 -0.42392451  1.27200034]\n",
      "Gradient: [ 0.00023136  0.02752722 -0.01047136]\n",
      "Loss value: 374.9785982519937\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 656\n",
      "Current point: [-2.46585294 -0.42397956  1.27202128]\n",
      "Gradient: [ 0.00022982  0.02733457 -0.01039811]\n",
      "Loss value: 374.97859652316276\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 657\n",
      "Current point: [-2.4658534  -0.42403423  1.27204207]\n",
      "Gradient: [ 0.00022828  0.02714327 -0.01032538]\n",
      "Loss value: 374.9785948184439\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 658\n",
      "Current point: [-2.46585386 -0.42408852  1.27206273]\n",
      "Gradient: [ 0.00022676  0.02695331 -0.01025316]\n",
      "Loss value: 374.97859313750075\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 659\n",
      "Current point: [-2.46585431 -0.42414243  1.27208323]\n",
      "Gradient: [ 0.00022524  0.02676468 -0.01018144]\n",
      "Loss value: 374.97859148000157\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 660\n",
      "Current point: [-2.46585476 -0.42419596  1.27210359]\n",
      "Gradient: [ 0.00022374  0.02657737 -0.01011022]\n",
      "Loss value: 374.97858984561935\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 661\n",
      "Current point: [-2.46585521 -0.42424911  1.27212381]\n",
      "Gradient: [ 0.00022224  0.02639137 -0.0100395 ]\n",
      "Loss value: 374.97858823403163\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 662\n",
      "Current point: [-2.46585565 -0.42430189  1.27214389]\n",
      "Gradient: [ 0.00022076  0.02620668 -0.00996928]\n",
      "Loss value: 374.97858664492037\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 663\n",
      "Current point: [-2.46585609 -0.42435431  1.27216383]\n",
      "Gradient: [ 0.00021928  0.02602327 -0.00989954]\n",
      "Loss value: 374.9785850779721\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 664\n",
      "Current point: [-2.46585653 -0.42440635  1.27218363]\n",
      "Gradient: [ 0.00021781  0.02584115 -0.0098303 ]\n",
      "Loss value: 374.9785835328777\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 665\n",
      "Current point: [-2.46585697 -0.42445803  1.27220329]\n",
      "Gradient: [ 0.00021635  0.02566031 -0.00976154]\n",
      "Loss value: 374.9785820093322\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 666\n",
      "Current point: [-2.4658574  -0.42450936  1.27222282]\n",
      "Gradient: [ 0.00021491  0.02548073 -0.00969326]\n",
      "Loss value: 374.978580507035\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 667\n",
      "Current point: [-2.46585783 -0.42456032  1.2722422 ]\n",
      "Gradient: [ 0.00021347  0.02530241 -0.00962546]\n",
      "Loss value: 374.97857902568984\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 668\n",
      "Current point: [-2.46585826 -0.42461092  1.27226145]\n",
      "Gradient: [ 0.00021204  0.02512534 -0.00955813]\n",
      "Loss value: 374.97857756500434\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 669\n",
      "Current point: [-2.46585868 -0.42466117  1.27228057]\n",
      "Gradient: [ 0.00021062  0.02494951 -0.00949127]\n",
      "Loss value: 374.97857612469033\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 670\n",
      "Current point: [-2.4658591  -0.42471107  1.27229955]\n",
      "Gradient: [ 0.0002092   0.02477491 -0.00942488]\n",
      "Loss value: 374.97857470446365\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 671\n",
      "Current point: [-2.46585952 -0.42476062  1.2723184 ]\n",
      "Gradient: [ 0.0002078   0.02460153 -0.00935895]\n",
      "Loss value: 374.97857330404406\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 672\n",
      "Current point: [-2.46585994 -0.42480982  1.27233712]\n",
      "Gradient: [ 0.00020641  0.02442937 -0.00929349]\n",
      "Loss value: 374.9785719231554\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 673\n",
      "Current point: [-2.46586035 -0.42485868  1.27235571]\n",
      "Gradient: [ 0.00020502  0.02425841 -0.00922848]\n",
      "Loss value: 374.9785705615251\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 674\n",
      "Current point: [-2.46586076 -0.4249072   1.27237416]\n",
      "Gradient: [ 0.00020364  0.02408865 -0.00916393]\n",
      "Loss value: 374.97856921888445\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 675\n",
      "Current point: [-2.46586117 -0.42495538  1.27239249]\n",
      "Gradient: [ 0.00020228  0.02392007 -0.00909983]\n",
      "Loss value: 374.97856789496876\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 676\n",
      "Current point: [-2.46586157 -0.42500322  1.27241069]\n",
      "Gradient: [ 0.00020092  0.02375268 -0.00903618]\n",
      "Loss value: 374.97856658951673\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 677\n",
      "Current point: [-2.46586197 -0.42505072  1.27242876]\n",
      "Gradient: [ 0.00019957  0.02358646 -0.00897297]\n",
      "Loss value: 374.9785653022709\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 678\n",
      "Current point: [-2.46586237 -0.4250979   1.27244671]\n",
      "Gradient: [ 0.00019823  0.0234214  -0.00891021]\n",
      "Loss value: 374.97856403297715\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 679\n",
      "Current point: [-2.46586277 -0.42514474  1.27246453]\n",
      "Gradient: [ 0.00019689  0.0232575  -0.00884788]\n",
      "Loss value: 374.9785627813852\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 680\n",
      "Current point: [-2.46586316 -0.42519125  1.27248223]\n",
      "Gradient: [ 0.00019557  0.02309475 -0.00878599]\n",
      "Loss value: 374.9785615472481\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 681\n",
      "Current point: [-2.46586355 -0.42523744  1.2724998 ]\n",
      "Gradient: [ 0.00019426  0.02293313 -0.00872454]\n",
      "Loss value: 374.97856033032235\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 682\n",
      "Current point: [-2.46586394 -0.42528331  1.27251725]\n",
      "Gradient: [ 0.00019295  0.02277265 -0.00866351]\n",
      "Loss value: 374.9785591303679\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 683\n",
      "Current point: [-2.46586433 -0.42532885  1.27253457]\n",
      "Gradient: [ 0.00019165  0.02261329 -0.00860291]\n",
      "Loss value: 374.9785579471481\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 684\n",
      "Current point: [-2.46586471 -0.42537408  1.27255178]\n",
      "Gradient: [ 0.00019036  0.02245504 -0.00854274]\n",
      "Loss value: 374.97855678042936\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 685\n",
      "Current point: [-2.46586509 -0.42541899  1.27256886]\n",
      "Gradient: [ 0.00018908  0.02229791 -0.00848298]\n",
      "Loss value: 374.97855562998166\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 686\n",
      "Current point: [-2.46586547 -0.42546359  1.27258583]\n",
      "Gradient: [ 0.0001878   0.02214187 -0.00842364]\n",
      "Loss value: 374.978554495578\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 687\n",
      "Current point: [-2.46586584 -0.42550787  1.27260268]\n",
      "Gradient: [ 0.00018654  0.02198692 -0.00836472]\n",
      "Loss value: 374.97855337699457\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 688\n",
      "Current point: [-2.46586622 -0.42555184  1.27261941]\n",
      "Gradient: [ 0.00018528  0.02183306 -0.00830621]\n",
      "Loss value: 374.9785522740108\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 689\n",
      "Current point: [-2.46586659 -0.42559551  1.27263602]\n",
      "Gradient: [ 0.00018403  0.02168028 -0.00824811]\n",
      "Loss value: 374.97855118640894\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 690\n",
      "Current point: [-2.46586696 -0.42563887  1.27265252]\n",
      "Gradient: [ 0.00018279  0.02152857 -0.00819042]\n",
      "Loss value: 374.9785501139745\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 691\n",
      "Current point: [-2.46586732 -0.42568193  1.2726689 ]\n",
      "Gradient: [ 0.00018156  0.02137792 -0.00813313]\n",
      "Loss value: 374.97854905649604\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 692\n",
      "Current point: [-2.46586769 -0.42572468  1.27268516]\n",
      "Gradient: [ 0.00018033  0.02122832 -0.00807624]\n",
      "Loss value: 374.9785480137648\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 693\n",
      "Current point: [-2.46586805 -0.42576714  1.27270132]\n",
      "Gradient: [ 0.00017912  0.02107977 -0.00801974]\n",
      "Loss value: 374.9785469855751\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 694\n",
      "Current point: [-2.4658684  -0.4258093   1.27271735]\n",
      "Gradient: [ 0.00017791  0.02093226 -0.00796365]\n",
      "Loss value: 374.97854597172426\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 695\n",
      "Current point: [-2.46586876 -0.42585116  1.27273328]\n",
      "Gradient: [ 0.0001767   0.02078579 -0.00790794]\n",
      "Loss value: 374.9785449720122\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 696\n",
      "Current point: [-2.46586911 -0.42589274  1.2727491 ]\n",
      "Gradient: [ 0.00017551  0.02064034 -0.00785263]\n",
      "Loss value: 374.97854398624156\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 697\n",
      "Current point: [-2.46586946 -0.42593402  1.2727648 ]\n",
      "Gradient: [ 0.00017433  0.0204959  -0.0077977 ]\n",
      "Loss value: 374.9785430142181\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 698\n",
      "Current point: [-2.46586981 -0.42597501  1.2727804 ]\n",
      "Gradient: [ 0.00017315  0.02035248 -0.00774316]\n",
      "Loss value: 374.97854205575\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 699\n",
      "Current point: [-2.46587016 -0.42601571  1.27279589]\n",
      "Gradient: [ 0.00017198  0.02021006 -0.00768899]\n",
      "Loss value: 374.97854111064817\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 700\n",
      "Current point: [-2.4658705  -0.42605613  1.27281126]\n",
      "Gradient: [ 0.00017081  0.02006864 -0.00763521]\n",
      "Loss value: 374.97854017872623\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 701\n",
      "Current point: [-2.46587084 -0.42609627  1.27282653]\n",
      "Gradient: [ 0.00016966  0.01992821 -0.0075818 ]\n",
      "Loss value: 374.9785392598003\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 702\n",
      "Current point: [-2.46587118 -0.42613613  1.2728417 ]\n",
      "Gradient: [ 0.00016851  0.01978876 -0.00752877]\n",
      "Loss value: 374.97853835368915\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 703\n",
      "Current point: [-2.46587152 -0.42617571  1.27285675]\n",
      "Gradient: [ 0.00016737  0.01965029 -0.00747611]\n",
      "Loss value: 374.97853746021406\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 704\n",
      "Current point: [-2.46587186 -0.42621501  1.27287171]\n",
      "Gradient: [ 0.00016624  0.01951279 -0.00742381]\n",
      "Loss value: 374.97853657919876\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 705\n",
      "Current point: [-2.46587219 -0.42625403  1.27288655]\n",
      "Gradient: [ 0.00016511  0.01937625 -0.00737188]\n",
      "Loss value: 374.9785357104695\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 706\n",
      "Current point: [-2.46587252 -0.42629278  1.2729013 ]\n",
      "Gradient: [ 0.00016399  0.01924067 -0.00732032]\n",
      "Loss value: 374.97853485385485\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 707\n",
      "Current point: [-2.46587285 -0.42633127  1.27291594]\n",
      "Gradient: [ 0.00016288  0.01910603 -0.00726911]\n",
      "Loss value: 374.97853400918586\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 708\n",
      "Current point: [-2.46587317 -0.42636948  1.27293048]\n",
      "Gradient: [ 0.00016178  0.01897234 -0.00721827]\n",
      "Loss value: 374.97853317629597\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 709\n",
      "Current point: [-2.4658735  -0.42640742  1.27294491]\n",
      "Gradient: [ 0.00016068  0.01883958 -0.00716778]\n",
      "Loss value: 374.97853235502095\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 710\n",
      "Current point: [-2.46587382 -0.4264451   1.27295925]\n",
      "Gradient: [ 0.00015959  0.01870775 -0.00711764]\n",
      "Loss value: 374.9785315451986\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 711\n",
      "Current point: [-2.46587414 -0.42648252  1.27297348]\n",
      "Gradient: [ 0.00015851  0.01857685 -0.00706785]\n",
      "Loss value: 374.97853074666943\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 712\n",
      "Current point: [-2.46587445 -0.42651967  1.27298762]\n",
      "Gradient: [ 0.00015744  0.01844686 -0.00701841]\n",
      "Loss value: 374.9785299592757\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 713\n",
      "Current point: [-2.46587477 -0.42655656  1.27300166]\n",
      "Gradient: [ 0.00015637  0.01831778 -0.00696932]\n",
      "Loss value: 374.9785291828623\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 714\n",
      "Current point: [-2.46587508 -0.4265932   1.2730156 ]\n",
      "Gradient: [ 0.00015531  0.01818961 -0.00692057]\n",
      "Loss value: 374.97852841727604\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 715\n",
      "Current point: [-2.46587539 -0.42662958  1.27302944]\n",
      "Gradient: [ 0.00015426  0.01806233 -0.00687216]\n",
      "Loss value: 374.97852766236576\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 716\n",
      "Current point: [-2.4658757  -0.4266657   1.27304318]\n",
      "Gradient: [ 0.00015321  0.01793594 -0.00682409]\n",
      "Loss value: 374.9785269179827\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 717\n",
      "Current point: [-2.46587601 -0.42670158  1.27305683]\n",
      "Gradient: [ 0.00015217  0.01781044 -0.00677636]\n",
      "Loss value: 374.9785261839801\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 718\n",
      "Current point: [-2.46587631 -0.4267372   1.27307038]\n",
      "Gradient: [ 0.00015114  0.01768582 -0.00672896]\n",
      "Loss value: 374.97852546021306\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 719\n",
      "Current point: [-2.46587661 -0.42677257  1.27308384]\n",
      "Gradient: [ 0.00015011  0.01756207 -0.00668189]\n",
      "Loss value: 374.9785247465388\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 720\n",
      "Current point: [-2.46587691 -0.42680769  1.2730972 ]\n",
      "Gradient: [ 0.00014909  0.01743918 -0.00663515]\n",
      "Loss value: 374.97852404281673\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 721\n",
      "Current point: [-2.46587721 -0.42684257  1.27311047]\n",
      "Gradient: [ 0.00014808  0.01731715 -0.00658874]\n",
      "Loss value: 374.97852334890797\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 722\n",
      "Current point: [-2.46587751 -0.4268772   1.27312365]\n",
      "Gradient: [ 0.00014707  0.01719598 -0.00654265]\n",
      "Loss value: 374.9785226646755\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 723\n",
      "Current point: [-2.4658778  -0.4269116   1.27313674]\n",
      "Gradient: [ 0.00014607  0.01707566 -0.00649689]\n",
      "Loss value: 374.9785219899846\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 724\n",
      "Current point: [-2.46587809 -0.42694575  1.27314973]\n",
      "Gradient: [ 0.00014508  0.01695618 -0.00645144]\n",
      "Loss value: 374.978521324702\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 725\n",
      "Current point: [-2.46587838 -0.42697966  1.27316263]\n",
      "Gradient: [ 0.00014409  0.01683753 -0.00640632]\n",
      "Loss value: 374.97852066869666\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 726\n",
      "Current point: [-2.46587867 -0.42701334  1.27317545]\n",
      "Gradient: [ 0.00014311  0.01671972 -0.00636151]\n",
      "Loss value: 374.9785200218392\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 727\n",
      "Current point: [-2.46587896 -0.42704677  1.27318817]\n",
      "Gradient: [ 0.00014214  0.01660273 -0.00631701]\n",
      "Loss value: 374.9785193840018\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 728\n",
      "Current point: [-2.46587924 -0.42707998  1.2732008 ]\n",
      "Gradient: [ 0.00014117  0.01648656 -0.00627282]\n",
      "Loss value: 374.9785187550589\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 729\n",
      "Current point: [-2.46587953 -0.42711295  1.27321335]\n",
      "Gradient: [ 0.00014021  0.0163712  -0.00622894]\n",
      "Loss value: 374.97851813488626\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 730\n",
      "Current point: [-2.46587981 -0.4271457   1.27322581]\n",
      "Gradient: [ 0.00013925  0.01625665 -0.00618537]\n",
      "Loss value: 374.9785175233618\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 731\n",
      "Current point: [-2.46588008 -0.42717821  1.27323818]\n",
      "Gradient: [ 0.00013831  0.0161429  -0.00614211]\n",
      "Loss value: 374.9785169203647\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 732\n",
      "Current point: [-2.46588036 -0.42721049  1.27325046]\n",
      "Gradient: [ 0.00013736  0.01602995 -0.00609914]\n",
      "Loss value: 374.9785163257762\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 733\n",
      "Current point: [-2.46588064 -0.42724255  1.27326266]\n",
      "Gradient: [ 0.00013643  0.01591779 -0.00605648]\n",
      "Loss value: 374.97851573947895\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 734\n",
      "Current point: [-2.46588091 -0.42727439  1.27327477]\n",
      "Gradient: [ 0.0001355   0.01580641 -0.00601412]\n",
      "Loss value: 374.9785151613572\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 735\n",
      "Current point: [-2.46588118 -0.427306    1.2732868 ]\n",
      "Gradient: [ 0.00013458  0.01569582 -0.00597205]\n",
      "Loss value: 374.97851459129714\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 736\n",
      "Current point: [-2.46588145 -0.42733739  1.27329875]\n",
      "Gradient: [ 0.00013366  0.01558599 -0.00593028]\n",
      "Loss value: 374.9785140291863\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 737\n",
      "Current point: [-2.46588172 -0.42736857  1.27331061]\n",
      "Gradient: [ 0.00013275  0.01547694 -0.00588879]\n",
      "Loss value: 374.9785134749137\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 738\n",
      "Current point: [-2.46588198 -0.42739952  1.27332238]\n",
      "Gradient: [ 0.00013184  0.01536865 -0.0058476 ]\n",
      "Loss value: 374.97851292837004\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 739\n",
      "Current point: [-2.46588225 -0.42743026  1.27333408]\n",
      "Gradient: [ 0.00013094  0.01526111 -0.0058067 ]\n",
      "Loss value: 374.9785123894477\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 740\n",
      "Current point: [-2.46588251 -0.42746078  1.27334569]\n",
      "Gradient: [ 0.00013005  0.01515433 -0.00576608]\n",
      "Loss value: 374.9785118580403\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 741\n",
      "Current point: [-2.46588277 -0.42749109  1.27335722]\n",
      "Gradient: [ 0.00012916  0.0150483  -0.00572575]\n",
      "Loss value: 374.9785113340429\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 742\n",
      "Current point: [-2.46588303 -0.42752119  1.27336868]\n",
      "Gradient: [ 0.00012828  0.01494301 -0.0056857 ]\n",
      "Loss value: 374.97851081735234\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 743\n",
      "Current point: [-2.46588328 -0.42755107  1.27338005]\n",
      "Gradient: [ 0.00012741  0.01483845 -0.00564593]\n",
      "Loss value: 374.9785103078666\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 744\n",
      "Current point: [-2.46588354 -0.42758075  1.27339134]\n",
      "Gradient: [ 0.00012654  0.01473463 -0.00560643]\n",
      "Loss value: 374.97850980548526\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 745\n",
      "Current point: [-2.46588379 -0.42761022  1.27340255]\n",
      "Gradient: [ 0.00012567  0.01463154 -0.00556722]\n",
      "Loss value: 374.9785093101093\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 746\n",
      "Current point: [-2.46588404 -0.42763948  1.27341369]\n",
      "Gradient: [ 0.00012482  0.01452916 -0.00552828]\n",
      "Loss value: 374.978508821641\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 747\n",
      "Current point: [-2.46588429 -0.42766854  1.27342474]\n",
      "Gradient: [ 0.00012396  0.0144275  -0.00548961]\n",
      "Loss value: 374.97850833998393\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 748\n",
      "Current point: [-2.46588454 -0.42769739  1.27343572]\n",
      "Gradient: [ 0.00012312  0.01432656 -0.00545121]\n",
      "Loss value: 374.9785078650432\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 749\n",
      "Current point: [-2.46588479 -0.42772605  1.27344662]\n",
      "Gradient: [ 0.00012228  0.01422632 -0.00541308]\n",
      "Loss value: 374.9785073967251\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 750\n",
      "Current point: [-2.46588503 -0.4277545   1.27345745]\n",
      "Gradient: [ 0.00012144  0.01412678 -0.00537521]\n",
      "Loss value: 374.9785069349373\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 751\n",
      "Current point: [-2.46588527 -0.42778275  1.2734682 ]\n",
      "Gradient: [ 0.00012061  0.01402794 -0.00533761]\n",
      "Loss value: 374.97850647958865\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 752\n",
      "Current point: [-2.46588551 -0.42781081  1.27347888]\n",
      "Gradient: [ 0.00011979  0.01392979 -0.00530028]\n",
      "Loss value: 374.9785060305895\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 753\n",
      "Current point: [-2.46588575 -0.42783867  1.27348948]\n",
      "Gradient: [ 0.00011897  0.01383232 -0.0052632 ]\n",
      "Loss value: 374.97850558785115\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 754\n",
      "Current point: [-2.46588599 -0.42786633  1.2735    ]\n",
      "Gradient: [ 0.00011815  0.01373554 -0.00522639]\n",
      "Loss value: 374.97850515128636\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 755\n",
      "Current point: [-2.46588623 -0.4278938   1.27351046]\n",
      "Gradient: [ 0.00011735  0.01363944 -0.00518983]\n",
      "Loss value: 374.97850472080904\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 756\n",
      "Current point: [-2.46588646 -0.42792108  1.27352084]\n",
      "Gradient: [ 0.00011654  0.01354401 -0.00515353]\n",
      "Loss value: 374.9785042963343\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 757\n",
      "Current point: [-2.4658867  -0.42794817  1.27353114]\n",
      "Gradient: [ 0.00011575  0.01344925 -0.00511748]\n",
      "Loss value: 374.9785038777784\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 758\n",
      "Current point: [-2.46588693 -0.42797507  1.27354138]\n",
      "Gradient: [ 0.00011495  0.01335515 -0.00508168]\n",
      "Loss value: 374.9785034650589\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 759\n",
      "Current point: [-2.46588716 -0.42800178  1.27355154]\n",
      "Gradient: [ 0.00011417  0.0132617  -0.00504614]\n",
      "Loss value: 374.97850305809425\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 760\n",
      "Current point: [-2.46588739 -0.4280283   1.27356163]\n",
      "Gradient: [ 0.00011339  0.01316892 -0.00501084]\n",
      "Loss value: 374.9785026568043\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 761\n",
      "Current point: [-2.46588761 -0.42805464  1.27357165]\n",
      "Gradient: [ 0.00011261  0.01307678 -0.00497579]\n",
      "Loss value: 374.9785022611099\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 762\n",
      "Current point: [-2.46588784 -0.4280808   1.27358161]\n",
      "Gradient: [ 0.00011184  0.01298529 -0.00494099]\n",
      "Loss value: 374.97850187093303\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 763\n",
      "Current point: [-2.46588806 -0.42810677  1.27359149]\n",
      "Gradient: [ 0.00011107  0.01289443 -0.00490642]\n",
      "Loss value: 374.9785014861967\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 764\n",
      "Current point: [-2.46588828 -0.42813255  1.2736013 ]\n",
      "Gradient: [ 0.00011031  0.01280421 -0.0048721 ]\n",
      "Loss value: 374.97850110682504\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 765\n",
      "Current point: [-2.4658885  -0.42815816  1.27361105]\n",
      "Gradient: [ 0.00010956  0.01271463 -0.00483802]\n",
      "Loss value: 374.97850073274327\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 766\n",
      "Current point: [-2.46588872 -0.42818359  1.27362072]\n",
      "Gradient: [ 0.00010881  0.01262567 -0.00480418]\n",
      "Loss value: 374.97850036387763\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 767\n",
      "Current point: [-2.46588894 -0.42820884  1.27363033]\n",
      "Gradient: [ 0.00010806  0.01253733 -0.00477058]\n",
      "Loss value: 374.9785000001554\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 768\n",
      "Current point: [-2.46588916 -0.42823392  1.27363987]\n",
      "Gradient: [ 0.00010732  0.01244962 -0.00473721]\n",
      "Loss value: 374.97849964150475\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 769\n",
      "Current point: [-2.46588937 -0.42825882  1.27364935]\n",
      "Gradient: [ 0.00010659  0.01236251 -0.00470407]\n",
      "Loss value: 374.97849928785513\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 770\n",
      "Current point: [-2.46588958 -0.42828354  1.27365875]\n",
      "Gradient: [ 0.00010586  0.01227602 -0.00467117]\n",
      "Loss value: 374.9784989391366\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 771\n",
      "Current point: [-2.4658898  -0.42830809  1.2736681 ]\n",
      "Gradient: [ 0.00010513  0.01219013 -0.00463849]\n",
      "Loss value: 374.9784985952805\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 772\n",
      "Current point: [-2.46589001 -0.42833247  1.27367737]\n",
      "Gradient: [ 0.00010441  0.01210484 -0.00460605]\n",
      "Loss value: 374.97849825621904\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 773\n",
      "Current point: [-2.46589021 -0.42835668  1.27368658]\n",
      "Gradient: [ 0.00010369  0.01202015 -0.00457383]\n",
      "Loss value: 374.9784979218854\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 774\n",
      "Current point: [-2.46589042 -0.42838072  1.27369573]\n",
      "Gradient: [ 0.00010298  0.01193605 -0.00454183]\n",
      "Loss value: 374.97849759221344\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 775\n",
      "Current point: [-2.46589063 -0.4284046   1.27370482]\n",
      "Gradient: [ 0.00010228  0.01185254 -0.00451007]\n",
      "Loss value: 374.9784972671384\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 776\n",
      "Current point: [-2.46589083 -0.4284283   1.27371384]\n",
      "Gradient: [ 0.00010157  0.01176961 -0.00447852]\n",
      "Loss value: 374.978496946596\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 777\n",
      "Current point: [-2.46589104 -0.42845184  1.27372279]\n",
      "Gradient: [ 0.00010088  0.01168727 -0.00444719]\n",
      "Loss value: 374.9784966305232\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 778\n",
      "Current point: [-2.46589124 -0.42847522  1.27373169]\n",
      "Gradient: [ 0.00010018  0.0116055  -0.00441608]\n",
      "Loss value: 374.97849631885754\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 779\n",
      "Current point: [-2.46589144 -0.42849843  1.27374052]\n",
      "Gradient: [ 9.94972693e-05  1.15242993e-02 -4.38519350e-03]\n",
      "Loss value: 374.9784960115376\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 780\n",
      "Current point: [-2.46589164 -0.42852148  1.27374929]\n",
      "Gradient: [ 9.88143408e-05  1.14436704e-02 -4.35451955e-03]\n",
      "Loss value: 374.9784957085028\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 781\n",
      "Current point: [-2.46589183 -0.42854436  1.273758  ]\n",
      "Gradient: [ 9.81360074e-05  1.13636057e-02 -4.32406015e-03]\n",
      "Loss value: 374.9784954096933\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 782\n",
      "Current point: [-2.46589203 -0.42856709  1.27376665]\n",
      "Gradient: [ 9.74622394e-05  1.12841012e-02 -4.29381381e-03]\n",
      "Loss value: 374.9784951150503\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 783\n",
      "Current point: [-2.46589223 -0.42858966  1.27377524]\n",
      "Gradient: [ 9.67930074e-05  1.12051532e-02 -4.26377904e-03]\n",
      "Loss value: 374.9784948245156\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 784\n",
      "Current point: [-2.46589242 -0.42861207  1.27378376]\n",
      "Gradient: [ 9.61282822e-05  1.11267575e-02 -4.23395435e-03]\n",
      "Loss value: 374.978494538032\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 785\n",
      "Current point: [-2.46589261 -0.42863432  1.27379223]\n",
      "Gradient: [ 9.54680346e-05  1.10489105e-02 -4.20433829e-03]\n",
      "Loss value: 374.97849425554296\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 786\n",
      "Current point: [-2.4658928  -0.42865642  1.27380064]\n",
      "Gradient: [ 9.48122357e-05  1.09716083e-02 -4.17492938e-03]\n",
      "Loss value: 374.9784939769928\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 787\n",
      "Current point: [-2.46589299 -0.42867836  1.27380899]\n",
      "Gradient: [ 9.41608569e-05  1.08948470e-02 -4.14572619e-03]\n",
      "Loss value: 374.97849370232655\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 788\n",
      "Current point: [-2.46589318 -0.42870015  1.27381728]\n",
      "Gradient: [ 9.35138694e-05  1.08186228e-02 -4.11672727e-03]\n",
      "Loss value: 374.97849343149005\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 789\n",
      "Current point: [-2.46589337 -0.42872179  1.27382551]\n",
      "Gradient: [ 9.28712449e-05  1.07429321e-02 -4.08793119e-03]\n",
      "Loss value: 374.97849316442995\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 790\n",
      "Current point: [-2.46589355 -0.42874328  1.27383369]\n",
      "Gradient: [ 9.22329551e-05  1.06677710e-02 -4.05933653e-03]\n",
      "Loss value: 374.9784929010936\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 791\n",
      "Current point: [-2.46589374 -0.42876461  1.27384181]\n",
      "Gradient: [ 9.15989720e-05  1.05931358e-02 -4.03094189e-03]\n",
      "Loss value: 374.978492641429\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 792\n",
      "Current point: [-2.46589392 -0.4287858   1.27384987]\n",
      "Gradient: [ 9.09692677e-05  1.05190230e-02 -4.00274587e-03]\n",
      "Loss value: 374.978492385385\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 793\n",
      "Current point: [-2.4658941  -0.42880684  1.27385788]\n",
      "Gradient: [ 9.03438143e-05  1.04454287e-02 -3.97474707e-03]\n",
      "Loss value: 374.9784921329111\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 794\n",
      "Current point: [-2.46589428 -0.42882773  1.27386583]\n",
      "Gradient: [ 8.97225843e-05  1.03723495e-02 -3.94694412e-03]\n",
      "Loss value: 374.9784918839574\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 795\n",
      "Current point: [-2.46589446 -0.42884847  1.27387372]\n",
      "Gradient: [ 8.91055502e-05  1.02997816e-02 -3.91933565e-03]\n",
      "Loss value: 374.9784916384751\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 796\n",
      "Current point: [-2.46589464 -0.42886907  1.27388156]\n",
      "Gradient: [ 8.84926848e-05  1.02277215e-02 -3.89192030e-03]\n",
      "Loss value: 374.9784913964155\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 797\n",
      "Current point: [-2.46589482 -0.42888953  1.27388934]\n",
      "Gradient: [ 8.78839611e-05  1.01561657e-02 -3.86469671e-03]\n",
      "Loss value: 374.978491157731\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 798\n",
      "Current point: [-2.46589499 -0.42890984  1.27389707]\n",
      "Gradient: [ 8.72793520e-05  1.00851106e-02 -3.83766354e-03]\n",
      "Loss value: 374.9784909223746\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 799\n",
      "Current point: [-2.46589517 -0.42893001  1.27390475]\n",
      "Gradient: [ 8.66788307e-05  1.00145527e-02 -3.81081947e-03]\n",
      "Loss value: 374.9784906902998\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 800\n",
      "Current point: [-2.46589534 -0.42895004  1.27391237]\n",
      "Gradient: [ 8.60823707e-05  9.94448849e-03 -3.78416317e-03]\n",
      "Loss value: 374.9784904614608\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 801\n",
      "Current point: [-2.46589551 -0.42896993  1.27391994]\n",
      "Gradient: [ 8.54899455e-05  9.87491459e-03 -3.75769333e-03]\n",
      "Loss value: 374.9784902358126\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 802\n",
      "Current point: [-2.46589569 -0.42898968  1.27392745]\n",
      "Gradient: [ 8.49015287e-05  9.80582754e-03 -3.73140864e-03]\n",
      "Loss value: 374.97849001331065\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 803\n",
      "Current point: [-2.46589585 -0.42900929  1.27393491]\n",
      "Gradient: [ 8.43170943e-05  9.73722392e-03 -3.70530780e-03]\n",
      "Loss value: 374.9784897939111\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 804\n",
      "Current point: [-2.46589602 -0.42902876  1.27394233]\n",
      "Gradient: [ 8.37366162e-05  9.66910035e-03 -3.67938954e-03]\n",
      "Loss value: 374.9784895775706\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 805\n",
      "Current point: [-2.46589619 -0.4290481   1.27394968]\n",
      "Gradient: [ 8.31600686e-05  9.60145348e-03 -3.65365257e-03]\n",
      "Loss value: 374.97848936424657\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 806\n",
      "Current point: [-2.46589636 -0.4290673   1.27395699]\n",
      "Gradient: [ 8.25874258e-05  9.53427996e-03 -3.62809563e-03]\n",
      "Loss value: 374.978489153897\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 807\n",
      "Current point: [-2.46589652 -0.42908637  1.27396425]\n",
      "Gradient: [ 8.20186623e-05  9.46757648e-03 -3.60271746e-03]\n",
      "Loss value: 374.9784889464803\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 808\n",
      "Current point: [-2.46589669 -0.42910531  1.27397145]\n",
      "Gradient: [ 8.14537528e-05  9.40133974e-03 -3.57751680e-03]\n",
      "Loss value: 374.97848874195563\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 809\n",
      "Current point: [-2.46589685 -0.42912411  1.27397861]\n",
      "Gradient: [ 8.08926719e-05  9.33556650e-03 -3.55249242e-03]\n",
      "Loss value: 374.9784885402826\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 810\n",
      "Current point: [-2.46589701 -0.42914278  1.27398571]\n",
      "Gradient: [ 8.03353946e-05  9.27025349e-03 -3.52764308e-03]\n",
      "Loss value: 374.9784883414216\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 811\n",
      "Current point: [-2.46589717 -0.42916132  1.27399277]\n",
      "Gradient: [ 7.97818961e-05  9.20539750e-03 -3.50296756e-03]\n",
      "Loss value: 374.9784881453333\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 812\n",
      "Current point: [-2.46589733 -0.42917973  1.27399977]\n",
      "Gradient: [ 7.92321515e-05  9.14099533e-03 -3.47846464e-03]\n",
      "Loss value: 374.978487951979\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 813\n",
      "Current point: [-2.46589749 -0.42919801  1.27400673]\n",
      "Gradient: [ 7.86861362e-05  9.07704379e-03 -3.45413311e-03]\n",
      "Loss value: 374.9784877613207\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 814\n",
      "Current point: [-2.46589765 -0.42921617  1.27401364]\n",
      "Gradient: [ 7.81438258e-05  9.01353975e-03 -3.42997178e-03]\n",
      "Loss value: 374.97848757332076\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 815\n",
      "Current point: [-2.4658978 -0.4292342  1.2740205]\n",
      "Gradient: [ 7.76051959e-05  8.95048006e-03 -3.40597945e-03]\n",
      "Loss value: 374.9784873879421\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 816\n",
      "Current point: [-2.46589796 -0.4292521   1.27402731]\n",
      "Gradient: [ 7.70702224e-05  8.88786162e-03 -3.38215495e-03]\n",
      "Loss value: 374.9784872051481\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 817\n",
      "Current point: [-2.46589811 -0.42926987  1.27403408]\n",
      "Gradient: [ 7.65388812e-05  8.82568133e-03 -3.35849710e-03]\n",
      "Loss value: 374.9784870249028\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 818\n",
      "Current point: [-2.46589827 -0.42928752  1.27404079]\n",
      "Gradient: [ 7.60111485e-05  8.76393614e-03 -3.33500473e-03]\n",
      "Loss value: 374.9784868471707\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 819\n",
      "Current point: [-2.46589842 -0.42930505  1.27404746]\n",
      "Gradient: [ 7.54870006e-05  8.70262298e-03 -3.31167668e-03]\n",
      "Loss value: 374.9784866719166\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 820\n",
      "Current point: [-2.46589857 -0.42932246  1.27405409]\n",
      "Gradient: [ 7.49664138e-05  8.64173885e-03 -3.28851182e-03]\n",
      "Loss value: 374.97848649910617\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 821\n",
      "Current point: [-2.46589872 -0.42933974  1.27406066]\n",
      "Gradient: [ 7.44493647e-05  8.58128074e-03 -3.26550898e-03]\n",
      "Loss value: 374.9784863287051\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 822\n",
      "Current point: [-2.46589887 -0.4293569   1.27406719]\n",
      "Gradient: [ 7.39358300e-05  8.52124566e-03 -3.24266705e-03]\n",
      "Loss value: 374.97848616067995\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 823\n",
      "Current point: [-2.46589902 -0.42937395  1.27407368]\n",
      "Gradient: [ 7.34257866e-05  8.46163065e-03 -3.21998490e-03]\n",
      "Loss value: 374.9784859949976\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 824\n",
      "Current point: [-2.46589916 -0.42939087  1.27408012]\n",
      "Gradient: [ 7.29192114e-05  8.40243278e-03 -3.19746140e-03]\n",
      "Loss value: 374.9784858316253\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 825\n",
      "Current point: [-2.46589931 -0.42940767  1.27408651]\n",
      "Gradient: [ 7.24160816e-05  8.34364912e-03 -3.17509546e-03]\n",
      "Loss value: 374.9784856705308\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 826\n",
      "Current point: [-2.46589945 -0.42942436  1.27409286]\n",
      "Gradient: [ 7.19163744e-05  8.28527678e-03 -3.15288596e-03]\n",
      "Loss value: 374.9784855116825\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 827\n",
      "Current point: [-2.4658996  -0.42944093  1.27409917]\n",
      "Gradient: [ 7.14200673e-05  8.22731287e-03 -3.13083181e-03]\n",
      "Loss value: 374.97848535504903\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 828\n",
      "Current point: [-2.46589974 -0.42945739  1.27410543]\n",
      "Gradient: [ 7.09271377e-05  8.16975454e-03 -3.10893193e-03]\n",
      "Loss value: 374.9784852005994\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 829\n",
      "Current point: [-2.46589988 -0.42947373  1.27411165]\n",
      "Gradient: [ 7.04375635e-05  8.11259896e-03 -3.08718523e-03]\n",
      "Loss value: 374.9784850483032\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 830\n",
      "Current point: [-2.46590002 -0.42948995  1.27411782]\n",
      "Gradient: [ 6.99513223e-05  8.05584329e-03 -3.06559065e-03]\n",
      "Loss value: 374.97848489813055\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 831\n",
      "Current point: [-2.46590016 -0.42950606  1.27412396]\n",
      "Gradient: [ 6.94683922e-05  7.99948474e-03 -3.04414712e-03]\n",
      "Loss value: 374.97848475005156\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 832\n",
      "Current point: [-2.4659003  -0.42952206  1.27413004]\n",
      "Gradient: [ 6.89887513e-05  7.94352053e-03 -3.02285359e-03]\n",
      "Loss value: 374.97848460403725\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 833\n",
      "Current point: [-2.46590044 -0.42953795  1.27413609]\n",
      "Gradient: [ 6.85123777e-05  7.88794791e-03 -3.00170900e-03]\n",
      "Loss value: 374.97848446005884\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 834\n",
      "Current point: [-2.46590058 -0.42955372  1.27414209]\n",
      "Gradient: [ 6.80392499e-05  7.83276412e-03 -2.98071232e-03]\n",
      "Loss value: 374.97848431808785\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 835\n",
      "Current point: [-2.46590071 -0.42956939  1.27414805]\n",
      "Gradient: [ 6.75693464e-05  7.77796646e-03 -2.95986250e-03]\n",
      "Loss value: 374.9784841780963\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 836\n",
      "Current point: [-2.46590085 -0.42958495  1.27415397]\n",
      "Gradient: [ 6.71026458e-05  7.72355221e-03 -2.93915853e-03]\n",
      "Loss value: 374.97848404005657\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 837\n",
      "Current point: [-2.46590098 -0.42960039  1.27415985]\n",
      "Gradient: [ 6.66391268e-05  7.66951869e-03 -2.91859938e-03]\n",
      "Loss value: 374.97848390394154\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 838\n",
      "Current point: [-2.46590112 -0.42961573  1.27416569]\n",
      "Gradient: [ 6.61787684e-05  7.61586325e-03 -2.89818404e-03]\n",
      "Loss value: 374.97848376972433\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 839\n",
      "Current point: [-2.46590125 -0.42963096  1.27417149]\n",
      "Gradient: [ 6.57215496e-05  7.56258322e-03 -2.87791150e-03]\n",
      "Loss value: 374.9784836373784\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 840\n",
      "Current point: [-2.46590138 -0.42964609  1.27417724]\n",
      "Gradient: [ 6.52674496e-05  7.50967600e-03 -2.85778076e-03]\n",
      "Loss value: 374.9784835068778\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 841\n",
      "Current point: [-2.46590151 -0.42966111  1.27418296]\n",
      "Gradient: [ 6.48164476e-05  7.45713895e-03 -2.83779083e-03]\n",
      "Loss value: 374.9784833781967\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 842\n",
      "Current point: [-2.46590164 -0.42967602  1.27418863]\n",
      "Gradient: [ 6.43685231e-05  7.40496951e-03 -2.81794074e-03]\n",
      "Loss value: 374.97848325130974\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 843\n",
      "Current point: [-2.46590177 -0.42969083  1.27419427]\n",
      "Gradient: [ 6.39236556e-05  7.35316508e-03 -2.79822949e-03]\n",
      "Loss value: 374.9784831261919\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 844\n",
      "Current point: [-2.4659019  -0.42970554  1.27419987]\n",
      "Gradient: [ 6.34818248e-05  7.30172312e-03 -2.77865612e-03]\n",
      "Loss value: 374.97848300281856\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 845\n",
      "Current point: [-2.46590202 -0.42972014  1.27420542]\n",
      "Gradient: [ 6.30430105e-05  7.25064110e-03 -2.75921966e-03]\n",
      "Loss value: 374.97848288116535\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 846\n",
      "Current point: [-2.46590215 -0.42973464  1.27421094]\n",
      "Gradient: [ 6.26071927e-05  7.19991648e-03 -2.73991916e-03]\n",
      "Loss value: 374.9784827612083\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 847\n",
      "Current point: [-2.46590227 -0.42974904  1.27421642]\n",
      "Gradient: [ 6.21743513e-05  7.14954678e-03 -2.72075366e-03]\n",
      "Loss value: 374.9784826429237\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 848\n",
      "Current point: [-2.4659024  -0.42976334  1.27422186]\n",
      "Gradient: [ 6.17444667e-05  7.09952950e-03 -2.70172223e-03]\n",
      "Loss value: 374.9784825262884\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 849\n",
      "Current point: [-2.46590252 -0.42977754  1.27422727]\n",
      "Gradient: [ 6.13175191e-05  7.04986218e-03 -2.68282391e-03]\n",
      "Loss value: 374.9784824112792\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 850\n",
      "Current point: [-2.46590264 -0.42979164  1.27423263]\n",
      "Gradient: [ 6.08934888e-05  7.00054237e-03 -2.66405779e-03]\n",
      "Loss value: 374.9784822978735\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 851\n",
      "Current point: [-2.46590277 -0.42980564  1.27423796]\n",
      "Gradient: [ 6.04723566e-05  6.95156764e-03 -2.64542294e-03]\n",
      "Loss value: 374.97848218604906\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 852\n",
      "Current point: [-2.46590289 -0.42981955  1.27424325]\n",
      "Gradient: [ 6.00541030e-05  6.90293558e-03 -2.62691843e-03]\n",
      "Loss value: 374.97848207578363\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 853\n",
      "Current point: [-2.46590301 -0.42983335  1.2742485 ]\n",
      "Gradient: [ 5.96387089e-05  6.85464378e-03 -2.60854336e-03]\n",
      "Loss value: 374.9784819670557\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 854\n",
      "Current point: [-2.46590313 -0.42984706  1.27425372]\n",
      "Gradient: [ 5.92261551e-05  6.80668986e-03 -2.59029682e-03]\n",
      "Loss value: 374.97848185984355\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 855\n",
      "Current point: [-2.46590325 -0.42986067  1.2742589 ]\n",
      "Gradient: [ 5.88164228e-05  6.75907146e-03 -2.57217791e-03]\n",
      "Loss value: 374.97848175412634\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 856\n",
      "Current point: [-2.46590336 -0.42987419  1.27426405]\n",
      "Gradient: [ 5.84094931e-05  6.71178624e-03 -2.55418575e-03]\n",
      "Loss value: 374.97848164988307\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 857\n",
      "Current point: [-2.46590348 -0.42988762  1.27426915]\n",
      "Gradient: [ 5.80053472e-05  6.66483185e-03 -2.53631943e-03]\n",
      "Loss value: 374.9784815470931\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 858\n",
      "Current point: [-2.4659036  -0.42990095  1.27427423]\n",
      "Gradient: [ 5.76039666e-05  6.61820599e-03 -2.51857809e-03]\n",
      "Loss value: 374.9784814457364\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 859\n",
      "Current point: [-2.46590371 -0.42991418  1.27427926]\n",
      "Gradient: [ 5.72053328e-05  6.57190635e-03 -2.50096085e-03]\n",
      "Loss value: 374.9784813457928\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 860\n",
      "Current point: [-2.46590383 -0.42992733  1.27428427]\n",
      "Gradient: [ 5.68094274e-05  6.52593066e-03 -2.48346684e-03]\n",
      "Loss value: 374.97848124724266\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 861\n",
      "Current point: [-2.46590394 -0.42994038  1.27428923]\n",
      "Gradient: [ 5.64162322e-05  6.48027664e-03 -2.46609520e-03]\n",
      "Loss value: 374.9784811500665\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 862\n",
      "Current point: [-2.46590405 -0.42995334  1.27429417]\n",
      "Gradient: [ 5.60257291e-05  6.43494205e-03 -2.44884507e-03]\n",
      "Loss value: 374.9784810542453\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 863\n",
      "Current point: [-2.46590416 -0.42996621  1.27429906]\n",
      "Gradient: [ 5.56378999e-05  6.38992464e-03 -2.43171560e-03]\n",
      "Loss value: 374.97848095976\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 864\n",
      "Current point: [-2.46590427 -0.42997899  1.27430393]\n",
      "Gradient: [ 5.52527269e-05  6.34522220e-03 -2.41470595e-03]\n",
      "Loss value: 374.97848086659207\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 865\n",
      "Current point: [-2.46590439 -0.42999168  1.27430876]\n",
      "Gradient: [ 5.48701922e-05  6.30083253e-03 -2.39781528e-03]\n",
      "Loss value: 374.97848077472315\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 866\n",
      "Current point: [-2.4659045  -0.43000428  1.27431355]\n",
      "Gradient: [ 5.44902781e-05  6.25675343e-03 -2.38104276e-03]\n",
      "Loss value: 374.9784806841351\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 867\n",
      "Current point: [-2.4659046  -0.43001679  1.27431831]\n",
      "Gradient: [ 5.41129672e-05  6.21298273e-03 -2.36438756e-03]\n",
      "Loss value: 374.97848059481004\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 868\n",
      "Current point: [-2.46590471 -0.43002922  1.27432304]\n",
      "Gradient: [ 5.37382419e-05  6.16951828e-03 -2.34784887e-03]\n",
      "Loss value: 374.9784805067304\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 869\n",
      "Current point: [-2.46590482 -0.43004156  1.27432774]\n",
      "Gradient: [ 5.33660849e-05  6.12635793e-03 -2.33142586e-03]\n",
      "Loss value: 374.9784804198788\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 870\n",
      "Current point: [-2.46590493 -0.43005381  1.2743324 ]\n",
      "Gradient: [ 5.29964790e-05  6.08349955e-03 -2.31511772e-03]\n",
      "Loss value: 374.9784803342381\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 871\n",
      "Current point: [-2.46590503 -0.43006598  1.27433703]\n",
      "Gradient: [ 5.26294070e-05  6.04094103e-03 -2.29892366e-03]\n",
      "Loss value: 374.9784802497914\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 872\n",
      "Current point: [-2.46590514 -0.43007806  1.27434163]\n",
      "Gradient: [ 5.22648520e-05  5.99868028e-03 -2.28284288e-03]\n",
      "Loss value: 374.97848016652205\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 873\n",
      "Current point: [-2.46590524 -0.43009006  1.27434619]\n",
      "Gradient: [ 5.19027971e-05  5.95671520e-03 -2.26687458e-03]\n",
      "Loss value: 374.9784800844138\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 874\n",
      "Current point: [-2.46590535 -0.43010197  1.27435073]\n",
      "Gradient: [ 5.15432254e-05  5.91504372e-03 -2.25101798e-03]\n",
      "Loss value: 374.9784800034502\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 875\n",
      "Current point: [-2.46590545 -0.4301138   1.27435523]\n",
      "Gradient: [ 5.11861202e-05  5.87366380e-03 -2.23527229e-03]\n",
      "Loss value: 374.9784799236155\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 876\n",
      "Current point: [-2.46590555 -0.43012555  1.2743597 ]\n",
      "Gradient: [ 5.08314651e-05  5.83257340e-03 -2.21963674e-03]\n",
      "Loss value: 374.97847984489385\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 877\n",
      "Current point: [-2.46590565 -0.43013721  1.27436414]\n",
      "Gradient: [ 5.04792436e-05  5.79177048e-03 -2.20411056e-03]\n",
      "Loss value: 374.97847976726973\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 878\n",
      "Current point: [-2.46590575 -0.4301488   1.27436855]\n",
      "Gradient: [ 5.01294392e-05  5.75125304e-03 -2.18869298e-03]\n",
      "Loss value: 374.97847969072785\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 879\n",
      "Current point: [-2.46590585 -0.4301603   1.27437293]\n",
      "Gradient: [ 4.97820357e-05  5.71101907e-03 -2.17338325e-03]\n",
      "Loss value: 374.97847961525315\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 880\n",
      "Current point: [-2.46590595 -0.43017172  1.27437727]\n",
      "Gradient: [ 4.94370170e-05  5.67106660e-03 -2.15818061e-03]\n",
      "Loss value: 374.9784795408308\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 881\n",
      "Current point: [-2.46590605 -0.43018306  1.27438159]\n",
      "Gradient: [ 4.90943670e-05  5.63139365e-03 -2.14308430e-03]\n",
      "Loss value: 374.978479467446\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 882\n",
      "Current point: [-2.46590615 -0.43019433  1.27438588]\n",
      "Gradient: [ 4.87540698e-05  5.59199827e-03 -2.12809360e-03]\n",
      "Loss value: 374.9784793950844\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 883\n",
      "Current point: [-2.46590625 -0.43020551  1.27439013]\n",
      "Gradient: [ 4.84161094e-05  5.55287851e-03 -2.11320775e-03]\n",
      "Loss value: 374.9784793237316\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 884\n",
      "Current point: [-2.46590635 -0.43021662  1.27439436]\n",
      "Gradient: [ 4.80804703e-05  5.51403246e-03 -2.09842603e-03]\n",
      "Loss value: 374.97847925337373\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 885\n",
      "Current point: [-2.46590644 -0.43022764  1.27439855]\n",
      "Gradient: [ 4.77471367e-05  5.47545818e-03 -2.08374771e-03]\n",
      "Loss value: 374.9784791839967\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 886\n",
      "Current point: [-2.46590654 -0.4302386   1.27440272]\n",
      "Gradient: [ 4.74160931e-05  5.43715378e-03 -2.06917206e-03]\n",
      "Loss value: 374.978479115587\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 887\n",
      "Current point: [-2.46590663 -0.43024947  1.27440686]\n",
      "Gradient: [ 4.70873241e-05  5.39911738e-03 -2.05469836e-03]\n",
      "Loss value: 374.97847904813096\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 888\n",
      "Current point: [-2.46590673 -0.43026027  1.27441097]\n",
      "Gradient: [ 4.67608143e-05  5.36134709e-03 -2.04032591e-03]\n",
      "Loss value: 374.9784789816155\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 889\n",
      "Current point: [-2.46590682 -0.43027099  1.27441505]\n",
      "Gradient: [ 4.64365485e-05  5.32384105e-03 -2.02605399e-03]\n",
      "Loss value: 374.9784789160274\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 890\n",
      "Current point: [-2.46590691 -0.43028164  1.2744191 ]\n",
      "Gradient: [ 4.61145115e-05  5.28659742e-03 -2.01188190e-03]\n",
      "Loss value: 374.97847885135377\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 891\n",
      "Current point: [-2.465907   -0.43029221  1.27442313]\n",
      "Gradient: [ 4.57946884e-05  5.24961435e-03 -1.99780894e-03]\n",
      "Loss value: 374.9784787875818\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 892\n",
      "Current point: [-2.4659071  -0.43030271  1.27442712]\n",
      "Gradient: [ 4.54770641e-05  5.21289003e-03 -1.98383442e-03]\n",
      "Loss value: 374.9784787246989\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 893\n",
      "Current point: [-2.46590719 -0.43031314  1.27443109]\n",
      "Gradient: [ 4.51616238e-05  5.17642265e-03 -1.96995766e-03]\n",
      "Loss value: 374.9784786626928\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 894\n",
      "Current point: [-2.46590728 -0.43032349  1.27443503]\n",
      "Gradient: [ 4.48483528e-05  5.14021040e-03 -1.95617795e-03]\n",
      "Loss value: 374.97847860155116\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 895\n",
      "Current point: [-2.46590737 -0.43033377  1.27443894]\n",
      "Gradient: [ 4.45372364e-05  5.10425150e-03 -1.94249464e-03]\n",
      "Loss value: 374.978478541262\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 896\n",
      "Current point: [-2.46590746 -0.43034398  1.27444283]\n",
      "Gradient: [ 4.42282601e-05  5.06854418e-03 -1.92890704e-03]\n",
      "Loss value: 374.97847848181334\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 897\n",
      "Current point: [-2.46590754 -0.43035411  1.27444668]\n",
      "Gradient: [ 4.39214094e-05  5.03308668e-03 -1.91541448e-03]\n",
      "Loss value: 374.9784784231935\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 898\n",
      "Current point: [-2.46590763 -0.43036418  1.27445052]\n",
      "Gradient: [ 4.36166698e-05  4.99787724e-03 -1.90201631e-03]\n",
      "Loss value: 374.97847836539097\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 899\n",
      "Current point: [-2.46590772 -0.43037418  1.27445432]\n",
      "Gradient: [ 4.33140272e-05  4.96291414e-03 -1.88871185e-03]\n",
      "Loss value: 374.97847830839436\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 900\n",
      "Current point: [-2.46590781 -0.4303841   1.2744581 ]\n",
      "Gradient: [ 4.30134674e-05  4.92819565e-03 -1.87550045e-03]\n",
      "Loss value: 374.9784782521924\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 901\n",
      "Current point: [-2.46590789 -0.43039396  1.27446185]\n",
      "Gradient: [ 4.27149762e-05  4.89372006e-03 -1.86238147e-03]\n",
      "Loss value: 374.978478196774\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 902\n",
      "Current point: [-2.46590798 -0.43040375  1.27446557]\n",
      "Gradient: [ 4.24185397e-05  4.85948567e-03 -1.84935425e-03]\n",
      "Loss value: 374.9784781421282\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 903\n",
      "Current point: [-2.46590806 -0.43041347  1.27446927]\n",
      "Gradient: [ 4.21241440e-05  4.82549079e-03 -1.83641816e-03]\n",
      "Loss value: 374.97847808824434\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 904\n",
      "Current point: [-2.46590815 -0.43042312  1.27447294]\n",
      "Gradient: [ 4.18317751e-05  4.79173374e-03 -1.82357255e-03]\n",
      "Loss value: 374.9784780351117\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 905\n",
      "Current point: [-2.46590823 -0.4304327   1.27447659]\n",
      "Gradient: [ 4.15414195e-05  4.75821286e-03 -1.81081680e-03]\n",
      "Loss value: 374.9784779827198\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 906\n",
      "Current point: [-2.46590831 -0.43044222  1.27448021]\n",
      "Gradient: [ 4.12530635e-05  4.72492650e-03 -1.79815028e-03]\n",
      "Loss value: 374.9784779310585\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 907\n",
      "Current point: [-2.4659084  -0.43045167  1.27448381]\n",
      "Gradient: [ 4.09666935e-05  4.69187302e-03 -1.78557235e-03]\n",
      "Loss value: 374.9784778801173\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 908\n",
      "Current point: [-2.46590848 -0.43046105  1.27448738]\n",
      "Gradient: [ 4.06822960e-05  4.65905078e-03 -1.77308241e-03]\n",
      "Loss value: 374.9784778298864\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 909\n",
      "Current point: [-2.46590856 -0.43047037  1.27449093]\n",
      "Gradient: [ 4.03998578e-05  4.62645818e-03 -1.76067983e-03]\n",
      "Loss value: 374.9784777803558\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 910\n",
      "Current point: [-2.46590864 -0.43047962  1.27449445]\n",
      "Gradient: [ 4.01193655e-05  4.59409360e-03 -1.74836400e-03]\n",
      "Loss value: 374.9784777315157\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 911\n",
      "Current point: [-2.46590872 -0.43048881  1.27449794]\n",
      "Gradient: [ 3.98408059e-05  4.56195544e-03 -1.73613433e-03]\n",
      "Loss value: 374.9784776833566\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 912\n",
      "Current point: [-2.4659088  -0.43049793  1.27450142]\n",
      "Gradient: [ 3.95641658e-05  4.53004213e-03 -1.72399020e-03]\n",
      "Loss value: 374.9784776358689\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 913\n",
      "Current point: [-2.46590888 -0.43050699  1.27450487]\n",
      "Gradient: [ 3.92894324e-05  4.49835208e-03 -1.71193101e-03]\n",
      "Loss value: 374.9784775890432\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 914\n",
      "Current point: [-2.46590896 -0.43051599  1.27450829]\n",
      "Gradient: [ 3.90165926e-05  4.46688374e-03 -1.69995618e-03]\n",
      "Loss value: 374.97847754287045\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 915\n",
      "Current point: [-2.46590904 -0.43052492  1.27451169]\n",
      "Gradient: [ 3.87456336e-05  4.43563556e-03 -1.68806512e-03]\n",
      "Loss value: 374.97847749734143\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 916\n",
      "Current point: [-2.46590911 -0.43053379  1.27451507]\n",
      "Gradient: [ 3.84765426e-05  4.40460600e-03 -1.67625723e-03]\n",
      "Loss value: 374.9784774524471\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 917\n",
      "Current point: [-2.46590919 -0.4305426   1.27451842]\n",
      "Gradient: [ 3.82093069e-05  4.37379352e-03 -1.66453193e-03]\n",
      "Loss value: 374.9784774081788\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 918\n",
      "Current point: [-2.46590927 -0.43055135  1.27452175]\n",
      "Gradient: [ 3.79439139e-05  4.34319660e-03 -1.65288865e-03]\n",
      "Loss value: 374.97847736452763\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 919\n",
      "Current point: [-2.46590934 -0.43056004  1.27452505]\n",
      "Gradient: [ 3.76803512e-05  4.31281375e-03 -1.64132682e-03]\n",
      "Loss value: 374.978477321485\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 920\n",
      "Current point: [-2.46590942 -0.43056866  1.27452834]\n",
      "Gradient: [ 3.74186062e-05  4.28264345e-03 -1.62984586e-03]\n",
      "Loss value: 374.9784772790425\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 921\n",
      "Current point: [-2.46590949 -0.43057723  1.27453159]\n",
      "Gradient: [ 3.71586667e-05  4.25268423e-03 -1.61844520e-03]\n",
      "Loss value: 374.9784772371918\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 922\n",
      "Current point: [-2.46590957 -0.43058573  1.27453483]\n",
      "Gradient: [ 3.69005203e-05  4.22293460e-03 -1.60712430e-03]\n",
      "Loss value: 374.9784771959245\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 923\n",
      "Current point: [-2.46590964 -0.43059418  1.27453805]\n",
      "Gradient: [ 3.66441548e-05  4.19339310e-03 -1.59588258e-03]\n",
      "Loss value: 374.9784771552325\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 924\n",
      "Current point: [-2.46590971 -0.43060257  1.27454124]\n",
      "Gradient: [ 3.63895582e-05  4.16405828e-03 -1.58471950e-03]\n",
      "Loss value: 374.97847711510786\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 925\n",
      "Current point: [-2.46590979 -0.4306109   1.27454441]\n",
      "Gradient: [ 3.61367184e-05  4.13492868e-03 -1.57363450e-03]\n",
      "Loss value: 374.9784770755427\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 926\n",
      "Current point: [-2.46590986 -0.43061917  1.27454755]\n",
      "Gradient: [ 3.58856234e-05  4.10600287e-03 -1.56262704e-03]\n",
      "Loss value: 374.97847703652906\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 927\n",
      "Current point: [-2.46590993 -0.43062738  1.27455068]\n",
      "Gradient: [ 3.56362614e-05  4.07727943e-03 -1.55169658e-03]\n",
      "Loss value: 374.9784769980594\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 928\n",
      "Current point: [-2.46591    -0.43063553  1.27455378]\n",
      "Gradient: [ 3.53886206e-05  4.04875694e-03 -1.54084257e-03]\n",
      "Loss value: 374.97847696012605\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 929\n",
      "Current point: [-2.46591007 -0.43064363  1.27455686]\n",
      "Gradient: [ 3.51426893e-05  4.02043399e-03 -1.53006449e-03]\n",
      "Loss value: 374.9784769227216\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 930\n",
      "Current point: [-2.46591014 -0.43065167  1.27455992]\n",
      "Gradient: [ 3.48984558e-05  3.99230919e-03 -1.51936180e-03]\n",
      "Loss value: 374.97847688583863\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 931\n",
      "Current point: [-2.46591021 -0.43065965  1.27456296]\n",
      "Gradient: [ 3.46559085e-05  3.96438115e-03 -1.50873397e-03]\n",
      "Loss value: 374.97847684946987\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 932\n",
      "Current point: [-2.46591028 -0.43066758  1.27456598]\n",
      "Gradient: [ 3.44150361e-05  3.93664849e-03 -1.49818049e-03]\n",
      "Loss value: 374.97847681360815\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 933\n",
      "Current point: [-2.46591035 -0.43067546  1.27456898]\n",
      "Gradient: [ 3.41758270e-05  3.90910985e-03 -1.48770082e-03]\n",
      "Loss value: 374.97847677824643\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 934\n",
      "Current point: [-2.46591042 -0.43068327  1.27457195]\n",
      "Gradient: [ 3.39382699e-05  3.88176387e-03 -1.47729446e-03]\n",
      "Loss value: 374.97847674337766\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 935\n",
      "Current point: [-2.46591049 -0.43069104  1.27457491]\n",
      "Gradient: [ 3.37023536e-05  3.85460920e-03 -1.46696089e-03]\n",
      "Loss value: 374.97847670899506\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 936\n",
      "Current point: [-2.46591056 -0.43069875  1.27457784]\n",
      "Gradient: [ 3.34680668e-05  3.82764450e-03 -1.45669961e-03]\n",
      "Loss value: 374.9784766750918\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 937\n",
      "Current point: [-2.46591062 -0.4307064   1.27458075]\n",
      "Gradient: [ 3.32353986e-05  3.80086845e-03 -1.44651010e-03]\n",
      "Loss value: 374.97847664166125\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 938\n",
      "Current point: [-2.46591069 -0.430714    1.27458365]\n",
      "Gradient: [ 3.30043378e-05  3.77427972e-03 -1.43639186e-03]\n",
      "Loss value: 374.9784766086967\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 939\n",
      "Current point: [-2.46591075 -0.43072155  1.27458652]\n",
      "Gradient: [ 3.27748735e-05  3.74787700e-03 -1.42634440e-03]\n",
      "Loss value: 374.97847657619184\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 940\n",
      "Current point: [-2.46591082 -0.43072905  1.27458937]\n",
      "Gradient: [ 3.25469947e-05  3.72165899e-03 -1.41636722e-03]\n",
      "Loss value: 374.9784765441401\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 941\n",
      "Current point: [-2.46591089 -0.43073649  1.27459221]\n",
      "Gradient: [ 3.23206908e-05  3.69562440e-03 -1.40645984e-03]\n",
      "Loss value: 374.9784765125352\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 942\n",
      "Current point: [-2.46591095 -0.43074388  1.27459502]\n",
      "Gradient: [ 3.20959508e-05  3.66977195e-03 -1.39662175e-03]\n",
      "Loss value: 374.978476481371\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 943\n",
      "Current point: [-2.46591101 -0.43075122  1.27459781]\n",
      "Gradient: [ 3.18727641e-05  3.64410036e-03 -1.38685248e-03]\n",
      "Loss value: 374.97847645064127\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 944\n",
      "Current point: [-2.46591108 -0.43075851  1.27460059]\n",
      "Gradient: [ 3.16511202e-05  3.61860836e-03 -1.37715154e-03]\n",
      "Loss value: 374.9784764203399\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 945\n",
      "Current point: [-2.46591114 -0.43076575  1.27460334]\n",
      "Gradient: [ 3.14310085e-05  3.59329470e-03 -1.36751846e-03]\n",
      "Loss value: 374.97847639046097\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 946\n",
      "Current point: [-2.4659112  -0.43077294  1.27460608]\n",
      "Gradient: [ 3.12124186e-05  3.56815814e-03 -1.35795277e-03]\n",
      "Loss value: 374.97847636099874\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 947\n",
      "Current point: [-2.46591127 -0.43078007  1.27460879]\n",
      "Gradient: [ 3.09953399e-05  3.54319742e-03 -1.34845398e-03]\n",
      "Loss value: 374.97847633194715\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 948\n",
      "Current point: [-2.46591133 -0.43078716  1.27461149]\n",
      "Gradient: [ 3.07797623e-05  3.51841133e-03 -1.33902164e-03]\n",
      "Loss value: 374.9784763033007\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 949\n",
      "Current point: [-2.46591139 -0.43079419  1.27461417]\n",
      "Gradient: [ 3.05656755e-05  3.49379864e-03 -1.32965528e-03]\n",
      "Loss value: 374.9784762750535\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 950\n",
      "Current point: [-2.46591145 -0.43080118  1.27461683]\n",
      "Gradient: [ 3.03530692e-05  3.46935813e-03 -1.32035443e-03]\n",
      "Loss value: 374.97847624720015\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 951\n",
      "Current point: [-2.46591151 -0.43080812  1.27461947]\n",
      "Gradient: [ 3.01419334e-05  3.44508861e-03 -1.31111865e-03]\n",
      "Loss value: 374.9784762197351\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 952\n",
      "Current point: [-2.46591157 -0.43081501  1.27462209]\n",
      "Gradient: [ 2.99322580e-05  3.42098887e-03 -1.30194746e-03]\n",
      "Loss value: 374.97847619265303\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 953\n",
      "Current point: [-2.46591163 -0.43082185  1.27462469]\n",
      "Gradient: [ 2.97240329e-05  3.39705773e-03 -1.29284043e-03]\n",
      "Loss value: 374.97847616594856\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 954\n",
      "Current point: [-2.46591169 -0.43082865  1.27462728]\n",
      "Gradient: [ 2.95172484e-05  3.37329401e-03 -1.28379710e-03]\n",
      "Loss value: 374.9784761396163\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 955\n",
      "Current point: [-2.46591175 -0.43083539  1.27462985]\n",
      "Gradient: [ 2.93118945e-05  3.34969653e-03 -1.27481703e-03]\n",
      "Loss value: 374.9784761136512\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 956\n",
      "Current point: [-2.46591181 -0.43084209  1.2746324 ]\n",
      "Gradient: [ 2.91079615e-05  3.32626414e-03 -1.26589977e-03]\n",
      "Loss value: 374.9784760880481\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 957\n",
      "Current point: [-2.46591187 -0.43084875  1.27463493]\n",
      "Gradient: [ 2.89054397e-05  3.30299568e-03 -1.25704489e-03]\n",
      "Loss value: 374.978476062802\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 958\n",
      "Current point: [-2.46591193 -0.43085535  1.27463744]\n",
      "Gradient: [ 2.87043193e-05  3.27988999e-03 -1.24825195e-03]\n",
      "Loss value: 374.9784760379078\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 959\n",
      "Current point: [-2.46591198 -0.43086191  1.27463994]\n",
      "Gradient: [ 2.85045907e-05  3.25694595e-03 -1.23952051e-03]\n",
      "Loss value: 374.9784760133607\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 960\n",
      "Current point: [-2.46591204 -0.43086843  1.27464242]\n",
      "Gradient: [ 2.83062446e-05  3.23416243e-03 -1.23085015e-03]\n",
      "Loss value: 374.9784759891558\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 961\n",
      "Current point: [-2.4659121  -0.43087489  1.27464488]\n",
      "Gradient: [ 2.81092713e-05  3.21153829e-03 -1.22224044e-03]\n",
      "Loss value: 374.97847596528834\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 962\n",
      "Current point: [-2.46591215 -0.43088132  1.27464732]\n",
      "Gradient: [ 2.79136615e-05  3.18907242e-03 -1.21369095e-03]\n",
      "Loss value: 374.9784759417537\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 963\n",
      "Current point: [-2.46591221 -0.43088769  1.27464975]\n",
      "Gradient: [ 2.77194059e-05  3.16676372e-03 -1.20520126e-03]\n",
      "Loss value: 374.97847591854713\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 964\n",
      "Current point: [-2.46591226 -0.43089403  1.27465216]\n",
      "Gradient: [ 2.75264952e-05  3.14461109e-03 -1.19677096e-03]\n",
      "Loss value: 374.97847589566413\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 965\n",
      "Current point: [-2.46591232 -0.43090032  1.27465455]\n",
      "Gradient: [ 2.73349201e-05  3.12261343e-03 -1.18839963e-03]\n",
      "Loss value: 374.9784758731001\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 966\n",
      "Current point: [-2.46591237 -0.43090656  1.27465693]\n",
      "Gradient: [ 2.71446715e-05  3.10076966e-03 -1.18008686e-03]\n",
      "Loss value: 374.9784758508507\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 967\n",
      "Current point: [-2.46591243 -0.43091276  1.27465929]\n",
      "Gradient: [ 2.69557404e-05  3.07907870e-03 -1.17183223e-03]\n",
      "Loss value: 374.9784758289115\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 968\n",
      "Current point: [-2.46591248 -0.43091892  1.27466163]\n",
      "Gradient: [ 2.67681177e-05  3.05753949e-03 -1.16363534e-03]\n",
      "Loss value: 374.9784758072781\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 969\n",
      "Current point: [-2.46591254 -0.43092504  1.27466396]\n",
      "Gradient: [ 2.65817943e-05  3.03615096e-03 -1.15549579e-03]\n",
      "Loss value: 374.97847578594633\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 970\n",
      "Current point: [-2.46591259 -0.43093111  1.27466627]\n",
      "Gradient: [ 2.63967616e-05  3.01491206e-03 -1.14741318e-03]\n",
      "Loss value: 374.978475764912\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 971\n",
      "Current point: [-2.46591264 -0.43093714  1.27466857]\n",
      "Gradient: [ 2.62130104e-05  2.99382175e-03 -1.13938710e-03]\n",
      "Loss value: 374.9784757441709\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 972\n",
      "Current point: [-2.46591269 -0.43094313  1.27467085]\n",
      "Gradient: [ 2.60305322e-05  2.97287897e-03 -1.13141716e-03]\n",
      "Loss value: 374.978475723719\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 973\n",
      "Current point: [-2.46591275 -0.43094907  1.27467311]\n",
      "Gradient: [ 2.58493181e-05  2.95208270e-03 -1.12350298e-03]\n",
      "Loss value: 374.9784757035522\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 974\n",
      "Current point: [-2.4659128  -0.43095498  1.27467536]\n",
      "Gradient: [ 2.56693595e-05  2.93143192e-03 -1.11564415e-03]\n",
      "Loss value: 374.97847568366655\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 975\n",
      "Current point: [-2.46591285 -0.43096084  1.27467759]\n",
      "Gradient: [ 2.54906478e-05  2.91092561e-03 -1.10784029e-03]\n",
      "Loss value: 374.9784756640581\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 976\n",
      "Current point: [-2.4659129  -0.43096666  1.2746798 ]\n",
      "Gradient: [ 2.53131743e-05  2.89056275e-03 -1.10009102e-03]\n",
      "Loss value: 374.97847564472306\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 977\n",
      "Current point: [-2.46591295 -0.43097244  1.274682  ]\n",
      "Gradient: [ 2.51369307e-05  2.87034234e-03 -1.09239596e-03]\n",
      "Loss value: 374.9784756256576\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 978\n",
      "Current point: [-2.465913   -0.43097818  1.27468419]\n",
      "Gradient: [ 2.49619084e-05  2.85026339e-03 -1.08475472e-03]\n",
      "Loss value: 374.97847560685796\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 979\n",
      "Current point: [-2.46591305 -0.43098388  1.27468636]\n",
      "Gradient: [ 2.47880992e-05  2.83032490e-03 -1.07716694e-03]\n",
      "Loss value: 374.97847558832035\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 980\n",
      "Current point: [-2.4659131  -0.43098955  1.27468851]\n",
      "Gradient: [ 2.46154946e-05  2.81052590e-03 -1.06963223e-03]\n",
      "Loss value: 374.97847557004127\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 981\n",
      "Current point: [-2.46591315 -0.43099517  1.27469065]\n",
      "Gradient: [ 2.44440864e-05  2.79086540e-03 -1.06215022e-03]\n",
      "Loss value: 374.978475552017\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 982\n",
      "Current point: [-2.4659132  -0.43100075  1.27469278]\n",
      "Gradient: [ 2.42738664e-05  2.77134245e-03 -1.05472055e-03]\n",
      "Loss value: 374.97847553424396\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 983\n",
      "Current point: [-2.46591325 -0.43100629  1.27469489]\n",
      "Gradient: [ 2.41048263e-05  2.75195607e-03 -1.04734285e-03]\n",
      "Loss value: 374.97847551671873\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 984\n",
      "Current point: [-2.46591329 -0.43101179  1.27469698]\n",
      "Gradient: [ 2.39369582e-05  2.73270531e-03 -1.04001676e-03]\n",
      "Loss value: 374.9784754994379\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 985\n",
      "Current point: [-2.46591334 -0.43101726  1.27469906]\n",
      "Gradient: [ 2.37702540e-05  2.71358922e-03 -1.03274191e-03]\n",
      "Loss value: 374.97847548239787\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 986\n",
      "Current point: [-2.46591339 -0.43102269  1.27470113]\n",
      "Gradient: [ 2.36047056e-05  2.69460686e-03 -1.02551795e-03]\n",
      "Loss value: 374.97847546559547\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 987\n",
      "Current point: [-2.46591344 -0.43102808  1.27470318]\n",
      "Gradient: [ 2.34403051e-05  2.67575729e-03 -1.01834452e-03]\n",
      "Loss value: 374.97847544902737\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 988\n",
      "Current point: [-2.46591348 -0.43103343  1.27470521]\n",
      "Gradient: [ 2.32770447e-05  2.65703959e-03 -1.01122126e-03]\n",
      "Loss value: 374.9784754326902\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 989\n",
      "Current point: [-2.46591353 -0.43103874  1.27470724]\n",
      "Gradient: [ 2.31149164e-05  2.63845284e-03 -1.00414784e-03]\n",
      "Loss value: 374.9784754165808\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 990\n",
      "Current point: [-2.46591358 -0.43104402  1.27470924]\n",
      "Gradient: [ 2.29539126e-05  2.61999610e-03 -9.97123890e-04]\n",
      "Loss value: 374.978475400696\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 991\n",
      "Current point: [-2.46591362 -0.43104926  1.27471124]\n",
      "Gradient: [ 2.27940254e-05  2.60166849e-03 -9.90149073e-04]\n",
      "Loss value: 374.97847538503265\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 992\n",
      "Current point: [-2.46591367 -0.43105446  1.27471322]\n",
      "Gradient: [ 2.26352473e-05  2.58346908e-03 -9.83223046e-04]\n",
      "Loss value: 374.9784753695876\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 993\n",
      "Current point: [-2.46591371 -0.43105963  1.27471518]\n",
      "Gradient: [ 2.24775705e-05  2.56539700e-03 -9.76345465e-04]\n",
      "Loss value: 374.978475354358\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 994\n",
      "Current point: [-2.46591376 -0.43106476  1.27471714]\n",
      "Gradient: [ 2.23209875e-05  2.54745134e-03 -9.69515992e-04]\n",
      "Loss value: 374.97847533934066\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 995\n",
      "Current point: [-2.4659138  -0.43106985  1.27471908]\n",
      "Gradient: [ 2.21654908e-05  2.52963122e-03 -9.62734291e-04]\n",
      "Loss value: 374.9784753245327\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 996\n",
      "Current point: [-2.46591385 -0.43107491  1.274721  ]\n",
      "Gradient: [ 2.20110729e-05  2.51193576e-03 -9.56000028e-04]\n",
      "Loss value: 374.97847530993124\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 997\n",
      "Current point: [-2.46591389 -0.43107994  1.27472291]\n",
      "Gradient: [ 2.18577263e-05  2.49436409e-03 -9.49312870e-04]\n",
      "Loss value: 374.97847529553326\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 998\n",
      "Current point: [-2.46591394 -0.43108493  1.27472481]\n",
      "Gradient: [ 2.17054438e-05  2.47691534e-03 -9.42672488e-04]\n",
      "Loss value: 374.97847528133605\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 999\n",
      "Current point: [-2.46591398 -0.43108988  1.2747267 ]\n",
      "Gradient: [ 2.15542180e-05  2.45958867e-03 -9.36078556e-04]\n",
      "Loss value: 374.97847526733676\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1000\n",
      "Current point: [-2.46591402 -0.4310948   1.27472857]\n",
      "Gradient: [ 2.14040416e-05  2.44238320e-03 -9.29530747e-04]\n",
      "Loss value: 374.9784752535326\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1001\n",
      "Current point: [-2.46591406 -0.43109968  1.27473043]\n",
      "Gradient: [ 2.12549074e-05  2.42529809e-03 -9.23028740e-04]\n",
      "Loss value: 374.97847523992095\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1002\n",
      "Current point: [-2.46591411 -0.43110453  1.27473228]\n",
      "Gradient: [ 2.11068082e-05  2.40833250e-03 -9.16572213e-04]\n",
      "Loss value: 374.9784752264991\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1003\n",
      "Current point: [-2.46591415 -0.43110935  1.27473411]\n",
      "Gradient: [ 2.09597369e-05  2.39148560e-03 -9.10160850e-04]\n",
      "Loss value: 374.9784752132643\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1004\n",
      "Current point: [-2.46591419 -0.43111413  1.27473593]\n",
      "Gradient: [ 2.08136864e-05  2.37475655e-03 -9.03794334e-04]\n",
      "Loss value: 374.978475200214\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1005\n",
      "Current point: [-2.46591423 -0.43111888  1.27473774]\n",
      "Gradient: [ 2.06686497e-05  2.35814453e-03 -8.97472351e-04]\n",
      "Loss value: 374.9784751873457\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1006\n",
      "Current point: [-2.46591427 -0.4311236   1.27473953]\n",
      "Gradient: [ 2.05246198e-05  2.34164872e-03 -8.91194590e-04]\n",
      "Loss value: 374.9784751746568\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1007\n",
      "Current point: [-2.46591432 -0.43112828  1.27474131]\n",
      "Gradient: [ 2.03815897e-05  2.32526830e-03 -8.84960741e-04]\n",
      "Loss value: 374.9784751621448\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1008\n",
      "Current point: [-2.46591436 -0.43113293  1.27474308]\n",
      "Gradient: [ 2.02395526e-05  2.30900248e-03 -8.78770498e-04]\n",
      "Loss value: 374.9784751498072\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1009\n",
      "Current point: [-2.4659144  -0.43113755  1.27474484]\n",
      "Gradient: [ 2.00985017e-05  2.29285045e-03 -8.72623555e-04]\n",
      "Loss value: 374.9784751376416\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1010\n",
      "Current point: [-2.46591444 -0.43114214  1.27474659]\n",
      "Gradient: [ 1.99584301e-05  2.27681140e-03 -8.66519609e-04]\n",
      "Loss value: 374.9784751256456\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1011\n",
      "Current point: [-2.46591448 -0.43114669  1.27474832]\n",
      "Gradient: [ 1.98193311e-05  2.26088456e-03 -8.60458360e-04]\n",
      "Loss value: 374.9784751138169\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1012\n",
      "Current point: [-2.46591452 -0.43115121  1.27475004]\n",
      "Gradient: [ 1.96811981e-05  2.24506914e-03 -8.54439509e-04]\n",
      "Loss value: 374.9784751021531\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1013\n",
      "Current point: [-2.46591456 -0.4311557   1.27475175]\n",
      "Gradient: [ 1.95440242e-05  2.22936435e-03 -8.48462759e-04]\n",
      "Loss value: 374.9784750906519\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1014\n",
      "Current point: [-2.4659146  -0.43116016  1.27475345]\n",
      "Gradient: [ 1.94078029e-05  2.21376943e-03 -8.42527816e-04]\n",
      "Loss value: 374.978475079311\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1015\n",
      "Current point: [-2.46591463 -0.43116459  1.27475513]\n",
      "Gradient: [ 1.92725278e-05  2.19828360e-03 -8.36634388e-04]\n",
      "Loss value: 374.9784750681282\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1016\n",
      "Current point: [-2.46591467 -0.43116899  1.2747568 ]\n",
      "Gradient: [ 1.91381921e-05  2.18290610e-03 -8.30782184e-04]\n",
      "Loss value: 374.9784750571014\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1017\n",
      "Current point: [-2.46591471 -0.43117335  1.27475847]\n",
      "Gradient: [ 1.90047895e-05  2.16763618e-03 -8.24970915e-04]\n",
      "Loss value: 374.97847504622825\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1018\n",
      "Current point: [-2.46591475 -0.43117769  1.27476012]\n",
      "Gradient: [ 1.88723135e-05  2.15247307e-03 -8.19200296e-04]\n",
      "Loss value: 374.9784750355067\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1019\n",
      "Current point: [-2.46591479 -0.43118199  1.27476175]\n",
      "Gradient: [ 1.87407577e-05  2.13741604e-03 -8.13470042e-04]\n",
      "Loss value: 374.97847502493465\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1020\n",
      "Current point: [-2.46591482 -0.43118627  1.27476338]\n",
      "Gradient: [ 1.86101159e-05  2.12246434e-03 -8.07779871e-04]\n",
      "Loss value: 374.97847501450997\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1021\n",
      "Current point: [-2.46591486 -0.43119051  1.274765  ]\n",
      "Gradient: [ 1.84803816e-05  2.10761724e-03 -8.02129502e-04]\n",
      "Loss value: 374.97847500423063\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1022\n",
      "Current point: [-2.4659149  -0.43119473  1.2747666 ]\n",
      "Gradient: [ 1.83515486e-05  2.09287400e-03 -7.96518657e-04]\n",
      "Loss value: 374.97847499409465\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1023\n",
      "Current point: [-2.46591493 -0.43119891  1.27476819]\n",
      "Gradient: [ 1.82236107e-05  2.07823389e-03 -7.90947060e-04]\n",
      "Loss value: 374.9784749840999\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1024\n",
      "Current point: [-2.46591497 -0.43120307  1.27476978]\n",
      "Gradient: [ 1.80965617e-05  2.06369620e-03 -7.85414435e-04]\n",
      "Loss value: 374.9784749742445\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1025\n",
      "Current point: [-2.46591501 -0.4312072   1.27477135]\n",
      "Gradient: [ 1.79703956e-05  2.04926021e-03 -7.79920511e-04]\n",
      "Loss value: 374.9784749645265\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1026\n",
      "Current point: [-2.46591504 -0.4312113   1.27477291]\n",
      "Gradient: [ 1.78451061e-05  2.03492520e-03 -7.74465016e-04]\n",
      "Loss value: 374.97847495494403\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1027\n",
      "Current point: [-2.46591508 -0.43121537  1.27477446]\n",
      "Gradient: [ 1.77206872e-05  2.02069048e-03 -7.69047682e-04]\n",
      "Loss value: 374.9784749454951\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1028\n",
      "Current point: [-2.46591511 -0.43121941  1.27477599]\n",
      "Gradient: [ 1.75971330e-05  2.00655533e-03 -7.63668242e-04]\n",
      "Loss value: 374.9784749361779\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1029\n",
      "Current point: [-2.46591515 -0.43122342  1.27477752]\n",
      "Gradient: [ 1.74744374e-05  1.99251907e-03 -7.58326431e-04]\n",
      "Loss value: 374.97847492699066\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1030\n",
      "Current point: [-2.46591518 -0.4312274   1.27477904]\n",
      "Gradient: [ 1.73525946e-05  1.97858099e-03 -7.53021985e-04]\n",
      "Loss value: 374.9784749179315\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1031\n",
      "Current point: [-2.46591522 -0.43123136  1.27478054]\n",
      "Gradient: [ 1.72315986e-05  1.96474042e-03 -7.47754644e-04]\n",
      "Loss value: 374.9784749089986\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1032\n",
      "Current point: [-2.46591525 -0.43123529  1.27478204]\n",
      "Gradient: [ 1.71114437e-05  1.95099667e-03 -7.42524147e-04]\n",
      "Loss value: 374.97847490019024\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1033\n",
      "Current point: [-2.46591529 -0.43123919  1.27478352]\n",
      "Gradient: [ 1.69921239e-05  1.93734906e-03 -7.37330237e-04]\n",
      "Loss value: 374.9784748915047\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1034\n",
      "Current point: [-2.46591532 -0.43124307  1.274785  ]\n",
      "Gradient: [ 1.68736336e-05  1.92379693e-03 -7.32172658e-04]\n",
      "Loss value: 374.9784748829402\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1035\n",
      "Current point: [-2.46591536 -0.43124692  1.27478646]\n",
      "Gradient: [ 1.67559669e-05  1.91033959e-03 -7.27051156e-04]\n",
      "Loss value: 374.9784748744951\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1036\n",
      "Current point: [-2.46591539 -0.43125074  1.27478792]\n",
      "Gradient: [ 1.66391183e-05  1.89697640e-03 -7.21965479e-04]\n",
      "Loss value: 374.97847486616774\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1037\n",
      "Current point: [-2.46591542 -0.43125453  1.27478936]\n",
      "Gradient: [ 1.65230820e-05  1.88370669e-03 -7.16915375e-04]\n",
      "Loss value: 374.9784748579566\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1038\n",
      "Current point: [-2.46591546 -0.4312583   1.2747908 ]\n",
      "Gradient: [ 1.64078524e-05  1.87052981e-03 -7.11900597e-04]\n",
      "Loss value: 374.9784748498598\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1039\n",
      "Current point: [-2.46591549 -0.43126204  1.27479222]\n",
      "Gradient: [ 1.62934240e-05  1.85744510e-03 -7.06920897e-04]\n",
      "Loss value: 374.9784748418759\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1040\n",
      "Current point: [-2.46591552 -0.43126575  1.27479363]\n",
      "Gradient: [ 1.61797913e-05  1.84445193e-03 -7.01976029e-04]\n",
      "Loss value: 374.97847483400324\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1041\n",
      "Current point: [-2.46591555 -0.43126944  1.27479504]\n",
      "Gradient: [ 1.60669486e-05  1.83154965e-03 -6.97065750e-04]\n",
      "Loss value: 374.9784748262405\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1042\n",
      "Current point: [-2.46591559 -0.43127311  1.27479643]\n",
      "Gradient: [ 1.59548906e-05  1.81873762e-03 -6.92189819e-04]\n",
      "Loss value: 374.9784748185859\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1043\n",
      "Current point: [-2.46591562 -0.43127674  1.27479782]\n",
      "Gradient: [ 1.58436119e-05  1.80601523e-03 -6.87347994e-04]\n",
      "Loss value: 374.978474811038\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1044\n",
      "Current point: [-2.46591565 -0.43128036  1.27479919]\n",
      "Gradient: [ 1.57331070e-05  1.79338183e-03 -6.82540037e-04]\n",
      "Loss value: 374.9784748035953\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1045\n",
      "Current point: [-2.46591568 -0.43128394  1.27480056]\n",
      "Gradient: [ 1.56233707e-05  1.78083680e-03 -6.77765712e-04]\n",
      "Loss value: 374.9784747962565\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1046\n",
      "Current point: [-2.46591571 -0.4312875   1.27480191]\n",
      "Gradient: [ 1.55143976e-05  1.76837954e-03 -6.73024783e-04]\n",
      "Loss value: 374.97847478901986\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1047\n",
      "Current point: [-2.46591574 -0.43129104  1.27480326]\n",
      "Gradient: [ 1.54061823e-05  1.75600942e-03 -6.68317016e-04]\n",
      "Loss value: 374.97847478188413\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1048\n",
      "Current point: [-2.46591577 -0.43129455  1.27480459]\n",
      "Gradient: [ 1.52987198e-05  1.74372583e-03 -6.63642180e-04]\n",
      "Loss value: 374.978474774848\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1049\n",
      "Current point: [-2.4659158  -0.43129804  1.27480592]\n",
      "Gradient: [ 1.51920047e-05  1.73152817e-03 -6.59000044e-04]\n",
      "Loss value: 374.97847476790986\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1050\n",
      "Current point: [-2.46591583 -0.4313015   1.27480724]\n",
      "Gradient: [ 1.50860319e-05  1.71941583e-03 -6.54390379e-04]\n",
      "Loss value: 374.9784747610684\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1051\n",
      "Current point: [-2.46591586 -0.43130494  1.27480855]\n",
      "Gradient: [ 1.49807963e-05  1.70738823e-03 -6.49812959e-04]\n",
      "Loss value: 374.9784747543224\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1052\n",
      "Current point: [-2.46591589 -0.43130836  1.27480985]\n",
      "Gradient: [ 1.48762927e-05  1.69544477e-03 -6.45267557e-04]\n",
      "Loss value: 374.9784747476705\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1053\n",
      "Current point: [-2.46591592 -0.43131175  1.27481114]\n",
      "Gradient: [ 1.47725162e-05  1.68358485e-03 -6.40753951e-04]\n",
      "Loss value: 374.9784747411112\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1054\n",
      "Current point: [-2.46591595 -0.43131511  1.27481242]\n",
      "Gradient: [ 1.46694617e-05  1.67180790e-03 -6.36271916e-04]\n",
      "Loss value: 374.97847473464344\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1055\n",
      "Current point: [-2.46591598 -0.43131846  1.27481369]\n",
      "Gradient: [ 1.45671241e-05  1.66011334e-03 -6.31821233e-04]\n",
      "Loss value: 374.97847472826584\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1056\n",
      "Current point: [-2.46591601 -0.43132178  1.27481496]\n",
      "Gradient: [ 1.44654985e-05  1.64850058e-03 -6.27401682e-04]\n",
      "Loss value: 374.97847472197714\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1057\n",
      "Current point: [-2.46591604 -0.43132508  1.27481621]\n",
      "Gradient: [ 1.43645800e-05  1.63696906e-03 -6.23013046e-04]\n",
      "Loss value: 374.9784747157761\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1058\n",
      "Current point: [-2.46591607 -0.43132835  1.27481746]\n",
      "Gradient: [ 1.42643637e-05  1.62551820e-03 -6.18655108e-04]\n",
      "Loss value: 374.97847470966155\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1059\n",
      "Current point: [-2.4659161  -0.4313316   1.27481869]\n",
      "Gradient: [ 1.41648448e-05  1.61414745e-03 -6.14327653e-04]\n",
      "Loss value: 374.9784747036322\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1060\n",
      "Current point: [-2.46591613 -0.43133483  1.27481992]\n",
      "Gradient: [ 1.40660184e-05  1.60285624e-03 -6.10030469e-04]\n",
      "Loss value: 374.9784746976869\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1061\n",
      "Current point: [-2.46591616 -0.43133803  1.27482114]\n",
      "Gradient: [ 1.39678796e-05  1.59164401e-03 -6.05763343e-04]\n",
      "Loss value: 374.97847469182454\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1062\n",
      "Current point: [-2.46591618 -0.43134122  1.27482235]\n",
      "Gradient: [ 1.38704239e-05  1.58051022e-03 -6.01526066e-04]\n",
      "Loss value: 374.97847468604385\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1063\n",
      "Current point: [-2.46591621 -0.43134438  1.27482356]\n",
      "Gradient: [ 1.37736463e-05  1.56945432e-03 -5.97318428e-04]\n",
      "Loss value: 374.9784746803438\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1064\n",
      "Current point: [-2.46591624 -0.43134752  1.27482475]\n",
      "Gradient: [ 1.36775424e-05  1.55847575e-03 -5.93140222e-04]\n",
      "Loss value: 374.9784746747232\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1065\n",
      "Current point: [-2.46591627 -0.43135063  1.27482594]\n",
      "Gradient: [ 1.35821072e-05  1.54757398e-03 -5.88991242e-04]\n",
      "Loss value: 374.97847466918097\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1066\n",
      "Current point: [-2.46591629 -0.43135373  1.27482712]\n",
      "Gradient: [ 1.34873364e-05  1.53674848e-03 -5.84871284e-04]\n",
      "Loss value: 374.978474663716\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1067\n",
      "Current point: [-2.46591632 -0.4313568   1.27482829]\n",
      "Gradient: [ 1.33932251e-05  1.52599870e-03 -5.80780145e-04]\n",
      "Loss value: 374.9784746583272\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1068\n",
      "Current point: [-2.46591635 -0.43135986  1.27482945]\n",
      "Gradient: [ 1.32997689e-05  1.51532412e-03 -5.76717623e-04]\n",
      "Loss value: 374.97847465301356\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1069\n",
      "Current point: [-2.46591637 -0.43136289  1.2748306 ]\n",
      "Gradient: [ 1.32069633e-05  1.50472422e-03 -5.72683518e-04]\n",
      "Loss value: 374.97847464777396\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1070\n",
      "Current point: [-2.4659164  -0.4313659   1.27483175]\n",
      "Gradient: [ 1.31148037e-05  1.49419846e-03 -5.68677632e-04]\n",
      "Loss value: 374.9784746426074\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1071\n",
      "Current point: [-2.46591643 -0.43136888  1.27483288]\n",
      "Gradient: [ 1.30232856e-05  1.48374634e-03 -5.64699766e-04]\n",
      "Loss value: 374.97847463751293\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1072\n",
      "Current point: [-2.46591645 -0.43137185  1.27483401]\n",
      "Gradient: [ 1.29324047e-05  1.47336733e-03 -5.60749726e-04]\n",
      "Loss value: 374.9784746324894\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1073\n",
      "Current point: [-2.46591648 -0.4313748   1.27483513]\n",
      "Gradient: [ 1.28421564e-05  1.46306092e-03 -5.56827315e-04]\n",
      "Loss value: 374.97847462753595\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1074\n",
      "Current point: [-2.4659165  -0.43137772  1.27483625]\n",
      "Gradient: [ 1.27525364e-05  1.45282661e-03 -5.52932342e-04]\n",
      "Loss value: 374.97847462265156\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1075\n",
      "Current point: [-2.46591653 -0.43138063  1.27483735]\n",
      "Gradient: [ 1.26635404e-05  1.44266390e-03 -5.49064614e-04]\n",
      "Loss value: 374.9784746178353\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1076\n",
      "Current point: [-2.46591655 -0.43138352  1.27483845]\n",
      "Gradient: [ 1.25751641e-05  1.43257227e-03 -5.45223940e-04]\n",
      "Loss value: 374.9784746130861\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1077\n",
      "Current point: [-2.46591658 -0.43138638  1.27483954]\n",
      "Gradient: [ 1.24874030e-05  1.42255124e-03 -5.41410131e-04]\n",
      "Loss value: 374.9784746084032\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1078\n",
      "Current point: [-2.4659166  -0.43138923  1.27484062]\n",
      "Gradient: [ 1.24002531e-05  1.41260031e-03 -5.37623000e-04]\n",
      "Loss value: 374.9784746037855\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1079\n",
      "Current point: [-2.46591663 -0.43139205  1.2748417 ]\n",
      "Gradient: [ 1.23137099e-05  1.40271899e-03 -5.33862360e-04]\n",
      "Loss value: 374.9784745992322\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1080\n",
      "Current point: [-2.46591665 -0.43139486  1.27484277]\n",
      "Gradient: [ 1.22277694e-05  1.39290680e-03 -5.30128025e-04]\n",
      "Loss value: 374.97847459474247\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1081\n",
      "Current point: [-2.46591668 -0.43139764  1.27484383]\n",
      "Gradient: [ 1.21424274e-05  1.38316324e-03 -5.26419811e-04]\n",
      "Loss value: 374.9784745903152\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1082\n",
      "Current point: [-2.4659167  -0.43140041  1.27484488]\n",
      "Gradient: [ 1.20576797e-05  1.37348784e-03 -5.22737536e-04]\n",
      "Loss value: 374.9784745859497\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1083\n",
      "Current point: [-2.46591673 -0.43140316  1.27484593]\n",
      "Gradient: [ 1.19735221e-05  1.36388012e-03 -5.19081018e-04]\n",
      "Loss value: 374.97847458164506\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1084\n",
      "Current point: [-2.46591675 -0.43140588  1.27484696]\n",
      "Gradient: [ 1.18899507e-05  1.35433962e-03 -5.15450078e-04]\n",
      "Loss value: 374.9784745774005\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1085\n",
      "Current point: [-2.46591677 -0.43140859  1.274848  ]\n",
      "Gradient: [ 1.18069613e-05  1.34486585e-03 -5.11844536e-04]\n",
      "Loss value: 374.97847457321507\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1086\n",
      "Current point: [-2.4659168  -0.43141128  1.27484902]\n",
      "Gradient: [ 1.17245499e-05  1.33545835e-03 -5.08264214e-04]\n",
      "Loss value: 374.97847456908795\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1087\n",
      "Current point: [-2.46591682 -0.43141395  1.27485004]\n",
      "Gradient: [ 1.16427124e-05  1.32611666e-03 -5.04708936e-04]\n",
      "Loss value: 374.97847456501836\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1088\n",
      "Current point: [-2.46591684 -0.4314166   1.27485105]\n",
      "Gradient: [ 1.15614450e-05  1.31684032e-03 -5.01178527e-04]\n",
      "Loss value: 374.97847456100556\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1089\n",
      "Current point: [-2.46591687 -0.43141924  1.27485205]\n",
      "Gradient: [ 1.14807437e-05  1.30762887e-03 -4.97672813e-04]\n",
      "Loss value: 374.9784745570487\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1090\n",
      "Current point: [-2.46591689 -0.43142185  1.27485304]\n",
      "Gradient: [ 1.14006044e-05  1.29848186e-03 -4.94191622e-04]\n",
      "Loss value: 374.97847455314695\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1091\n",
      "Current point: [-2.46591691 -0.43142445  1.27485403]\n",
      "Gradient: [ 1.13210234e-05  1.28939883e-03 -4.90734781e-04]\n",
      "Loss value: 374.9784745492997\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1092\n",
      "Current point: [-2.46591694 -0.43142703  1.27485501]\n",
      "Gradient: [ 1.12419967e-05  1.28037934e-03 -4.87302121e-04]\n",
      "Loss value: 374.97847454550595\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1093\n",
      "Current point: [-2.46591696 -0.43142959  1.27485599]\n",
      "Gradient: [ 1.11635206e-05  1.27142294e-03 -4.83893471e-04]\n",
      "Loss value: 374.9784745417652\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1094\n",
      "Current point: [-2.46591698 -0.43143213  1.27485695]\n",
      "Gradient: [ 1.10855911e-05  1.26252920e-03 -4.80508665e-04]\n",
      "Loss value: 374.97847453807657\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1095\n",
      "Current point: [-2.465917   -0.43143466  1.27485792]\n",
      "Gradient: [ 1.10082046e-05  1.25369767e-03 -4.77147536e-04]\n",
      "Loss value: 374.97847453443933\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1096\n",
      "Current point: [-2.46591703 -0.43143717  1.27485887]\n",
      "Gradient: [ 1.09313571e-05  1.24492792e-03 -4.73809917e-04]\n",
      "Loss value: 374.97847453085285\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1097\n",
      "Current point: [-2.46591705 -0.43143966  1.27485982]\n",
      "Gradient: [ 1.08550451e-05  1.23621952e-03 -4.70495645e-04]\n",
      "Loss value: 374.9784745273163\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1098\n",
      "Current point: [-2.46591707 -0.43144213  1.27486076]\n",
      "Gradient: [ 1.07792647e-05  1.22757203e-03 -4.67204556e-04]\n",
      "Loss value: 374.97847452382905\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1099\n",
      "Current point: [-2.46591709 -0.43144458  1.27486169]\n",
      "Gradient: [ 1.07040123e-05  1.21898504e-03 -4.63936488e-04]\n",
      "Loss value: 374.9784745203905\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1100\n",
      "Current point: [-2.46591711 -0.43144702  1.27486262]\n",
      "Gradient: [ 1.06292843e-05  1.21045811e-03 -4.60691280e-04]\n",
      "Loss value: 374.9784745169999\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1101\n",
      "Current point: [-2.46591713 -0.43144944  1.27486354]\n",
      "Gradient: [ 1.05550769e-05  1.20199083e-03 -4.57468772e-04]\n",
      "Loss value: 374.9784745136565\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1102\n",
      "Current point: [-2.46591715 -0.43145185  1.27486446]\n",
      "Gradient: [ 1.04813866e-05  1.19358279e-03 -4.54268805e-04]\n",
      "Loss value: 374.9784745103597\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1103\n",
      "Current point: [-2.46591718 -0.43145423  1.27486537]\n",
      "Gradient: [ 1.04082098e-05  1.18523355e-03 -4.51091221e-04]\n",
      "Loss value: 374.9784745071089\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1104\n",
      "Current point: [-2.4659172  -0.4314566   1.27486627]\n",
      "Gradient: [ 1.03355429e-05  1.17694273e-03 -4.47935865e-04]\n",
      "Loss value: 374.9784745039034\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1105\n",
      "Current point: [-2.46591722 -0.43145896  1.27486716]\n",
      "Gradient: [ 1.02633824e-05  1.16870990e-03 -4.44802580e-04]\n",
      "Loss value: 374.97847450074266\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1106\n",
      "Current point: [-2.46591724 -0.43146129  1.27486805]\n",
      "Gradient: [ 1.01917247e-05  1.16053466e-03 -4.41691212e-04]\n",
      "Loss value: 374.9784744976259\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1107\n",
      "Current point: [-2.46591726 -0.43146362  1.27486894]\n",
      "Gradient: [ 1.01205664e-05  1.15241661e-03 -4.38601609e-04]\n",
      "Loss value: 374.97847449455253\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1108\n",
      "Current point: [-2.46591728 -0.43146592  1.27486981]\n",
      "Gradient: [ 1.00499041e-05  1.14435535e-03 -4.35533616e-04]\n",
      "Loss value: 374.9784744915222\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1109\n",
      "Current point: [-2.4659173  -0.43146821  1.27487069]\n",
      "Gradient: [ 9.97973416e-06  1.13635047e-03 -4.32487084e-04]\n",
      "Loss value: 374.97847448853395\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1110\n",
      "Current point: [-2.46591732 -0.43147048  1.27487155]\n",
      "Gradient: [ 9.91005328e-06  1.12840160e-03 -4.29461863e-04]\n",
      "Loss value: 374.9784744855874\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1111\n",
      "Current point: [-2.46591734 -0.43147274  1.27487241]\n",
      "Gradient: [ 9.84085804e-06  1.12050833e-03 -4.26457802e-04]\n",
      "Loss value: 374.9784744826819\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1112\n",
      "Current point: [-2.46591736 -0.43147498  1.27487326]\n",
      "Gradient: [ 9.77214507e-06  1.11267027e-03 -4.23474755e-04]\n",
      "Loss value: 374.978474479817\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1113\n",
      "Current point: [-2.46591738 -0.4314772   1.27487411]\n",
      "Gradient: [ 9.70391104e-06  1.10488704e-03 -4.20512574e-04]\n",
      "Loss value: 374.978474476992\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1114\n",
      "Current point: [-2.4659174  -0.43147941  1.27487495]\n",
      "Gradient: [ 9.63615261e-06  1.09715826e-03 -4.17571113e-04]\n",
      "Loss value: 374.97847447420634\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1115\n",
      "Current point: [-2.46591742 -0.43148161  1.27487579]\n",
      "Gradient: [ 9.56886648e-06  1.08948354e-03 -4.14650228e-04]\n",
      "Loss value: 374.9784744714596\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1116\n",
      "Current point: [-2.46591743 -0.43148379  1.27487661]\n",
      "Gradient: [ 9.50204936e-06  1.08186251e-03 -4.11749774e-04]\n",
      "Loss value: 374.97847446875113\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1117\n",
      "Current point: [-2.46591745 -0.43148595  1.27487744]\n",
      "Gradient: [ 9.43569798e-06  1.07429479e-03 -4.08869609e-04]\n",
      "Loss value: 374.97847446608034\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1118\n",
      "Current point: [-2.46591747 -0.4314881   1.27487826]\n",
      "Gradient: [ 9.36980913e-06  1.06678000e-03 -4.06009590e-04]\n",
      "Loss value: 374.9784744634469\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1119\n",
      "Current point: [-2.46591749 -0.43149023  1.27487907]\n",
      "Gradient: [ 9.30437959e-06  1.05931779e-03 -4.03169577e-04]\n",
      "Loss value: 374.97847446085007\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1120\n",
      "Current point: [-2.46591751 -0.43149235  1.27487987]\n",
      "Gradient: [ 9.23940617e-06  1.05190777e-03 -4.00349429e-04]\n",
      "Loss value: 374.9784744582895\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1121\n",
      "Current point: [-2.46591753 -0.43149446  1.27488067]\n",
      "Gradient: [ 9.17488570e-06  1.04454959e-03 -3.97549008e-04]\n",
      "Loss value: 374.9784744557646\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1122\n",
      "Current point: [-2.46591755 -0.43149655  1.27488147]\n",
      "Gradient: [ 9.11081501e-06  1.03724288e-03 -3.94768176e-04]\n",
      "Loss value: 374.9784744532749\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1123\n",
      "Current point: [-2.46591757 -0.43149862  1.27488226]\n",
      "Gradient: [ 9.04719101e-06  1.02998729e-03 -3.92006796e-04]\n",
      "Loss value: 374.97847445081993\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1124\n",
      "Current point: [-2.46591758 -0.43150068  1.27488304]\n",
      "Gradient: [ 8.98401059e-06  1.02278245e-03 -3.89264731e-04]\n",
      "Loss value: 374.9784744483992\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1125\n",
      "Current point: [-2.4659176  -0.43150273  1.27488382]\n",
      "Gradient: [ 8.92127065e-06  1.01562800e-03 -3.86541847e-04]\n",
      "Loss value: 374.97847444601217\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1126\n",
      "Current point: [-2.46591762 -0.43150476  1.2748846 ]\n",
      "Gradient: [ 8.85896817e-06  1.00852361e-03 -3.83838010e-04]\n",
      "Loss value: 374.97847444365846\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1127\n",
      "Current point: [-2.46591764 -0.43150677  1.27488536]\n",
      "Gradient: [ 8.79710004e-06  1.00146891e-03 -3.81153085e-04]\n",
      "Loss value: 374.97847444133754\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1128\n",
      "Current point: [-2.46591765 -0.43150878  1.27488613]\n",
      "Gradient: [ 8.73566332e-06  9.94463558e-04 -3.78486942e-04]\n",
      "Loss value: 374.978474439049\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1129\n",
      "Current point: [-2.46591767 -0.43151077  1.27488688]\n",
      "Gradient: [ 8.67465496e-06  9.87507211e-04 -3.75839448e-04]\n",
      "Loss value: 374.97847443679234\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1130\n",
      "Current point: [-2.46591769 -0.43151274  1.27488763]\n",
      "Gradient: [ 8.61407198e-06  9.80599526e-04 -3.73210473e-04]\n",
      "Loss value: 374.97847443456715\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1131\n",
      "Current point: [-2.46591771 -0.4315147   1.27488838]\n",
      "Gradient: [ 8.55391146e-06  9.73740161e-04 -3.70599887e-04]\n",
      "Loss value: 374.978474432373\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1132\n",
      "Current point: [-2.46591772 -0.43151665  1.27488912]\n",
      "Gradient: [ 8.49417045e-06  9.66928779e-04 -3.68007563e-04]\n",
      "Loss value: 374.97847443020936\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1133\n",
      "Current point: [-2.46591774 -0.43151858  1.27488986]\n",
      "Gradient: [ 8.43484600e-06  9.60165044e-04 -3.65433371e-04]\n",
      "Loss value: 374.97847442807597\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1134\n",
      "Current point: [-2.46591776 -0.4315205   1.27489059]\n",
      "Gradient: [ 8.37593527e-06  9.53448623e-04 -3.62877186e-04]\n",
      "Loss value: 374.9784744259723\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1135\n",
      "Current point: [-2.46591777 -0.43152241  1.27489131]\n",
      "Gradient: [ 8.31743531e-06  9.46779184e-04 -3.60338881e-04]\n",
      "Loss value: 374.9784744238979\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1136\n",
      "Current point: [-2.46591779 -0.4315243   1.27489203]\n",
      "Gradient: [ 8.25934333e-06  9.40156399e-04 -3.57818332e-04]\n",
      "Loss value: 374.9784744218525\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1137\n",
      "Current point: [-2.46591781 -0.43152618  1.27489275]\n",
      "Gradient: [ 8.20165646e-06  9.33579942e-04 -3.55315413e-04]\n",
      "Loss value: 374.9784744198356\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1138\n",
      "Current point: [-2.46591782 -0.43152805  1.27489346]\n",
      "Gradient: [ 8.14437193e-06  9.27049488e-04 -3.52830003e-04]\n",
      "Loss value: 374.9784744178468\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1139\n",
      "Current point: [-2.46591784 -0.43152991  1.27489417]\n",
      "Gradient: [ 8.08748690e-06  9.20564716e-04 -3.50361977e-04]\n",
      "Loss value: 374.9784744158857\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1140\n",
      "Current point: [-2.46591786 -0.43153175  1.27489487]\n",
      "Gradient: [ 8.03099860e-06  9.14125307e-04 -3.47911216e-04]\n",
      "Loss value: 374.978474413952\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1141\n",
      "Current point: [-2.46591787 -0.43153357  1.27489556]\n",
      "Gradient: [ 7.97490427e-06  9.07730942e-04 -3.45477597e-04]\n",
      "Loss value: 374.9784744120452\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1142\n",
      "Current point: [-2.46591789 -0.43153539  1.27489625]\n",
      "Gradient: [ 7.91920116e-06  9.01381307e-04 -3.43061001e-04]\n",
      "Loss value: 374.97847441016506\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1143\n",
      "Current point: [-2.4659179  -0.43153719  1.27489694]\n",
      "Gradient: [ 7.86388659e-06  8.95076088e-04 -3.40661309e-04]\n",
      "Loss value: 374.97847440831106\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1144\n",
      "Current point: [-2.46591792 -0.43153898  1.27489762]\n",
      "Gradient: [ 7.80895779e-06  8.88814976e-04 -3.38278403e-04]\n",
      "Loss value: 374.978474406483\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1145\n",
      "Current point: [-2.46591794 -0.43154076  1.2748983 ]\n",
      "Gradient: [ 7.75441215e-06  8.82597662e-04 -3.35912166e-04]\n",
      "Loss value: 374.9784744046803\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1146\n",
      "Current point: [-2.46591795 -0.43154253  1.27489897]\n",
      "Gradient: [ 7.70024694e-06  8.76423838e-04 -3.33562479e-04]\n",
      "Loss value: 374.9784744029028\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1147\n",
      "Current point: [-2.46591797 -0.43154428  1.27489964]\n",
      "Gradient: [ 7.64645959e-06  8.70293202e-04 -3.31229229e-04]\n",
      "Loss value: 374.97847440115004\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1148\n",
      "Current point: [-2.46591798 -0.43154602  1.2749003 ]\n",
      "Gradient: [ 7.5930474e-06  8.6420545e-04 -3.2891230e-04]\n",
      "Loss value: 374.97847439942177\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1149\n",
      "Current point: [-2.465918   -0.43154775  1.27490096]\n",
      "Gradient: [ 7.54000781e-06  8.58160284e-04 -3.26611577e-04]\n",
      "Loss value: 374.97847439771755\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1150\n",
      "Current point: [-2.46591801 -0.43154946  1.27490161]\n",
      "Gradient: [ 7.48733820e-06  8.52157404e-04 -3.24326948e-04]\n",
      "Loss value: 374.9784743960371\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1151\n",
      "Current point: [-2.46591803 -0.43155117  1.27490226]\n",
      "Gradient: [ 7.43503596e-06  8.46196515e-04 -3.22058300e-04]\n",
      "Loss value: 374.9784743943801\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1152\n",
      "Current point: [-2.46591804 -0.43155286  1.2749029 ]\n",
      "Gradient: [ 7.38309861e-06  8.40277324e-04 -3.19805521e-04]\n",
      "Loss value: 374.9784743927462\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1153\n",
      "Current point: [-2.46591806 -0.43155454  1.27490354]\n",
      "Gradient: [ 7.33152359e-06  8.34399538e-04 -3.17568500e-04]\n",
      "Loss value: 374.978474391135\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1154\n",
      "Current point: [-2.46591807 -0.43155621  1.27490418]\n",
      "Gradient: [ 7.28030835e-06  8.28562869e-04 -3.15347126e-04]\n",
      "Loss value: 374.97847438954636\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1155\n",
      "Current point: [-2.46591809 -0.43155787  1.27490481]\n",
      "Gradient: [ 7.22945041e-06  8.22767028e-04 -3.13141291e-04]\n",
      "Loss value: 374.9784743879799\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1156\n",
      "Current point: [-2.4659181  -0.43155951  1.27490544]\n",
      "Gradient: [ 7.17894728e-06  8.17011730e-04 -3.10950886e-04]\n",
      "Loss value: 374.9784743864352\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1157\n",
      "Current point: [-2.46591811 -0.43156115  1.27490606]\n",
      "Gradient: [ 7.12879650e-06  8.11296691e-04 -3.08775802e-04]\n",
      "Loss value: 374.97847438491203\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1158\n",
      "Current point: [-2.46591813 -0.43156277  1.27490667]\n",
      "Gradient: [ 7.07899560e-06  8.05621629e-04 -3.06615934e-04]\n",
      "Loss value: 374.97847438341006\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1159\n",
      "Current point: [-2.46591814 -0.43156438  1.27490729]\n",
      "Gradient: [ 7.02954215e-06  7.99986266e-04 -3.04471173e-04]\n",
      "Loss value: 374.9784743819291\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1160\n",
      "Current point: [-2.46591816 -0.43156598  1.2749079 ]\n",
      "Gradient: [ 6.98043375e-06  7.94390323e-04 -3.02341414e-04]\n",
      "Loss value: 374.97847438046875\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1161\n",
      "Current point: [-2.46591817 -0.43156757  1.2749085 ]\n",
      "Gradient: [ 6.93166795e-06  7.88833524e-04 -3.00226554e-04]\n",
      "Loss value: 374.97847437902874\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1162\n",
      "Current point: [-2.46591818 -0.43156915  1.2749091 ]\n",
      "Gradient: [ 6.88324242e-06  7.83315596e-04 -2.98126486e-04]\n",
      "Loss value: 374.9784743776089\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1163\n",
      "Current point: [-2.4659182  -0.43157071  1.2749097 ]\n",
      "Gradient: [ 6.83515476e-06  7.77836266e-04 -2.96041108e-04]\n",
      "Loss value: 374.9784743762088\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1164\n",
      "Current point: [-2.46591821 -0.43157227  1.27491029]\n",
      "Gradient: [ 6.78740267e-06  7.72395266e-04 -2.93970318e-04]\n",
      "Loss value: 374.9784743748282\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1165\n",
      "Current point: [-2.46591823 -0.43157381  1.27491088]\n",
      "Gradient: [ 6.73998373e-06  7.66992326e-04 -2.91914012e-04]\n",
      "Loss value: 374.97847437346684\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1166\n",
      "Current point: [-2.46591824 -0.43157535  1.27491146]\n",
      "Gradient: [ 6.69289569e-06  7.61627180e-04 -2.89872091e-04]\n",
      "Loss value: 374.9784743721245\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1167\n",
      "Current point: [-2.46591825 -0.43157687  1.27491204]\n",
      "Gradient: [ 6.64613619e-06  7.56299564e-04 -2.87844452e-04]\n",
      "Loss value: 374.9784743708008\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1168\n",
      "Current point: [-2.46591827 -0.43157838  1.27491262]\n",
      "Gradient: [ 6.59970302e-06  7.51009216e-04 -2.85830997e-04]\n",
      "Loss value: 374.97847436949564\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1169\n",
      "Current point: [-2.46591828 -0.43157989  1.27491319]\n",
      "Gradient: [ 6.55359385e-06  7.45755874e-04 -2.83831625e-04]\n",
      "Loss value: 374.9784743682086\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1170\n",
      "Current point: [-2.46591829 -0.43158138  1.27491376]\n",
      "Gradient: [ 6.50780642e-06  7.40539280e-04 -2.81846239e-04]\n",
      "Loss value: 374.9784743669396\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1171\n",
      "Current point: [-2.46591831 -0.43158286  1.27491432]\n",
      "Gradient: [ 6.46233852e-06  7.35359178e-04 -2.79874741e-04]\n",
      "Loss value: 374.9784743656882\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1172\n",
      "Current point: [-2.46591832 -0.43158433  1.27491488]\n",
      "Gradient: [ 6.41718794e-06  7.30215310e-04 -2.77917033e-04]\n",
      "Loss value: 374.97847436445426\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1173\n",
      "Current point: [-2.46591833 -0.43158579  1.27491544]\n",
      "Gradient: [ 6.37235243e-06  7.25107425e-04 -2.75973019e-04]\n",
      "Loss value: 374.9784743632376\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1174\n",
      "Current point: [-2.46591834 -0.43158724  1.27491599]\n",
      "Gradient: [ 6.32782979e-06  7.20035270e-04 -2.74042604e-04]\n",
      "Loss value: 374.97847436203784\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1175\n",
      "Current point: [-2.46591836 -0.43158868  1.27491654]\n",
      "Gradient: [ 6.28361789e-06  7.14998595e-04 -2.72125692e-04]\n",
      "Loss value: 374.9784743608548\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1176\n",
      "Current point: [-2.46591837 -0.43159011  1.27491708]\n",
      "Gradient: [ 6.23971453e-06  7.09997153e-04 -2.70222188e-04]\n",
      "Loss value: 374.9784743596882\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1177\n",
      "Current point: [-2.46591838 -0.43159153  1.27491762]\n",
      "Gradient: [ 6.19611758e-06  7.05030696e-04 -2.68331999e-04]\n",
      "Loss value: 374.978474358538\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1178\n",
      "Current point: [-2.46591839 -0.43159294  1.27491816]\n",
      "Gradient: [ 6.15282488e-06  7.00098980e-04 -2.66455032e-04]\n",
      "Loss value: 374.97847435740374\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1179\n",
      "Current point: [-2.46591841 -0.43159434  1.27491869]\n",
      "Gradient: [ 6.10983433e-06  6.95201762e-04 -2.64591194e-04]\n",
      "Loss value: 374.97847435628535\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1180\n",
      "Current point: [-2.46591842 -0.43159573  1.27491922]\n",
      "Gradient: [ 6.06714383e-06  6.90338801e-04 -2.62740394e-04]\n",
      "Loss value: 374.97847435518247\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1181\n",
      "Current point: [-2.46591843 -0.43159711  1.27491974]\n",
      "Gradient: [ 6.02475128e-06  6.85509857e-04 -2.60902540e-04]\n",
      "Loss value: 374.97847435409506\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1182\n",
      "Current point: [-2.46591844 -0.43159848  1.27492027]\n",
      "Gradient: [ 5.98265460e-06  6.80714692e-04 -2.59077542e-04]\n",
      "Loss value: 374.9784743530227\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1183\n",
      "Current point: [-2.46591845 -0.43159984  1.27492078]\n",
      "Gradient: [ 5.94085177e-06  6.75953070e-04 -2.57265309e-04]\n",
      "Loss value: 374.97847435196536\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1184\n",
      "Current point: [-2.46591847 -0.4316012   1.2749213 ]\n",
      "Gradient: [ 5.89934072e-06  6.71224756e-04 -2.55465753e-04]\n",
      "Loss value: 374.9784743509228\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1185\n",
      "Current point: [-2.46591848 -0.43160254  1.27492181]\n",
      "Gradient: [ 5.85811939e-06  6.66529517e-04 -2.53678784e-04]\n",
      "Loss value: 374.97847434989467\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1186\n",
      "Current point: [-2.46591849 -0.43160387  1.27492232]\n",
      "Gradient: [ 5.81718581e-06  6.61867122e-04 -2.51904316e-04]\n",
      "Loss value: 374.978474348881\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1187\n",
      "Current point: [-2.4659185  -0.4316052   1.27492282]\n",
      "Gradient: [ 5.77653794e-06  6.57237340e-04 -2.50142259e-04]\n",
      "Loss value: 374.9784743478814\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1188\n",
      "Current point: [-2.46591851 -0.43160651  1.27492332]\n",
      "Gradient: [ 5.73617379e-06  6.52639945e-04 -2.48392529e-04]\n",
      "Loss value: 374.9784743468957\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1189\n",
      "Current point: [-2.46591852 -0.43160781  1.27492382]\n",
      "Gradient: [ 5.69609140e-06  6.48074709e-04 -2.46655037e-04]\n",
      "Loss value: 374.9784743459238\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1190\n",
      "Current point: [-2.46591854 -0.43160911  1.27492431]\n",
      "Gradient: [ 5.65628879e-06  6.43541407e-04 -2.44929699e-04]\n",
      "Loss value: 374.9784743449654\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1191\n",
      "Current point: [-2.46591855 -0.4316104   1.2749248 ]\n",
      "Gradient: [ 5.61676404e-06  6.39039816e-04 -2.43216430e-04]\n",
      "Loss value: 374.97847434402036\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1192\n",
      "Current point: [-2.46591856 -0.43161168  1.27492529]\n",
      "Gradient: [ 5.57751519e-06  6.34569715e-04 -2.41515145e-04]\n",
      "Loss value: 374.9784743430885\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1193\n",
      "Current point: [-2.46591857 -0.43161295  1.27492577]\n",
      "Gradient: [ 5.53854032e-06  6.30130882e-04 -2.39825760e-04]\n",
      "Loss value: 374.97847434216965\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1194\n",
      "Current point: [-2.46591858 -0.43161421  1.27492625]\n",
      "Gradient: [ 5.49983753e-06  6.25723099e-04 -2.38148192e-04]\n",
      "Loss value: 374.9784743412636\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1195\n",
      "Current point: [-2.46591859 -0.43161546  1.27492673]\n",
      "Gradient: [ 5.46140491e-06  6.21346149e-04 -2.36482359e-04]\n",
      "Loss value: 374.9784743403702\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1196\n",
      "Current point: [-2.4659186 -0.4316167  1.2749272]\n",
      "Gradient: [ 5.42324060e-06  6.16999817e-04 -2.34828179e-04]\n",
      "Loss value: 374.97847433948925\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1197\n",
      "Current point: [-2.46591861 -0.43161793  1.27492767]\n",
      "Gradient: [ 5.38534272e-06  6.12683887e-04 -2.33185569e-04]\n",
      "Loss value: 374.9784743386206\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1198\n",
      "Current point: [-2.46591862 -0.43161916  1.27492814]\n",
      "Gradient: [ 5.34770942e-06  6.08398148e-04 -2.31554449e-04]\n",
      "Loss value: 374.97847433776406\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1199\n",
      "Current point: [-2.46591863 -0.43162038  1.2749286 ]\n",
      "Gradient: [ 5.31033883e-06  6.04142388e-04 -2.29934739e-04]\n",
      "Loss value: 374.97847433691936\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1200\n",
      "Current point: [-2.46591865 -0.43162158  1.27492906]\n",
      "Gradient: [ 5.27322914e-06  5.99916398e-04 -2.28326358e-04]\n",
      "Loss value: 374.9784743360865\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1201\n",
      "Current point: [-2.46591866 -0.43162278  1.27492952]\n",
      "Gradient: [ 5.23637851e-06  5.95719969e-04 -2.26729228e-04]\n",
      "Loss value: 374.9784743352653\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1202\n",
      "Current point: [-2.46591867 -0.43162398  1.27492997]\n",
      "Gradient: [ 5.19978518e-06  5.91552894e-04 -2.25143270e-04]\n",
      "Loss value: 374.97847433445554\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1203\n",
      "Current point: [-2.46591868 -0.43162516  1.27493042]\n",
      "Gradient: [ 5.16344731e-06  5.87414968e-04 -2.23568406e-04]\n",
      "Loss value: 374.978474333657\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1204\n",
      "Current point: [-2.46591869 -0.43162633  1.27493087]\n",
      "Gradient: [ 5.12736319e-06  5.83305988e-04 -2.22004557e-04]\n",
      "Loss value: 374.9784743328696\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1205\n",
      "Current point: [-2.4659187  -0.4316275   1.27493131]\n",
      "Gradient: [ 5.09153097e-06  5.79225750e-04 -2.20451648e-04]\n",
      "Loss value: 374.97847433209324\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1206\n",
      "Current point: [-2.46591871 -0.43162866  1.27493175]\n",
      "Gradient: [ 5.05594892e-06  5.75174054e-04 -2.18909601e-04]\n",
      "Loss value: 374.9784743313277\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1207\n",
      "Current point: [-2.46591872 -0.43162981  1.27493219]\n",
      "Gradient: [ 5.02061532e-06  5.71150700e-04 -2.17378341e-04]\n",
      "Loss value: 374.9784743305728\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1208\n",
      "Current point: [-2.46591873 -0.43163095  1.27493262]\n",
      "Gradient: [ 4.98552844e-06  5.67155490e-04 -2.15857792e-04]\n",
      "Loss value: 374.97847432982843\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1209\n",
      "Current point: [-2.46591874 -0.43163209  1.27493306]\n",
      "Gradient: [ 4.95068652e-06  5.63188226e-04 -2.14347879e-04]\n",
      "Loss value: 374.9784743290944\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1210\n",
      "Current point: [-2.46591875 -0.43163321  1.27493348]\n",
      "Gradient: [ 4.91608789e-06  5.59248714e-04 -2.12848527e-04]\n",
      "Loss value: 374.9784743283707\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1211\n",
      "Current point: [-2.46591876 -0.43163433  1.27493391]\n",
      "Gradient: [ 4.88173083e-06  5.55336759e-04 -2.11359664e-04]\n",
      "Loss value: 374.978474327657\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1212\n",
      "Current point: [-2.46591877 -0.43163544  1.27493433]\n",
      "Gradient: [ 4.84761368e-06  5.51452169e-04 -2.09881215e-04]\n",
      "Loss value: 374.9784743269533\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1213\n",
      "Current point: [-2.46591878 -0.43163654  1.27493475]\n",
      "Gradient: [ 4.81373475e-06  5.47594751e-04 -2.08413108e-04]\n",
      "Loss value: 374.9784743262594\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1214\n",
      "Current point: [-2.46591879 -0.43163764  1.27493517]\n",
      "Gradient: [ 4.78009239e-06  5.43764317e-04 -2.06955270e-04]\n",
      "Loss value: 374.9784743255751\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1215\n",
      "Current point: [-2.4659188  -0.43163873  1.27493558]\n",
      "Gradient: [ 4.74668495e-06  5.39960677e-04 -2.05507629e-04]\n",
      "Loss value: 374.97847432490045\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1216\n",
      "Current point: [-2.46591881 -0.43163981  1.27493599]\n",
      "Gradient: [ 4.71351077e-06  5.36183644e-04 -2.04070115e-04]\n",
      "Loss value: 374.9784743242351\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1217\n",
      "Current point: [-2.46591882 -0.43164088  1.2749364 ]\n",
      "Gradient: [ 4.68056826e-06  5.32433031e-04 -2.02642656e-04]\n",
      "Loss value: 374.9784743235791\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1218\n",
      "Current point: [-2.46591882 -0.43164194  1.27493681]\n",
      "Gradient: [ 4.64785578e-06  5.28708654e-04 -2.01225182e-04]\n",
      "Loss value: 374.97847432293224\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1219\n",
      "Current point: [-2.46591883 -0.431643    1.27493721]\n",
      "Gradient: [ 4.61537172e-06  5.25010330e-04 -1.99817623e-04]\n",
      "Loss value: 374.97847432229435\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1220\n",
      "Current point: [-2.46591884 -0.43164405  1.27493761]\n",
      "Gradient: [ 4.58311452e-06  5.21337875e-04 -1.98419910e-04]\n",
      "Loss value: 374.97847432166543\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1221\n",
      "Current point: [-2.46591885 -0.43164509  1.27493801]\n",
      "Gradient: [ 4.55108257e-06  5.17691110e-04 -1.97031974e-04]\n",
      "Loss value: 374.97847432104527\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1222\n",
      "Current point: [-2.46591886 -0.43164613  1.2749384 ]\n",
      "Gradient: [ 4.51927431e-06  5.14069854e-04 -1.95653747e-04]\n",
      "Loss value: 374.97847432043375\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1223\n",
      "Current point: [-2.46591887 -0.43164716  1.27493879]\n",
      "Gradient: [ 4.48768818e-06  5.10473929e-04 -1.94285160e-04]\n",
      "Loss value: 374.97847431983064\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1224\n",
      "Current point: [-2.46591888 -0.43164818  1.27493918]\n",
      "Gradient: [ 4.45632264e-06  5.06903158e-04 -1.92926146e-04]\n",
      "Loss value: 374.97847431923606\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1225\n",
      "Current point: [-2.46591889 -0.43164919  1.27493957]\n",
      "Gradient: [ 4.42517613e-06  5.03357364e-04 -1.91576639e-04]\n",
      "Loss value: 374.9784743186497\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1226\n",
      "Current point: [-2.4659189  -0.4316502   1.27493995]\n",
      "Gradient: [ 4.39424714e-06  4.99836374e-04 -1.90236571e-04]\n",
      "Loss value: 374.97847431807156\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1227\n",
      "Current point: [-2.46591891 -0.4316512   1.27494033]\n",
      "Gradient: [ 4.36353416e-06  4.96340014e-04 -1.88905877e-04]\n",
      "Loss value: 374.9784743175015\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1228\n",
      "Current point: [-2.46591891 -0.43165219  1.27494071]\n",
      "Gradient: [ 4.33303566e-06  4.92868110e-04 -1.87584491e-04]\n",
      "Loss value: 374.97847431693936\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1229\n",
      "Current point: [-2.46591892 -0.43165318  1.27494108]\n",
      "Gradient: [ 4.30275017e-06  4.89420493e-04 -1.86272348e-04]\n",
      "Loss value: 374.9784743163851\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1230\n",
      "Current point: [-2.46591893 -0.43165416  1.27494146]\n",
      "Gradient: [ 4.27267620e-06  4.85996992e-04 -1.84969383e-04]\n",
      "Loss value: 374.9784743158385\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1231\n",
      "Current point: [-2.46591894 -0.43165513  1.27494183]\n",
      "Gradient: [ 4.24281226e-06  4.82597439e-04 -1.83675533e-04]\n",
      "Loss value: 374.9784743152995\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1232\n",
      "Current point: [-2.46591895 -0.43165609  1.27494219]\n",
      "Gradient: [ 4.21315688e-06  4.79221666e-04 -1.82390733e-04]\n",
      "Loss value: 374.97847431476805\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1233\n",
      "Current point: [-2.46591896 -0.43165705  1.27494256]\n",
      "Gradient: [ 4.18370863e-06  4.75869507e-04 -1.81114920e-04]\n",
      "Loss value: 374.978474314244\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1234\n",
      "Current point: [-2.46591897 -0.431658    1.27494292]\n",
      "Gradient: [ 4.15446604e-06  4.72540796e-04 -1.79848031e-04]\n",
      "Loss value: 374.97847431372736\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1235\n",
      "Current point: [-2.46591897 -0.43165895  1.27494328]\n",
      "Gradient: [ 4.12542770e-06  4.69235370e-04 -1.78590004e-04]\n",
      "Loss value: 374.97847431321776\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1236\n",
      "Current point: [-2.46591898 -0.43165989  1.27494364]\n",
      "Gradient: [ 4.09659217e-06  4.65953065e-04 -1.77340777e-04]\n",
      "Loss value: 374.9784743127154\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1237\n",
      "Current point: [-2.46591899 -0.43166082  1.27494399]\n",
      "Gradient: [ 4.06795806e-06  4.62693721e-04 -1.76100288e-04]\n",
      "Loss value: 374.97847431222\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1238\n",
      "Current point: [-2.465919   -0.43166174  1.27494434]\n",
      "Gradient: [ 4.03952396e-06  4.59457176e-04 -1.74868476e-04]\n",
      "Loss value: 374.9784743117314\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1239\n",
      "Current point: [-2.46591901 -0.43166266  1.27494469]\n",
      "Gradient: [ 4.01128842e-06  4.56243270e-04 -1.73645281e-04]\n",
      "Loss value: 374.97847431124967\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1240\n",
      "Current point: [-2.46591901 -0.43166358  1.27494504]\n",
      "Gradient: [ 3.98325012e-06  4.53051847e-04 -1.72430642e-04]\n",
      "Loss value: 374.97847431077474\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1241\n",
      "Current point: [-2.46591902 -0.43166448  1.27494539]\n",
      "Gradient: [ 3.95540766e-06  4.49882747e-04 -1.71224500e-04]\n",
      "Loss value: 374.97847431030635\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1242\n",
      "Current point: [-2.46591903 -0.43166538  1.27494573]\n",
      "Gradient: [ 3.92775969e-06  4.46735815e-04 -1.70026794e-04]\n",
      "Loss value: 374.9784743098445\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1243\n",
      "Current point: [-2.46591904 -0.43166627  1.27494607]\n",
      "Gradient: [ 3.90030482e-06  4.43610897e-04 -1.68837466e-04]\n",
      "Loss value: 374.9784743093892\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1244\n",
      "Current point: [-2.46591905 -0.43166716  1.27494641]\n",
      "Gradient: [ 3.87304173e-06  4.40507837e-04 -1.67656457e-04]\n",
      "Loss value: 374.9784743089401\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1245\n",
      "Current point: [-2.46591905 -0.43166804  1.27494674]\n",
      "Gradient: [ 3.84596909e-06  4.37426483e-04 -1.66483710e-04]\n",
      "Loss value: 374.9784743084973\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1246\n",
      "Current point: [-2.46591906 -0.43166892  1.27494707]\n",
      "Gradient: [ 3.81908552e-06  4.34366684e-04 -1.65319166e-04]\n",
      "Loss value: 374.97847430806064\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1247\n",
      "Current point: [-2.46591907 -0.43166979  1.2749474 ]\n",
      "Gradient: [ 3.79238975e-06  4.31328288e-04 -1.64162767e-04]\n",
      "Loss value: 374.9784743076302\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1248\n",
      "Current point: [-2.46591908 -0.43167065  1.27494773]\n",
      "Gradient: [ 3.76588046e-06  4.28311146e-04 -1.63014458e-04]\n",
      "Loss value: 374.9784743072057\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1249\n",
      "Current point: [-2.46591908 -0.43167151  1.27494806]\n",
      "Gradient: [ 3.73955634e-06  4.25315109e-04 -1.61874181e-04]\n",
      "Loss value: 374.978474306787\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1250\n",
      "Current point: [-2.46591909 -0.43167236  1.27494838]\n",
      "Gradient: [ 3.71341611e-06  4.22340030e-04 -1.60741880e-04]\n",
      "Loss value: 374.9784743063743\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1251\n",
      "Current point: [-2.4659191 -0.4316732  1.2749487]\n",
      "Gradient: [ 3.68745849e-06  4.19385761e-04 -1.59617499e-04]\n",
      "Loss value: 374.97847430596727\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1252\n",
      "Current point: [-2.46591911 -0.43167404  1.27494902]\n",
      "Gradient: [ 3.66168218e-06  4.16452158e-04 -1.58500984e-04]\n",
      "Loss value: 374.9784743055659\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1253\n",
      "Current point: [-2.46591911 -0.43167487  1.27494934]\n",
      "Gradient: [ 3.63608593e-06  4.13539075e-04 -1.57392278e-04]\n",
      "Loss value: 374.97847430517015\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1254\n",
      "Current point: [-2.46591912 -0.4316757   1.27494966]\n",
      "Gradient: [ 3.61066851e-06  4.10646369e-04 -1.56291328e-04]\n",
      "Loss value: 374.9784743047799\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1255\n",
      "Current point: [-2.46591913 -0.43167652  1.27494997]\n",
      "Gradient: [ 3.58542865e-06  4.07773898e-04 -1.55198079e-04]\n",
      "Loss value: 374.97847430439515\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1256\n",
      "Current point: [-2.46591914 -0.43167734  1.27495028]\n",
      "Gradient: [ 3.56036512e-06  4.04921521e-04 -1.54112477e-04]\n",
      "Loss value: 374.97847430401566\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1257\n",
      "Current point: [-2.46591914 -0.43167815  1.27495059]\n",
      "Gradient: [ 3.53547666e-06  4.02089095e-04 -1.53034469e-04]\n",
      "Loss value: 374.9784743036416\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1258\n",
      "Current point: [-2.46591915 -0.43167895  1.27495089]\n",
      "Gradient: [ 3.51076207e-06  3.99276483e-04 -1.51964001e-04]\n",
      "Loss value: 374.9784743032726\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1259\n",
      "Current point: [-2.46591916 -0.43167975  1.2749512 ]\n",
      "Gradient: [ 3.48622013e-06  3.96483545e-04 -1.50901022e-04]\n",
      "Loss value: 374.97847430290886\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1260\n",
      "Current point: [-2.46591916 -0.43168054  1.2749515 ]\n",
      "Gradient: [ 3.46184964e-06  3.93710144e-04 -1.49845478e-04]\n",
      "Loss value: 374.9784743025501\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1261\n",
      "Current point: [-2.46591917 -0.43168133  1.2749518 ]\n",
      "Gradient: [ 3.43764939e-06  3.90956143e-04 -1.48797317e-04]\n",
      "Loss value: 374.97847430219645\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1262\n",
      "Current point: [-2.46591918 -0.43168211  1.2749521 ]\n",
      "Gradient: [ 3.41361823e-06  3.88221406e-04 -1.47756488e-04]\n",
      "Loss value: 374.97847430184765\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1263\n",
      "Current point: [-2.46591918 -0.43168289  1.27495239]\n",
      "Gradient: [ 3.38975495e-06  3.85505799e-04 -1.46722939e-04]\n",
      "Loss value: 374.97847430150375\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1264\n",
      "Current point: [-2.46591919 -0.43168366  1.27495268]\n",
      "Gradient: [ 3.36605842e-06  3.82809187e-04 -1.45696621e-04]\n",
      "Loss value: 374.9784743011646\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1265\n",
      "Current point: [-2.4659192  -0.43168442  1.27495298]\n",
      "Gradient: [ 3.34252743e-06  3.80131439e-04 -1.44677481e-04]\n",
      "Loss value: 374.97847430083027\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1266\n",
      "Current point: [-2.4659192  -0.43168519  1.27495327]\n",
      "Gradient: [ 3.31916083e-06  3.77472421e-04 -1.43665470e-04]\n",
      "Loss value: 374.9784743005005\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1267\n",
      "Current point: [-2.46591921 -0.43168594  1.27495355]\n",
      "Gradient: [ 3.29595748e-06  3.74832004e-04 -1.42660538e-04]\n",
      "Loss value: 374.97847430017544\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1268\n",
      "Current point: [-2.46591922 -0.43168669  1.27495384]\n",
      "Gradient: [ 3.27291622e-06  3.72210056e-04 -1.41662635e-04]\n",
      "Loss value: 374.97847429985484\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1269\n",
      "Current point: [-2.46591922 -0.43168743  1.27495412]\n",
      "Gradient: [ 3.25003595e-06  3.69606449e-04 -1.40671713e-04]\n",
      "Loss value: 374.9784742995387\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1270\n",
      "Current point: [-2.46591923 -0.43168817  1.2749544 ]\n",
      "Gradient: [ 3.22731552e-06  3.67021054e-04 -1.39687722e-04]\n",
      "Loss value: 374.97847429922695\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1271\n",
      "Current point: [-2.46591924 -0.43168891  1.27495468]\n",
      "Gradient: [ 3.20475385e-06  3.64453744e-04 -1.38710615e-04]\n",
      "Loss value: 374.9784742989196\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1272\n",
      "Current point: [-2.46591924 -0.43168964  1.27495496]\n",
      "Gradient: [ 3.18234982e-06  3.61904393e-04 -1.37740342e-04]\n",
      "Loss value: 374.97847429861645\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1273\n",
      "Current point: [-2.46591925 -0.43169036  1.27495524]\n",
      "Gradient: [ 3.16010234e-06  3.59372874e-04 -1.36776855e-04]\n",
      "Loss value: 374.97847429831756\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1274\n",
      "Current point: [-2.46591926 -0.43169108  1.27495551]\n",
      "Gradient: [ 3.13801028e-06  3.56859064e-04 -1.35820109e-04]\n",
      "Loss value: 374.9784742980229\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1275\n",
      "Current point: [-2.46591926 -0.43169179  1.27495578]\n",
      "Gradient: [ 3.11607256e-06  3.54362837e-04 -1.34870055e-04]\n",
      "Loss value: 374.9784742977323\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1276\n",
      "Current point: [-2.46591927 -0.4316925   1.27495605]\n",
      "Gradient: [ 3.09428814e-06  3.51884072e-04 -1.33926646e-04]\n",
      "Loss value: 374.97847429744576\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1277\n",
      "Current point: [-2.46591928 -0.4316932   1.27495632]\n",
      "Gradient: [ 3.07265592e-06  3.49422646e-04 -1.32989837e-04]\n",
      "Loss value: 374.97847429716325\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1278\n",
      "Current point: [-2.46591928 -0.4316939   1.27495658]\n",
      "Gradient: [ 3.05117486e-06  3.46978438e-04 -1.32059580e-04]\n",
      "Loss value: 374.97847429688466\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1279\n",
      "Current point: [-2.46591929 -0.4316946   1.27495685]\n",
      "Gradient: [ 3.02984387e-06  3.44551327e-04 -1.31135831e-04]\n",
      "Loss value: 374.9784742966099\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1280\n",
      "Current point: [-2.46591929 -0.43169529  1.27495711]\n",
      "Gradient: [ 3.00866193e-06  3.42141194e-04 -1.30218543e-04]\n",
      "Loss value: 374.978474296339\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1281\n",
      "Current point: [-2.4659193  -0.43169597  1.27495737]\n",
      "Gradient: [ 2.98762799e-06  3.39747920e-04 -1.29307672e-04]\n",
      "Loss value: 374.9784742960719\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1282\n",
      "Current point: [-2.46591931 -0.43169665  1.27495763]\n",
      "Gradient: [ 2.96674103e-06  3.37371386e-04 -1.28403172e-04]\n",
      "Loss value: 374.9784742958085\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1283\n",
      "Current point: [-2.46591931 -0.43169733  1.27495789]\n",
      "Gradient: [ 2.94600002e-06  3.35011477e-04 -1.27504999e-04]\n",
      "Loss value: 374.97847429554884\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1284\n",
      "Current point: [-2.46591932 -0.431698    1.27495814]\n",
      "Gradient: [ 2.92540392e-06  3.32668075e-04 -1.26613108e-04]\n",
      "Loss value: 374.9784742952927\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1285\n",
      "Current point: [-2.46591932 -0.43169866  1.27495839]\n",
      "Gradient: [ 2.90495174e-06  3.30341066e-04 -1.25727457e-04]\n",
      "Loss value: 374.97847429504014\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1286\n",
      "Current point: [-2.46591933 -0.43169932  1.27495865]\n",
      "Gradient: [ 2.88464245e-06  3.28030334e-04 -1.24848000e-04]\n",
      "Loss value: 374.97847429479117\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1287\n",
      "Current point: [-2.46591933 -0.43169998  1.2749589 ]\n",
      "Gradient: [ 2.86447511e-06  3.25735765e-04 -1.23974695e-04]\n",
      "Loss value: 374.9784742945456\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1288\n",
      "Current point: [-2.46591934 -0.43170063  1.27495914]\n",
      "Gradient: [ 2.84444869e-06  3.23457248e-04 -1.23107499e-04]\n",
      "Loss value: 374.9784742943035\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1289\n",
      "Current point: [-2.46591935 -0.43170128  1.27495939]\n",
      "Gradient: [ 2.82456221e-06  3.21194668e-04 -1.22246369e-04]\n",
      "Loss value: 374.97847429406477\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1290\n",
      "Current point: [-2.46591935 -0.43170192  1.27495963]\n",
      "Gradient: [ 2.80481469e-06  3.18947916e-04 -1.21391262e-04]\n",
      "Loss value: 374.9784742938294\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1291\n",
      "Current point: [-2.46591936 -0.43170256  1.27495988]\n",
      "Gradient: [ 2.78520517e-06  3.16716879e-04 -1.20542137e-04]\n",
      "Loss value: 374.9784742935973\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1292\n",
      "Current point: [-2.46591936 -0.43170319  1.27496012]\n",
      "Gradient: [ 2.76573265e-06  3.14501449e-04 -1.19698952e-04]\n",
      "Loss value: 374.9784742933683\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1293\n",
      "Current point: [-2.46591937 -0.43170382  1.27496036]\n",
      "Gradient: [ 2.74639620e-06  3.12301515e-04 -1.18861664e-04]\n",
      "Loss value: 374.97847429314265\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1294\n",
      "Current point: [-2.46591937 -0.43170444  1.2749606 ]\n",
      "Gradient: [ 2.72719488e-06  3.10116970e-04 -1.18030233e-04]\n",
      "Loss value: 374.97847429292005\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1295\n",
      "Current point: [-2.46591938 -0.43170506  1.27496083]\n",
      "Gradient: [ 2.70812775e-06  3.07947707e-04 -1.17204618e-04]\n",
      "Loss value: 374.97847429270064\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1296\n",
      "Current point: [-2.46591938 -0.43170568  1.27496107]\n",
      "Gradient: [ 2.68919385e-06  3.05793617e-04 -1.16384778e-04]\n",
      "Loss value: 374.9784742924842\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1297\n",
      "Current point: [-2.46591939 -0.43170629  1.2749613 ]\n",
      "Gradient: [ 2.67039229e-06  3.03654595e-04 -1.15570673e-04]\n",
      "Loss value: 374.9784742922709\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1298\n",
      "Current point: [-2.4659194  -0.4317069   1.27496153]\n",
      "Gradient: [ 2.65172208e-06  3.01530535e-04 -1.14762263e-04]\n",
      "Loss value: 374.97847429206047\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1299\n",
      "Current point: [-2.4659194  -0.4317075   1.27496176]\n",
      "Gradient: [ 2.63318235e-06  2.99421334e-04 -1.13959507e-04]\n",
      "Loss value: 374.978474291853\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1300\n",
      "Current point: [-2.46591941 -0.4317081   1.27496199]\n",
      "Gradient: [ 2.61477220e-06  2.97326886e-04 -1.13162367e-04]\n",
      "Loss value: 374.97847429164835\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1301\n",
      "Current point: [-2.46591941 -0.43170869  1.27496221]\n",
      "Gradient: [ 2.59649069e-06  2.95247089e-04 -1.12370802e-04]\n",
      "Loss value: 374.9784742914467\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1302\n",
      "Current point: [-2.46591942 -0.43170929  1.27496244]\n",
      "Gradient: [ 2.57833693e-06  2.93181841e-04 -1.11584775e-04]\n",
      "Loss value: 374.9784742912478\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1303\n",
      "Current point: [-2.46591942 -0.43170987  1.27496266]\n",
      "Gradient: [ 2.56031006e-06  2.91131039e-04 -1.10804245e-04]\n",
      "Loss value: 374.97847429105167\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1304\n",
      "Current point: [-2.46591943 -0.43171045  1.27496288]\n",
      "Gradient: [ 2.54240918e-06  2.89094582e-04 -1.10029176e-04]\n",
      "Loss value: 374.97847429085823\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1305\n",
      "Current point: [-2.46591943 -0.43171103  1.2749631 ]\n",
      "Gradient: [ 2.52463335e-06  2.87072370e-04 -1.09259528e-04]\n",
      "Loss value: 374.9784742906676\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1306\n",
      "Current point: [-2.46591944 -0.43171161  1.27496332]\n",
      "Gradient: [ 2.50698176e-06  2.85064304e-04 -1.08495263e-04]\n",
      "Loss value: 374.9784742904794\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1307\n",
      "Current point: [-2.46591944 -0.43171218  1.27496354]\n",
      "Gradient: [ 2.48945355e-06  2.83070284e-04 -1.07736345e-04]\n",
      "Loss value: 374.97847429029406\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1308\n",
      "Current point: [-2.46591945 -0.43171274  1.27496375]\n",
      "Gradient: [ 2.47204782e-06  2.81090212e-04 -1.06982735e-04]\n",
      "Loss value: 374.9784742901112\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1309\n",
      "Current point: [-2.46591945 -0.4317133   1.27496397]\n",
      "Gradient: [ 2.45476373e-06  2.79123991e-04 -1.06234397e-04]\n",
      "Loss value: 374.97847428993094\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1310\n",
      "Current point: [-2.46591946 -0.43171386  1.27496418]\n",
      "Gradient: [ 2.43760045e-06  2.77171524e-04 -1.05491293e-04]\n",
      "Loss value: 374.97847428975314\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1311\n",
      "Current point: [-2.46591946 -0.43171442  1.27496439]\n",
      "Gradient: [ 2.42055712e-06  2.75232714e-04 -1.04753387e-04]\n",
      "Loss value: 374.97847428957783\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1312\n",
      "Current point: [-2.46591947 -0.43171497  1.2749646 ]\n",
      "Gradient: [ 2.40363291e-06  2.73307466e-04 -1.04020643e-04]\n",
      "Loss value: 374.97847428940497\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1313\n",
      "Current point: [-2.46591947 -0.43171551  1.27496481]\n",
      "Gradient: [ 2.38682696e-06  2.71395686e-04 -1.03293025e-04]\n",
      "Loss value: 374.97847428923455\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1314\n",
      "Current point: [-2.46591948 -0.43171606  1.27496502]\n",
      "Gradient: [ 2.37013847e-06  2.69497278e-04 -1.02570496e-04]\n",
      "Loss value: 374.97847428906647\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1315\n",
      "Current point: [-2.46591948 -0.4317166   1.27496522]\n",
      "Gradient: [ 2.35356659e-06  2.67612150e-04 -1.01853021e-04]\n",
      "Loss value: 374.9784742889007\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1316\n",
      "Current point: [-2.46591949 -0.43171713  1.27496542]\n",
      "Gradient: [ 2.33711054e-06  2.65740208e-04 -1.01140564e-04]\n",
      "Loss value: 374.9784742887373\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1317\n",
      "Current point: [-2.46591949 -0.43171766  1.27496563]\n",
      "Gradient: [ 2.32076953e-06  2.63881360e-04 -1.00433092e-04]\n",
      "Loss value: 374.9784742885762\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1318\n",
      "Current point: [-2.46591949 -0.43171819  1.27496583]\n",
      "Gradient: [ 2.30454269e-06  2.62035516e-04 -9.97305679e-05]\n",
      "Loss value: 374.97847428841726\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1319\n",
      "Current point: [-2.4659195  -0.43171871  1.27496603]\n",
      "Gradient: [ 2.28842927e-06  2.60202582e-04 -9.90329580e-05]\n",
      "Loss value: 374.9784742882606\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1320\n",
      "Current point: [-2.4659195  -0.43171924  1.27496622]\n",
      "Gradient: [ 2.27242849e-06  2.58382471e-04 -9.83402280e-05]\n",
      "Loss value: 374.9784742881061\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1321\n",
      "Current point: [-2.46591951 -0.43171975  1.27496642]\n",
      "Gradient: [ 2.25653954e-06  2.56575091e-04 -9.76523435e-05]\n",
      "Loss value: 374.97847428795376\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1322\n",
      "Current point: [-2.46591951 -0.43172027  1.27496662]\n",
      "Gradient: [ 2.24076162e-06  2.54780353e-04 -9.69692707e-05]\n",
      "Loss value: 374.9784742878036\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1323\n",
      "Current point: [-2.46591952 -0.43172077  1.27496681]\n",
      "Gradient: [ 2.2250940e-06  2.5299817e-04 -9.6290976e-05]\n",
      "Loss value: 374.97847428765544\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1324\n",
      "Current point: [-2.46591952 -0.43172128  1.274967  ]\n",
      "Gradient: [ 2.20953585e-06  2.51228453e-04 -9.56174260e-05]\n",
      "Loss value: 374.97847428750936\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1325\n",
      "Current point: [-2.46591953 -0.43172178  1.27496719]\n",
      "Gradient: [ 2.19408647e-06  2.49471116e-04 -9.49485873e-05]\n",
      "Loss value: 374.9784742873653\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1326\n",
      "Current point: [-2.46591953 -0.43172228  1.27496738]\n",
      "Gradient: [ 2.17874508e-06  2.47726071e-04 -9.42844272e-05]\n",
      "Loss value: 374.9784742872233\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1327\n",
      "Current point: [-2.46591954 -0.43172278  1.27496757]\n",
      "Gradient: [ 2.16351088e-06  2.45993232e-04 -9.36249128e-05]\n",
      "Loss value: 374.97847428708326\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1328\n",
      "Current point: [-2.46591954 -0.43172327  1.27496776]\n",
      "Gradient: [ 2.14838318e-06  2.44272515e-04 -9.29700117e-05]\n",
      "Loss value: 374.97847428694524\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1329\n",
      "Current point: [-2.46591954 -0.43172376  1.27496795]\n",
      "Gradient: [ 2.13336121e-06  2.42563834e-04 -9.23196916e-05]\n",
      "Loss value: 374.97847428680905\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1330\n",
      "Current point: [-2.46591955 -0.43172424  1.27496813]\n",
      "Gradient: [ 2.11844426e-06  2.40867106e-04 -9.16739204e-05]\n",
      "Loss value: 374.97847428667484\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1331\n",
      "Current point: [-2.46591955 -0.43172472  1.27496831]\n",
      "Gradient: [ 2.10363155e-06  2.39182246e-04 -9.10326664e-05]\n",
      "Loss value: 374.97847428654245\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1332\n",
      "Current point: [-2.46591956 -0.4317252   1.2749685 ]\n",
      "Gradient: [ 2.08892239e-06  2.37509172e-04 -9.03958979e-05]\n",
      "Loss value: 374.9784742864119\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1333\n",
      "Current point: [-2.46591956 -0.43172568  1.27496868]\n",
      "Gradient: [ 2.07431604e-06  2.35847801e-04 -8.97635836e-05]\n",
      "Loss value: 374.9784742862832\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1334\n",
      "Current point: [-2.46591956 -0.43172615  1.27496886]\n",
      "Gradient: [ 2.05981177e-06  2.34198051e-04 -8.91356923e-05]\n",
      "Loss value: 374.97847428615626\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1335\n",
      "Current point: [-2.46591957 -0.43172662  1.27496903]\n",
      "Gradient: [ 2.04540887e-06  2.32559841e-04 -8.85121930e-05]\n",
      "Loss value: 374.9784742860311\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1336\n",
      "Current point: [-2.46591957 -0.43172708  1.27496921]\n",
      "Gradient: [ 2.03110665e-06  2.30933090e-04 -8.78930551e-05]\n",
      "Loss value: 374.9784742859077\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1337\n",
      "Current point: [-2.46591958 -0.43172755  1.27496939]\n",
      "Gradient: [ 2.01690442e-06  2.29317719e-04 -8.72782480e-05]\n",
      "Loss value: 374.978474285786\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1338\n",
      "Current point: [-2.46591958 -0.431728    1.27496956]\n",
      "Gradient: [ 2.00280144e-06  2.27713647e-04 -8.66677415e-05]\n",
      "Loss value: 374.97847428566604\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1339\n",
      "Current point: [-2.46591959 -0.43172846  1.27496974]\n",
      "Gradient: [ 1.98879704e-06  2.26120796e-04 -8.60615054e-05]\n",
      "Loss value: 374.9784742855477\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1340\n",
      "Current point: [-2.46591959 -0.43172891  1.27496991]\n",
      "Gradient: [ 1.97489053e-06  2.24539087e-04 -8.54595099e-05]\n",
      "Loss value: 374.97847428543093\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1341\n",
      "Current point: [-2.46591959 -0.43172936  1.27497008]\n",
      "Gradient: [ 1.96108125e-06  2.22968441e-04 -8.48617253e-05]\n",
      "Loss value: 374.97847428531594\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1342\n",
      "Current point: [-2.4659196  -0.43172981  1.27497025]\n",
      "Gradient: [ 1.94736847e-06  2.21408783e-04 -8.42681222e-05]\n",
      "Loss value: 374.9784742852025\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1343\n",
      "Current point: [-2.4659196  -0.43173025  1.27497042]\n",
      "Gradient: [ 1.93375154e-06  2.19860034e-04 -8.36786713e-05]\n",
      "Loss value: 374.9784742850906\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1344\n",
      "Current point: [-2.4659196  -0.43173069  1.27497058]\n",
      "Gradient: [ 1.92022980e-06  2.18322119e-04 -8.30933436e-05]\n",
      "Loss value: 374.97847428498034\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1345\n",
      "Current point: [-2.46591961 -0.43173113  1.27497075]\n",
      "Gradient: [ 1.90680259e-06  2.16794961e-04 -8.25121102e-05]\n",
      "Loss value: 374.9784742848716\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1346\n",
      "Current point: [-2.46591961 -0.43173156  1.27497092]\n",
      "Gradient: [ 1.89346925e-06  2.15278486e-04 -8.19349425e-05]\n",
      "Loss value: 374.97847428476433\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1347\n",
      "Current point: [-2.46591962 -0.43173199  1.27497108]\n",
      "Gradient: [ 1.88022909e-06  2.13772619e-04 -8.13618121e-05]\n",
      "Loss value: 374.97847428465855\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1348\n",
      "Current point: [-2.46591962 -0.43173242  1.27497124]\n",
      "Gradient: [ 1.86708146e-06  2.12277285e-04 -8.07926907e-05]\n",
      "Loss value: 374.9784742845543\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1349\n",
      "Current point: [-2.46591962 -0.43173284  1.2749714 ]\n",
      "Gradient: [ 1.85402576e-06  2.10792411e-04 -8.02275503e-05]\n",
      "Loss value: 374.9784742844515\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1350\n",
      "Current point: [-2.46591963 -0.43173326  1.27497156]\n",
      "Gradient: [ 1.84106130e-06  2.09317924e-04 -7.96663630e-05]\n",
      "Loss value: 374.97847428435006\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1351\n",
      "Current point: [-2.46591963 -0.43173368  1.27497172]\n",
      "Gradient: [ 1.82818748e-06  2.07853750e-04 -7.91091011e-05]\n",
      "Loss value: 374.97847428425007\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1352\n",
      "Current point: [-2.46591963 -0.4317341   1.27497188]\n",
      "Gradient: [ 1.81540367e-06  2.06399819e-04 -7.85557373e-05]\n",
      "Loss value: 374.9784742841515\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1353\n",
      "Current point: [-2.46591964 -0.43173451  1.27497204]\n",
      "Gradient: [ 1.80270921e-06  2.04956058e-04 -7.80062442e-05]\n",
      "Loss value: 374.9784742840543\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1354\n",
      "Current point: [-2.46591964 -0.43173492  1.27497219]\n",
      "Gradient: [ 1.79010346e-06  2.03522396e-04 -7.74605949e-05]\n",
      "Loss value: 374.97847428395846\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1355\n",
      "Current point: [-2.46591965 -0.43173533  1.27497235]\n",
      "Gradient: [ 1.77758586e-06  2.02098762e-04 -7.69187623e-05]\n",
      "Loss value: 374.978474283864\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1356\n",
      "Current point: [-2.46591965 -0.43173573  1.2749725 ]\n",
      "Gradient: [ 1.76515574e-06  2.00685087e-04 -7.63807197e-05]\n",
      "Loss value: 374.97847428377077\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1357\n",
      "Current point: [-2.46591965 -0.43173613  1.27497266]\n",
      "Gradient: [ 1.75281253e-06  1.99281301e-04 -7.58464408e-05]\n",
      "Loss value: 374.9784742836788\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1358\n",
      "Current point: [-2.46591966 -0.43173653  1.27497281]\n",
      "Gradient: [ 1.74055559e-06  1.97887334e-04 -7.53158991e-05]\n",
      "Loss value: 374.97847428358824\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1359\n",
      "Current point: [-2.46591966 -0.43173693  1.27497296]\n",
      "Gradient: [ 1.72838432e-06  1.96503117e-04 -7.47890686e-05]\n",
      "Loss value: 374.9784742834988\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1360\n",
      "Current point: [-2.46591966 -0.43173732  1.27497311]\n",
      "Gradient: [ 1.71629818e-06  1.95128584e-04 -7.42659231e-05]\n",
      "Loss value: 374.9784742834107\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1361\n",
      "Current point: [-2.46591967 -0.43173771  1.27497326]\n",
      "Gradient: [ 1.70429650e-06  1.93763665e-04 -7.37464370e-05]\n",
      "Loss value: 374.97847428332386\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1362\n",
      "Current point: [-2.46591967 -0.4317381   1.2749734 ]\n",
      "Gradient: [ 1.69237873e-06  1.92408294e-04 -7.32305848e-05]\n",
      "Loss value: 374.97847428323826\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1363\n",
      "Current point: [-2.46591967 -0.43173848  1.27497355]\n",
      "Gradient: [ 1.68054428e-06  1.91062403e-04 -7.27183408e-05]\n",
      "Loss value: 374.97847428315373\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1364\n",
      "Current point: [-2.46591968 -0.43173887  1.2749737 ]\n",
      "Gradient: [ 1.66879252e-06  1.89725927e-04 -7.22096801e-05]\n",
      "Loss value: 374.97847428307045\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1365\n",
      "Current point: [-2.46591968 -0.43173924  1.27497384]\n",
      "Gradient: [ 1.65712296e-06  1.88398800e-04 -7.17045773e-05]\n",
      "Loss value: 374.97847428298826\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1366\n",
      "Current point: [-2.46591968 -0.43173962  1.27497398]\n",
      "Gradient: [ 1.64553498e-06  1.87080956e-04 -7.12030077e-05]\n",
      "Loss value: 374.9784742829073\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1367\n",
      "Current point: [-2.46591969 -0.43174     1.27497413]\n",
      "Gradient: [ 1.63402798e-06  1.85772330e-04 -7.07049465e-05]\n",
      "Loss value: 374.9784742828274\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1368\n",
      "Current point: [-2.46591969 -0.43174037  1.27497427]\n",
      "Gradient: [ 1.62260142e-06  1.84472859e-04 -7.02103693e-05]\n",
      "Loss value: 374.97847428274866\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1369\n",
      "Current point: [-2.46591969 -0.43174074  1.27497441]\n",
      "Gradient: [ 1.61125477e-06  1.83182476e-04 -6.97192516e-05]\n",
      "Loss value: 374.978474282671\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1370\n",
      "Current point: [-2.4659197  -0.4317411   1.27497455]\n",
      "Gradient: [ 1.59998742e-06  1.81901121e-04 -6.92315693e-05]\n",
      "Loss value: 374.97847428259445\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1371\n",
      "Current point: [-2.4659197  -0.43174147  1.27497469]\n",
      "Gradient: [ 1.58879883e-06  1.80628728e-04 -6.87472983e-05]\n",
      "Loss value: 374.97847428251896\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1372\n",
      "Current point: [-2.4659197  -0.43174183  1.27497482]\n",
      "Gradient: [ 1.57768847e-06  1.79365235e-04 -6.82664147e-05]\n",
      "Loss value: 374.9784742824445\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1373\n",
      "Current point: [-2.46591971 -0.43174219  1.27497496]\n",
      "Gradient: [ 1.56665581e-06  1.78110581e-04 -6.77888948e-05]\n",
      "Loss value: 374.97847428237105\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1374\n",
      "Current point: [-2.46591971 -0.43174254  1.2749751 ]\n",
      "Gradient: [ 1.55570023e-06  1.76864703e-04 -6.73147153e-05]\n",
      "Loss value: 374.9784742822987\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1375\n",
      "Current point: [-2.46591971 -0.4317429   1.27497523]\n",
      "Gradient: [ 1.54482126e-06  1.75627540e-04 -6.68438525e-05]\n",
      "Loss value: 374.97847428222735\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1376\n",
      "Current point: [-2.46591972 -0.43174325  1.27497536]\n",
      "Gradient: [ 1.53401836e-06  1.74399031e-04 -6.63762834e-05]\n",
      "Loss value: 374.9784742821569\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1377\n",
      "Current point: [-2.46591972 -0.4317436   1.2749755 ]\n",
      "Gradient: [ 1.52329096e-06  1.73179115e-04 -6.59119850e-05]\n",
      "Loss value: 374.9784742820875\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1378\n",
      "Current point: [-2.46591972 -0.43174394  1.27497563]\n",
      "Gradient: [ 1.51263857e-06  1.71967733e-04 -6.54509342e-05]\n",
      "Loss value: 374.9784742820191\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1379\n",
      "Current point: [-2.46591972 -0.43174429  1.27497576]\n",
      "Gradient: [ 1.50206067e-06  1.70764824e-04 -6.49931085e-05]\n",
      "Loss value: 374.9784742819516\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1380\n",
      "Current point: [-2.46591973 -0.43174463  1.27497589]\n",
      "Gradient: [ 1.49155669e-06  1.69570329e-04 -6.45384853e-05]\n",
      "Loss value: 374.97847428188504\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1381\n",
      "Current point: [-2.46591973 -0.43174497  1.27497602]\n",
      "Gradient: [ 1.48112616e-06  1.68384190e-04 -6.40870421e-05]\n",
      "Loss value: 374.97847428181944\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1382\n",
      "Current point: [-2.46591973 -0.4317453   1.27497615]\n",
      "Gradient: [ 1.47076856e-06  1.67206349e-04 -6.36387567e-05]\n",
      "Loss value: 374.97847428175476\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1383\n",
      "Current point: [-2.46591974 -0.43174564  1.27497627]\n",
      "Gradient: [ 1.46048334e-06  1.66036746e-04 -6.31936071e-05]\n",
      "Loss value: 374.978474281691\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1384\n",
      "Current point: [-2.46591974 -0.43174597  1.2749764 ]\n",
      "Gradient: [ 1.45027005e-06  1.64875324e-04 -6.27515713e-05]\n",
      "Loss value: 374.97847428162805\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1385\n",
      "Current point: [-2.46591974 -0.4317463   1.27497653]\n",
      "Gradient: [ 1.44012814e-06  1.63722026e-04 -6.23126275e-05]\n",
      "Loss value: 374.97847428156604\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1386\n",
      "Current point: [-2.46591975 -0.43174663  1.27497665]\n",
      "Gradient: [ 1.43005714e-06  1.62576796e-04 -6.18767541e-05]\n",
      "Loss value: 374.9784742815049\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1387\n",
      "Current point: [-2.46591975 -0.43174695  1.27497677]\n",
      "Gradient: [ 1.42005657e-06  1.61439577e-04 -6.14439296e-05]\n",
      "Loss value: 374.97847428144456\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1388\n",
      "Current point: [-2.46591975 -0.43174728  1.2749769 ]\n",
      "Gradient: [ 1.41012593e-06  1.60310312e-04 -6.10141326e-05]\n",
      "Loss value: 374.9784742813851\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1389\n",
      "Current point: [-2.46591975 -0.4317476   1.27497702]\n",
      "Gradient: [ 1.40026468e-06  1.59188947e-04 -6.05873421e-05]\n",
      "Loss value: 374.97847428132644\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1390\n",
      "Current point: [-2.46591976 -0.43174791  1.27497714]\n",
      "Gradient: [ 1.39047242e-06  1.58075426e-04 -6.01635370e-05]\n",
      "Loss value: 374.97847428126863\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1391\n",
      "Current point: [-2.46591976 -0.43174823  1.27497726]\n",
      "Gradient: [ 1.38074859e-06  1.56969693e-04 -5.97426963e-05]\n",
      "Loss value: 374.97847428121156\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1392\n",
      "Current point: [-2.46591976 -0.43174854  1.27497738]\n",
      "Gradient: [ 1.37109274e-06  1.55871696e-04 -5.93247995e-05]\n",
      "Loss value: 374.9784742811554\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1393\n",
      "Current point: [-2.46591976 -0.43174886  1.2749775 ]\n",
      "Gradient: [ 1.36150440e-06  1.54781379e-04 -5.89098257e-05]\n",
      "Loss value: 374.9784742810999\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1394\n",
      "Current point: [-2.46591977 -0.43174917  1.27497762]\n",
      "Gradient: [ 1.35198309e-06  1.53698688e-04 -5.84977547e-05]\n",
      "Loss value: 374.97847428104524\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1395\n",
      "Current point: [-2.46591977 -0.43174947  1.27497773]\n",
      "Gradient: [ 1.34252836e-06  1.52623571e-04 -5.80885661e-05]\n",
      "Loss value: 374.97847428099135\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1396\n",
      "Current point: [-2.46591977 -0.43174978  1.27497785]\n",
      "Gradient: [ 1.33313976e-06  1.51555974e-04 -5.76822398e-05]\n",
      "Loss value: 374.97847428093826\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1397\n",
      "Current point: [-2.46591978 -0.43175008  1.27497797]\n",
      "Gradient: [ 1.32381676e-06  1.50495846e-04 -5.72787557e-05]\n",
      "Loss value: 374.9784742808858\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1398\n",
      "Current point: [-2.46591978 -0.43175038  1.27497808]\n",
      "Gradient: [ 1.31455894e-06  1.49443132e-04 -5.68780939e-05]\n",
      "Loss value: 374.9784742808341\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1399\n",
      "Current point: [-2.46591978 -0.43175068  1.27497819]\n",
      "Gradient: [ 1.30536588e-06  1.48397783e-04 -5.64802347e-05]\n",
      "Loss value: 374.9784742807832\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1400\n",
      "Current point: [-2.46591978 -0.43175098  1.27497831]\n",
      "Gradient: [ 1.29623709e-06  1.47359745e-04 -5.60851586e-05]\n",
      "Loss value: 374.9784742807329\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1401\n",
      "Current point: [-2.46591979 -0.43175127  1.27497842]\n",
      "Gradient: [ 1.28717212e-06  1.46328969e-04 -5.56928459e-05]\n",
      "Loss value: 374.9784742806834\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1402\n",
      "Current point: [-2.46591979 -0.43175157  1.27497853]\n",
      "Gradient: [ 1.27817051e-06  1.45305403e-04 -5.53032776e-05]\n",
      "Loss value: 374.9784742806345\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1403\n",
      "Current point: [-2.46591979 -0.43175186  1.27497864]\n",
      "Gradient: [ 1.26923186e-06  1.44288997e-04 -5.49164342e-05]\n",
      "Loss value: 374.9784742805863\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1404\n",
      "Current point: [-2.46591979 -0.43175215  1.27497875]\n",
      "Gradient: [ 1.26035570e-06  1.43279701e-04 -5.45322967e-05]\n",
      "Loss value: 374.9784742805389\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1405\n",
      "Current point: [-2.4659198  -0.43175243  1.27497886]\n",
      "Gradient: [ 1.25154162e-06  1.42277464e-04 -5.41508462e-05]\n",
      "Loss value: 374.978474280492\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1406\n",
      "Current point: [-2.4659198  -0.43175272  1.27497897]\n",
      "Gradient: [ 1.24278915e-06  1.41282238e-04 -5.37720640e-05]\n",
      "Loss value: 374.97847428044577\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1407\n",
      "Current point: [-2.4659198  -0.431753    1.27497908]\n",
      "Gradient: [ 1.23409785e-06  1.40293974e-04 -5.33959314e-05]\n",
      "Loss value: 374.97847428040023\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1408\n",
      "Current point: [-2.4659198  -0.43175328  1.27497918]\n",
      "Gradient: [ 1.22546731e-06  1.39312623e-04 -5.30224298e-05]\n",
      "Loss value: 374.9784742803554\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1409\n",
      "Current point: [-2.46591981 -0.43175356  1.27497929]\n",
      "Gradient: [ 1.21689715e-06  1.38338136e-04 -5.26515408e-05]\n",
      "Loss value: 374.97847428031105\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1410\n",
      "Current point: [-2.46591981 -0.43175383  1.27497939]\n",
      "Gradient: [ 1.20838689e-06  1.37370466e-04 -5.22832461e-05]\n",
      "Loss value: 374.9784742802674\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1411\n",
      "Current point: [-2.46591981 -0.43175411  1.2749795 ]\n",
      "Gradient: [ 1.19993613e-06  1.36409564e-04 -5.19175277e-05]\n",
      "Loss value: 374.9784742802243\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1412\n",
      "Current point: [-2.46591981 -0.43175438  1.2749796 ]\n",
      "Gradient: [ 1.19154448e-06  1.35455384e-04 -5.15543674e-05]\n",
      "Loss value: 374.97847428018184\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1413\n",
      "Current point: [-2.46591982 -0.43175465  1.27497971]\n",
      "Gradient: [ 1.18321151e-06  1.34507879e-04 -5.11937474e-05]\n",
      "Loss value: 374.97847428014\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1414\n",
      "Current point: [-2.46591982 -0.43175492  1.27497981]\n",
      "Gradient: [ 1.17493678e-06  1.33567001e-04 -5.08356499e-05]\n",
      "Loss value: 374.9784742800987\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1415\n",
      "Current point: [-2.46591982 -0.43175519  1.27497991]\n",
      "Gradient: [ 1.16671990e-06  1.32632705e-04 -5.04800573e-05]\n",
      "Loss value: 374.978474280058\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1416\n",
      "Current point: [-2.46591982 -0.43175545  1.27498001]\n",
      "Gradient: [ 1.15856050e-06  1.31704944e-04 -5.01269521e-05]\n",
      "Loss value: 374.97847428001785\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1417\n",
      "Current point: [-2.46591982 -0.43175572  1.27498011]\n",
      "Gradient: [ 1.15045812e-06  1.30783673e-04 -4.97763168e-05]\n",
      "Loss value: 374.9784742799783\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1418\n",
      "Current point: [-2.46591983 -0.43175598  1.27498021]\n",
      "Gradient: [ 1.14241241e-06  1.29868846e-04 -4.94281342e-05]\n",
      "Loss value: 374.97847427993923\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1419\n",
      "Current point: [-2.46591983 -0.43175624  1.27498031]\n",
      "Gradient: [ 1.13442296e-06  1.28960418e-04 -4.90823871e-05]\n",
      "Loss value: 374.97847427990075\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1420\n",
      "Current point: [-2.46591983 -0.4317565   1.27498041]\n",
      "Gradient: [ 1.12648935e-06  1.28058345e-04 -4.87390585e-05]\n",
      "Loss value: 374.9784742798628\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1421\n",
      "Current point: [-2.46591983 -0.43175675  1.2749805 ]\n",
      "Gradient: [ 1.11861123e-06  1.27162582e-04 -4.83981314e-05]\n",
      "Loss value: 374.9784742798254\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1422\n",
      "Current point: [-2.46591984 -0.43175701  1.2749806 ]\n",
      "Gradient: [ 1.11078820e-06  1.26273084e-04 -4.80595891e-05]\n",
      "Loss value: 374.97847427978843\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1423\n",
      "Current point: [-2.46591984 -0.43175726  1.2749807 ]\n",
      "Gradient: [ 1.10301987e-06  1.25389809e-04 -4.77234149e-05]\n",
      "Loss value: 374.9784742797521\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1424\n",
      "Current point: [-2.46591984 -0.43175751  1.27498079]\n",
      "Gradient: [ 1.09530585e-06  1.24512712e-04 -4.73895922e-05]\n",
      "Loss value: 374.9784742797162\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1425\n",
      "Current point: [-2.46591984 -0.43175776  1.27498089]\n",
      "Gradient: [ 1.08764577e-06  1.23641750e-04 -4.70581046e-05]\n",
      "Loss value: 374.9784742796809\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1426\n",
      "Current point: [-2.46591985 -0.43175801  1.27498098]\n",
      "Gradient: [ 1.08003925e-06  1.22776881e-04 -4.67289357e-05]\n",
      "Loss value: 374.978474279646\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1427\n",
      "Current point: [-2.46591985 -0.43175825  1.27498108]\n",
      "Gradient: [ 1.07248593e-06  1.21918061e-04 -4.64020694e-05]\n",
      "Loss value: 374.97847427961153\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1428\n",
      "Current point: [-2.46591985 -0.4317585   1.27498117]\n",
      "Gradient: [ 1.06498540e-06  1.21065249e-04 -4.60774894e-05]\n",
      "Loss value: 374.9784742795776\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1429\n",
      "Current point: [-2.46591985 -0.43175874  1.27498126]\n",
      "Gradient: [ 1.05753731e-06  1.20218402e-04 -4.57551799e-05]\n",
      "Loss value: 374.97847427954423\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1430\n",
      "Current point: [-2.46591985 -0.43175898  1.27498135]\n",
      "Gradient: [ 1.05014130e-06  1.19377479e-04 -4.54351250e-05]\n",
      "Loss value: 374.97847427951126\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1431\n",
      "Current point: [-2.46591986 -0.43175922  1.27498144]\n",
      "Gradient: [ 1.04279702e-06  1.18542438e-04 -4.51173087e-05]\n",
      "Loss value: 374.97847427947875\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1432\n",
      "Current point: [-2.46591986 -0.43175946  1.27498153]\n",
      "Gradient: [ 1.03550411e-06  1.17713238e-04 -4.48017156e-05]\n",
      "Loss value: 374.97847427944663\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1433\n",
      "Current point: [-2.46591986 -0.43175969  1.27498162]\n",
      "Gradient: [ 1.02826217e-06  1.16889839e-04 -4.44883300e-05]\n",
      "Loss value: 374.978474279415\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1434\n",
      "Current point: [-2.46591986 -0.43175992  1.27498171]\n",
      "Gradient: [ 1.02107088e-06  1.16072199e-04 -4.41771366e-05]\n",
      "Loss value: 374.9784742793838\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1435\n",
      "Current point: [-2.46591986 -0.43176016  1.2749818 ]\n",
      "Gradient: [ 1.01392988e-06  1.15260279e-04 -4.38681199e-05]\n",
      "Loss value: 374.9784742793531\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1436\n",
      "Current point: [-2.46591987 -0.43176039  1.27498189]\n",
      "Gradient: [ 1.00683881e-06  1.14454037e-04 -4.35612648e-05]\n",
      "Loss value: 374.97847427932277\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1437\n",
      "Current point: [-2.46591987 -0.43176062  1.27498197]\n",
      "Gradient: [ 9.99797302e-07  1.13653436e-04 -4.32565561e-05]\n",
      "Loss value: 374.97847427929287\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1438\n",
      "Current point: [-2.46591987 -0.43176084  1.27498206]\n",
      "Gradient: [ 9.92805031e-07  1.12858435e-04 -4.29539789e-05]\n",
      "Loss value: 374.9784742792634\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1439\n",
      "Current point: [-2.46591987 -0.43176107  1.27498215]\n",
      "Gradient: [ 9.85861682e-07  1.12068994e-04 -4.26535181e-05]\n",
      "Loss value: 374.9784742792344\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1440\n",
      "Current point: [-2.46591987 -0.43176129  1.27498223]\n",
      "Gradient: [ 9.78966879e-07  1.11285076e-04 -4.23551591e-05]\n",
      "Loss value: 374.9784742792057\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1441\n",
      "Current point: [-2.46591988 -0.43176152  1.27498232]\n",
      "Gradient: [ 9.72120251e-07  1.10506641e-04 -4.20588871e-05]\n",
      "Loss value: 374.9784742791774\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1442\n",
      "Current point: [-2.46591988 -0.43176174  1.2749824 ]\n",
      "Gradient: [ 9.65321533e-07  1.09733652e-04 -4.17646874e-05]\n",
      "Loss value: 374.97847427914957\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1443\n",
      "Current point: [-2.46591988 -0.43176196  1.27498248]\n",
      "Gradient: [ 9.58570329e-07  1.08966069e-04 -4.14725457e-05]\n",
      "Loss value: 374.9784742791221\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1444\n",
      "Current point: [-2.46591988 -0.43176217  1.27498257]\n",
      "Gradient: [ 9.51866353e-07  1.08203856e-04 -4.11824475e-05]\n",
      "Loss value: 374.97847427909505\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1445\n",
      "Current point: [-2.46591988 -0.43176239  1.27498265]\n",
      "Gradient: [ 9.45209242e-07  1.07446974e-04 -4.08943786e-05]\n",
      "Loss value: 374.97847427906834\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1446\n",
      "Current point: [-2.46591989 -0.43176261  1.27498273]\n",
      "Gradient: [ 9.38598689e-07  1.06695387e-04 -4.06083246e-05]\n",
      "Loss value: 374.97847427904196\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1447\n",
      "Current point: [-2.46591989 -0.43176282  1.27498281]\n",
      "Gradient: [ 9.32034347e-07  1.05949057e-04 -4.03242716e-05]\n",
      "Loss value: 374.9784742790159\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1448\n",
      "Current point: [-2.46591989 -0.43176303  1.27498289]\n",
      "Gradient: [ 9.25515901e-07  1.05207948e-04 -4.00422055e-05]\n",
      "Loss value: 374.97847427899035\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1449\n",
      "Current point: [-2.46591989 -0.43176324  1.27498297]\n",
      "Gradient: [ 9.19043067e-07  1.04472022e-04 -3.97621125e-05]\n",
      "Loss value: 374.9784742789651\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1450\n",
      "Current point: [-2.46591989 -0.43176345  1.27498305]\n",
      "Gradient: [ 9.12615494e-07  1.03741245e-04 -3.94839787e-05]\n",
      "Loss value: 374.9784742789402\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1451\n",
      "Current point: [-2.46591989 -0.43176366  1.27498313]\n",
      "Gradient: [ 9.06232844e-07  1.03015579e-04 -3.92077904e-05]\n",
      "Loss value: 374.97847427891566\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1452\n",
      "Current point: [-2.4659199  -0.43176386  1.27498321]\n",
      "Gradient: [ 8.99894832e-07  1.02294989e-04 -3.89335340e-05]\n",
      "Loss value: 374.97847427889144\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1453\n",
      "Current point: [-2.4659199  -0.43176407  1.27498329]\n",
      "Gradient: [ 8.93601156e-07  1.01579440e-04 -3.86611961e-05]\n",
      "Loss value: 374.97847427886757\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1454\n",
      "Current point: [-2.4659199  -0.43176427  1.27498337]\n",
      "Gradient: [ 8.87351469e-07  1.00868896e-04 -3.83907631e-05]\n",
      "Loss value: 374.978474278844\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1455\n",
      "Current point: [-2.4659199  -0.43176447  1.27498344]\n",
      "Gradient: [ 8.81145486e-07  1.00163322e-04 -3.81222218e-05]\n",
      "Loss value: 374.97847427882084\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1456\n",
      "Current point: [-2.4659199  -0.43176467  1.27498352]\n",
      "Gradient: [ 8.74982931e-07  9.94626839e-05 -3.78555589e-05]\n",
      "Loss value: 374.9784742787979\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1457\n",
      "Current point: [-2.46591991 -0.43176487  1.27498359]\n",
      "Gradient: [ 8.68863432e-07  9.87669466e-05 -3.75907614e-05]\n",
      "Loss value: 374.97847427877537\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1458\n",
      "Current point: [-2.46591991 -0.43176507  1.27498367]\n",
      "Gradient: [ 8.62786743e-07  9.80760758e-05 -3.73278160e-05]\n",
      "Loss value: 374.978474278753\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1459\n",
      "Current point: [-2.46591991 -0.43176527  1.27498374]\n",
      "Gradient: [ 8.56752544e-07  9.73900377e-05 -3.70667100e-05]\n",
      "Loss value: 374.9784742787311\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1460\n",
      "Current point: [-2.46591991 -0.43176546  1.27498382]\n",
      "Gradient: [ 8.50760556e-07  9.67087985e-05 -3.68074303e-05]\n",
      "Loss value: 374.9784742787095\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1461\n",
      "Current point: [-2.46591991 -0.43176565  1.27498389]\n",
      "Gradient: [ 8.44810443e-07  9.60323244e-05 -3.65499644e-05]\n",
      "Loss value: 374.9784742786881\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1462\n",
      "Current point: [-2.46591991 -0.43176585  1.27498397]\n",
      "Gradient: [ 8.38901938e-07  9.53605823e-05 -3.62942994e-05]\n",
      "Loss value: 374.9784742786671\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1463\n",
      "Current point: [-2.46591992 -0.43176604  1.27498404]\n",
      "Gradient: [ 8.33034767e-07  9.46935390e-05 -3.60404227e-05]\n",
      "Loss value: 374.97847427864633\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1464\n",
      "Current point: [-2.46591992 -0.43176623  1.27498411]\n",
      "Gradient: [ 8.27208618e-07  9.40311617e-05 -3.57883219e-05]\n",
      "Loss value: 374.97847427862587\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1465\n",
      "Current point: [-2.46591992 -0.43176641  1.27498418]\n",
      "Gradient: [ 8.21423218e-07  9.33734177e-05 -3.55379846e-05]\n",
      "Loss value: 374.97847427860574\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1466\n",
      "Current point: [-2.46591992 -0.4317666   1.27498425]\n",
      "Gradient: [ 8.15678261e-07  9.27202745e-05 -3.52893983e-05]\n",
      "Loss value: 374.9784742785858\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1467\n",
      "Current point: [-2.46591992 -0.43176679  1.27498432]\n",
      "Gradient: [ 8.09973488e-07  9.20717001e-05 -3.50425509e-05]\n",
      "Loss value: 374.9784742785662\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1468\n",
      "Current point: [-2.46591992 -0.43176697  1.27498439]\n",
      "Gradient: [ 8.04308598e-07  9.14276624e-05 -3.47974301e-05]\n",
      "Loss value: 374.97847427854686\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1469\n",
      "Current point: [-2.46591993 -0.43176715  1.27498446]\n",
      "Gradient: [ 7.98683314e-07  9.07881298e-05 -3.45540240e-05]\n",
      "Loss value: 374.9784742785277\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1470\n",
      "Current point: [-2.46591993 -0.43176734  1.27498453]\n",
      "Gradient: [ 7.93097364e-07  9.01530707e-05 -3.43123205e-05]\n",
      "Loss value: 374.9784742785089\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1471\n",
      "Current point: [-2.46591993 -0.43176752  1.2749846 ]\n",
      "Gradient: [ 7.87550493e-07  8.95224537e-05 -3.40723077e-05]\n",
      "Loss value: 374.9784742784904\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1472\n",
      "Current point: [-2.46591993 -0.43176769  1.27498467]\n",
      "Gradient: [ 7.82042402e-07  8.88962480e-05 -3.38339738e-05]\n",
      "Loss value: 374.97847427847216\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1473\n",
      "Current point: [-2.46591993 -0.43176787  1.27498474]\n",
      "Gradient: [ 7.76572823e-07  8.82744225e-05 -3.35973070e-05]\n",
      "Loss value: 374.9784742784541\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1474\n",
      "Current point: [-2.46591993 -0.43176805  1.2749848 ]\n",
      "Gradient: [ 7.71141508e-07  8.76569467e-05 -3.33622956e-05]\n",
      "Loss value: 374.9784742784363\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1475\n",
      "Current point: [-2.46591994 -0.43176822  1.27498487]\n",
      "Gradient: [ 7.65748156e-07  8.70437901e-05 -3.31289282e-05]\n",
      "Loss value: 374.97847427841873\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1476\n",
      "Current point: [-2.46591994 -0.4317684   1.27498494]\n",
      "Gradient: [ 7.60392536e-07  8.64349225e-05 -3.28971931e-05]\n",
      "Loss value: 374.97847427840145\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1477\n",
      "Current point: [-2.46591994 -0.43176857  1.274985  ]\n",
      "Gradient: [ 7.55074365e-07  8.58303139e-05 -3.26670791e-05]\n",
      "Loss value: 374.9784742783844\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1478\n",
      "Current point: [-2.46591994 -0.43176874  1.27498507]\n",
      "Gradient: [ 7.49793376e-07  8.52299345e-05 -3.24385746e-05]\n",
      "Loss value: 374.9784742783676\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1479\n",
      "Current point: [-2.46591994 -0.43176891  1.27498513]\n",
      "Gradient: [ 7.44549331e-07  8.46337548e-05 -3.22116686e-05]\n",
      "Loss value: 374.9784742783511\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1480\n",
      "Current point: [-2.46591994 -0.43176908  1.2749852 ]\n",
      "Gradient: [ 7.39341962e-07  8.40417454e-05 -3.19863497e-05]\n",
      "Loss value: 374.9784742783347\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1481\n",
      "Current point: [-2.46591994 -0.43176925  1.27498526]\n",
      "Gradient: [ 7.34171009e-07  8.34538770e-05 -3.17626069e-05]\n",
      "Loss value: 374.9784742783186\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1482\n",
      "Current point: [-2.46591995 -0.43176942  1.27498532]\n",
      "Gradient: [ 7.29036213e-07  8.28701207e-05 -3.15404292e-05]\n",
      "Loss value: 374.97847427830266\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1483\n",
      "Current point: [-2.46591995 -0.43176958  1.27498539]\n",
      "Gradient: [ 7.23937321e-07  8.22904478e-05 -3.13198056e-05]\n",
      "Loss value: 374.97847427828697\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1484\n",
      "Current point: [-2.46591995 -0.43176975  1.27498545]\n",
      "Gradient: [ 7.18874062e-07  8.17148297e-05 -3.11007253e-05]\n",
      "Loss value: 374.9784742782715\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1485\n",
      "Current point: [-2.46591995 -0.43176991  1.27498551]\n",
      "Gradient: [ 7.13846225e-07  8.11432380e-05 -3.08831774e-05]\n",
      "Loss value: 374.97847427825633\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1486\n",
      "Current point: [-2.46591995 -0.43177007  1.27498557]\n",
      "Gradient: [ 7.08853540e-07  8.05756446e-05 -3.06671513e-05]\n",
      "Loss value: 374.97847427824126\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1487\n",
      "Current point: [-2.46591995 -0.43177023  1.27498564]\n",
      "Gradient: [ 7.03895786e-07  8.00120215e-05 -3.04526362e-05]\n",
      "Loss value: 374.9784742782265\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1488\n",
      "Current point: [-2.46591995 -0.43177039  1.2749857 ]\n",
      "Gradient: [ 6.98972702e-07  7.94523409e-05 -3.02396217e-05]\n",
      "Loss value: 374.9784742782119\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1489\n",
      "Current point: [-2.46591996 -0.43177055  1.27498576]\n",
      "Gradient: [ 6.94084065e-07  7.88965752e-05 -3.00280971e-05]\n",
      "Loss value: 374.9784742781975\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1490\n",
      "Current point: [-2.46591996 -0.43177071  1.27498582]\n",
      "Gradient: [ 6.89229589e-07  7.83446971e-05 -2.98180522e-05]\n",
      "Loss value: 374.9784742781832\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1491\n",
      "Current point: [-2.46591996 -0.43177087  1.27498588]\n",
      "Gradient: [ 6.84409080e-07  7.77966794e-05 -2.96094765e-05]\n",
      "Loss value: 374.97847427816924\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1492\n",
      "Current point: [-2.46591996 -0.43177102  1.27498594]\n",
      "Gradient: [ 6.79622282e-07  7.72524951e-05 -2.94023599e-05]\n",
      "Loss value: 374.9784742781555\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1493\n",
      "Current point: [-2.46591996 -0.43177118  1.27498599]\n",
      "Gradient: [ 6.74868931e-07  7.67121173e-05 -2.91966920e-05]\n",
      "Loss value: 374.9784742781418\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1494\n",
      "Current point: [-2.46591996 -0.43177133  1.27498605]\n",
      "Gradient: [ 6.70148852e-07  7.61755194e-05 -2.89924627e-05]\n",
      "Loss value: 374.9784742781284\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1495\n",
      "Current point: [-2.46591996 -0.43177148  1.27498611]\n",
      "Gradient: [ 6.65461763e-07  7.56426750e-05 -2.87896620e-05]\n",
      "Loss value: 374.9784742781152\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1496\n",
      "Current point: [-2.46591997 -0.43177164  1.27498617]\n",
      "Gradient: [ 6.60807463e-07  7.51135579e-05 -2.85882799e-05]\n",
      "Loss value: 374.9784742781021\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1497\n",
      "Current point: [-2.46591997 -0.43177179  1.27498623]\n",
      "Gradient: [ 6.56185707e-07  7.45881418e-05 -2.83883064e-05]\n",
      "Loss value: 374.97847427808927\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1498\n",
      "Current point: [-2.46591997 -0.43177193  1.27498628]\n",
      "Gradient: [ 6.51596256e-07  7.40664011e-05 -2.81897318e-05]\n",
      "Loss value: 374.97847427807653\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n",
      "Iteration 1499\n",
      "Current point: [-2.46591997 -0.43177208  1.27498634]\n",
      "Gradient: [ 6.47038905e-07  7.35483099e-05 -2.79925461e-05]\n",
      "Loss value: 374.978474278064\n",
      "Accuracy: 0.7801120448179272\n",
      "====================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2.46591997, -0.43177223,  1.27498639])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent_process(starting_point, learning_rate, iterations):\n",
    "\n",
    "    # clear the graphs folder\n",
    "    # import os\n",
    "    # import shutil\n",
    "    # shutil.rmtree('./graphs')\n",
    "    # os.mkdir('./graphs')\n",
    "\n",
    "    x = np.linspace(-3, 0.3, 100)\n",
    "    y = np.linspace(-0.1, 2, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "    cur_point = starting_point\n",
    "    history = []\n",
    "    for i in range(iterations):\n",
    "        plt.clf()\n",
    "        Z = [[func.value(x, y, cur_point[2]) for x in x] for y in y]\n",
    "        plt.contourf(X, Y, Z, 20, cmap = 'RdGy')\n",
    "        plt.title('z = {z}'.format(z = cur_point[2]))\n",
    "        plt.colorbar()\n",
    "        \n",
    "        gradient = func.derivative(cur_point[0], cur_point[1], cur_point[2])\n",
    "        print(\"Iteration {id}\".format(id = i))\n",
    "        print(\"Current point: {point}\".format(point = cur_point))\n",
    "        print(\"Gradient: {grad}\".format(grad = gradient))\n",
    "        print(\"Loss value: {loss}\".format(loss = func.value(cur_point[0], cur_point[1], cur_point[2])))\n",
    "        print(\"Accuracy: {acc}\".format(acc = func.accuracy(cur_point[0], cur_point[1], cur_point[2])))\n",
    "        print(\"====================================\")\n",
    "        next_point = cur_point - gradient * learning_rate\n",
    "        \n",
    "        # plot the history of the points as gray, the current point as red\n",
    "        history.append(cur_point)\n",
    "        for point in history:\n",
    "            plt.scatter(point[0], point[1], c = 'grey')\n",
    "        plt.scatter(next_point[0], next_point[1], c = 'red')\n",
    "        #plot the line between points in history\n",
    "        for i in range(len(history) - 1):\n",
    "            plt.plot([history[i][0], history[i + 1][0]], [history[i][1], history[i + 1][1]], c = 'yellow')\n",
    "        plt.plot([cur_point[0], next_point[0]], [cur_point[1], next_point[1]], c = 'yellow')\n",
    "        \n",
    "        plt.savefig('./graphs/iteration_{id}.png'.format(id = i))\n",
    "\n",
    "        cur_point = next_point\n",
    "\n",
    "        if (np.linalg.norm(gradient) < 0.00001):\n",
    "            break\n",
    "\n",
    "    return cur_point\n",
    "\n",
    "starting_point = np.array([-0.5, 0.00, -0.1])\n",
    "learning_rate = 0.002\n",
    "iterations = 1500 # epochs\n",
    "gradient_descent_process(starting_point, learning_rate, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_3200\\1080423327.py:7: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images.append(imageio.imread('./graphs/' + 'iteration_{id}.png'.format(id = i)))\n"
     ]
    }
   ],
   "source": [
    "# save all images in ./graphs as a gif\n",
    "import imageio\n",
    "import os\n",
    "images = []\n",
    "for i in range(iterations):\n",
    "    try:\n",
    "        images.append(imageio.imread('./graphs/' + 'iteration_{id}.png'.format(id = i)))\n",
    "    except:\n",
    "        pass\n",
    "imageio.mimsave('./titanic-normalized.gif', images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace pclass column with random number from 1 to 10\n",
    "pclass = np.random.randint(1, 11, size = len(data['Pclass']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import LossFunction\n",
    "    \n",
    "func = LossFunction(sex, age, survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there\n",
      "How are you?\n"
     ]
    }
   ],
   "source": [
    "from hello import hi\n",
    "hi()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
