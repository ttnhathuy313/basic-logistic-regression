{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = {\n",
    "    'test': 'https://transfer.sh/1fbFgD7w4F/mnist_test.csv',\n",
    "    'train': 'https://transfer.sh/OfL7x1GAjx/mnist_train.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "if (not os.path.exists('./data/mnist_train.csv')):\n",
    "    os.makedirs('./data', exist_ok=True)\n",
    "    for key in data_url:\n",
    "        print('Downloading', key, 'data...')\n",
    "        subprocess.call(['curl', data_url[key], '-o', './data/mnist_' + key + '.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/mnist_train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 10 model, model i answer whether the picture is of the i-th digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "class LossFunction:\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def loss(self, a : np.ndarray):\n",
    "        prob = sigmoid(np.dot(self.X, a))\n",
    "        values = - self.y * np.log(prob) - (1 - self.y) * np.log(1 - prob)\n",
    "        return np.nansum(values) / self.y.shape[0]\n",
    "\n",
    "    def gradient(self, a : np.ndarray):\n",
    "        prob = sigmoid(np.dot(self.X, a))\n",
    "        sub_coefficient = -(self.y - prob) \n",
    "        return np.dot(self.X.T, sub_coefficient) / self.y.shape[0]\n",
    "    \n",
    "    def precision(self, a : np.ndarray):\n",
    "        prob = sigmoid(np.dot(self.X, a))\n",
    "        prob = np.array(prob >= 0.5, dtype=np.int32)\n",
    "        return np.sum(prob == self.y) / self.y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 785), (60000,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.drop('label', axis=1).values\n",
    "# X: shape m x 784\n",
    "#append a column of 1s\n",
    "X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "y = data['label'].values\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_functions = []\n",
    "for digit in range(10):\n",
    "    y_digit = np.array(y == digit, dtype=np.int32)\n",
    "    loss_functions.append(LossFunction(X, y_digit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.LossFunction at 0x1675f3d5bd0>,\n",
       " <__main__.LossFunction at 0x1675f3d7e80>,\n",
       " <__main__.LossFunction at 0x1675f3d4df0>,\n",
       " <__main__.LossFunction at 0x1675f3d5f00>,\n",
       " <__main__.LossFunction at 0x1675f3d5c60>,\n",
       " <__main__.LossFunction at 0x1675f3d7040>,\n",
       " <__main__.LossFunction at 0x1675f3d7af0>,\n",
       " <__main__.LossFunction at 0x1675f3d7a30>,\n",
       " <__main__.LossFunction at 0x1675f3d7a90>,\n",
       " <__main__.LossFunction at 0x1675f3d4f10>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(loss_func, starting_point, learning_rate = 0.00001, num_steps = 40, precision=0.00001):\n",
    "    cur_point = starting_point\n",
    "    for i in range(num_steps):\n",
    "        grad = loss_func.gradient(cur_point)\n",
    "        print(\"Iteration {}: loss = {}, precision = {}\".format(i, loss_func.loss(cur_point), loss_func.precision(cur_point)))\n",
    "        cur_point = cur_point - learning_rate * grad\n",
    "        if np.linalg.norm(grad) < precision:\n",
    "            break\n",
    "    return cur_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for digit 0: 0.9843333333333333\n",
      "Accuracy for digit 1: 0.98625\n",
      "Accuracy for digit 2: 0.9673333333333334\n",
      "Accuracy for digit 3: 0.96225\n",
      "Accuracy for digit 4: 0.9729333333333333\n",
      "Accuracy for digit 5: 0.9528166666666666\n",
      "Accuracy for digit 6: 0.9795666666666667\n",
      "Accuracy for digit 7: 0.9779166666666667\n",
      "Accuracy for digit 8: 0.9358166666666666\n",
      "Accuracy for digit 9: 0.9471333333333334\n"
     ]
    }
   ],
   "source": [
    "optimal_points = []\n",
    "\n",
    "for digit in range(10):\n",
    "    optimal_point = gradient_descent(loss_functions[digit], np.zeros(X.shape[1]))\n",
    "    print(\"Accuracy for digit {}: {}\".format(digit, loss_functions[digit].precision(optimal_point)))\n",
    "    optimal_points.append(optimal_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(X, optimal_points):\n",
    "    prob = np.zeros((X.shape[0], 10))\n",
    "    for digit in range(10):\n",
    "        prob[:, digit] = sigmoid(np.dot(X, optimal_points[digit]))\n",
    "    return np.argmax(prob, axis=1), prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_digit(data, row_id):\n",
    "    row = data.iloc[row_id]\n",
    "    label = row['label']\n",
    "    image = row.drop('label').values.reshape(28, 28)\n",
    "    plt.title('Digit Label = {}'.format(label))\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('data/mnist_test.csv')\n",
    "X_test = data_test.drop('label', axis=1).values\n",
    "X_test = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis=1)\n",
    "y_test = data_test['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_digit(data_test, 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5], dtype=int64),\n",
       " array([[0.01650046, 0.01635738, 0.00897801, 0.08814713, 0.04182052,\n",
       "         0.53346969, 0.03824936, 0.00192412, 0.50665186, 0.02340841]]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(np.array([X_test[2003]]), optimal_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.01293639, 0.01282422, 0.00703878, 0.06910753, 0.03278737,\n",
       "        0.4182413 , 0.02998758, 0.00150851, 0.39721607, 0.01835224]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, prob = infer(np.array([X_test[2003]]), optimal_points)\n",
    "# calculate sum of probabilities\n",
    "sum = np.sum(prob)\n",
    "new_prob = prob / sum\n",
    "print(np.sum(new_prob))\n",
    "new_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0: 0.01650046\n",
    "1: 0.01635738\n",
    "2: 0.00897801\n",
    "3: 0.08814713 (0, 1)\n",
    "4: 0.04182052 / 1.27 (0, 1)\n",
    "5: 0.53346969 / 1.27 (0 ,1)\n",
    "6: 0.03824936\n",
    "7: 0.00192412\n",
    "8: 0.50665186\n",
    "9: 0.02340841\n",
    "\n",
    "tá»•ng: \n",
    "\n",
    "P(i) = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\n",
    "\n",
    "(Omega, P, Sigma_Algebra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8781"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(X, y, optimal_points):\n",
    "    y_pred = infer(X, optimal_points)\n",
    "    return np.sum(y_pred == y) / y.shape[0]\n",
    "evaluate(X_test, y_test, optimal_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bad behaviors:\n",
    "\n",
    "- Insignificant probability: 66, 200, 234: all prob values are too small -> the model is not sure of anything\n",
    "\n",
    "- Indecisive: 2003, 2001: Having close probability between categories, even when the result is correct. \n",
    "\n",
    "- Scale of probability: two categories having prob value > 0.5 but the wrong one got higher prob."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper reasons\n",
    "\n",
    "- Lack of communication between the models. -> bad scaling of prob values\n",
    "- Loss function has not take into account all the prob values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New sample space\n",
    "\n",
    "- We want to: merge 10 distributions into one\n",
    "- Sample space = {0, 1, 2, 3..., 9}\n",
    "- Sum of prob = 1\n",
    "- Rankings must remain\n",
    "\n",
    "-> Just divide by sum of probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785) (60000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22.912636254759526"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    # Implement the stable version of softmax\n",
    "    return np.exp(z - np.max(z, axis=1, keepdims=True)) / np.sum(np.exp(z - np.max(z, axis=1, keepdims=True)), axis=1, keepdims=True)\n",
    "\n",
    "class LossFunction:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = np.zeros((y.shape[0], 10))\n",
    "        self.y[np.arange(y.shape[0]), y] = 1\n",
    "        print(self.X.shape, self.y.shape)\n",
    "\n",
    "    def loss(self, W : np.ndarray):\n",
    "        # Calculate the loss function\n",
    "        # W: shape 785 x 10\n",
    "        prob = softmax(np.dot(self.X, W))\n",
    "        #avoid inf\n",
    "        prob = np.clip(prob, 1e-15, 1 - 1e-15)\n",
    "        values = - np.nansum(self.y * np.log(prob))\n",
    "        return values / self.X.shape[0]\n",
    "    \n",
    "    def gradient(self, W : np.ndarray):\n",
    "        # Calculate the gradient\n",
    "        prob = softmax(np.dot(self.X, W))\n",
    "        difference = -(self.y - prob) \n",
    "        return np.dot(self.X.T, difference) / self.X.shape[0]\n",
    "    \n",
    "    def precision(self, W : np.ndarray):\n",
    "        prob = softmax(np.dot(self.X, W))\n",
    "        prob = np.argmax(prob, axis=1)\n",
    "        return np.sum(prob == y) / y.shape[0]\n",
    "loss_func = LossFunction(X, y)\n",
    "# Randomly initialize W, each has real value from normal distribution between -0.01 and 0.01\n",
    "W = np.random.normal(0, 0.01, (X.shape[1], 10))\n",
    "loss_func.loss(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent with line search Wolfe Condition to find the suitable learning rate\n",
    "def gradient_descent(loss_func, starting_point, num_steps = 40, precision=0.00001):\n",
    "    cur_point = starting_point\n",
    "    for i in range(num_steps):\n",
    "        initial_learning_rate = 0.001\n",
    "        grad = loss_func.gradient(cur_point)\n",
    "        while (True):\n",
    "            # Check the Wolfe condition\n",
    "            if (loss_func.loss(cur_point - initial_learning_rate * grad) <= loss_func.loss(cur_point) - 0.0001 * initial_learning_rate * np.sum(grad ** 2)):\n",
    "                break\n",
    "            initial_learning_rate = initial_learning_rate * 0.8\n",
    "        cur_point = cur_point - initial_learning_rate * grad\n",
    "        print(\"Iteration {}: loss = {}, precision = {}\".format(i, loss_func.loss(cur_point), loss_func.precision(cur_point)))\n",
    "    return cur_point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer\n",
    "def adam(loss_func, starting_point, num_steps = 40, precision=0.00001):\n",
    "    cur_point = starting_point\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "    m = np.zeros(cur_point.shape)\n",
    "    v = np.zeros(cur_point.shape)\n",
    "    for i in range(num_steps):\n",
    "        grad = loss_func.gradient(cur_point)\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "        m_hat = m / (1 - beta1 ** (i + 1))\n",
    "        v_hat = v / (1 - beta2 ** (i + 1))\n",
    "        cur_point = cur_point - 0.006 * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "        print(\"Iteration {}: loss = {}, precision = {}\".format(i, loss_func.loss(cur_point), loss_func.precision(cur_point)))\n",
    "    return cur_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = gradient_descent(loss_func, W, num_steps=100, precision=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss = 21.344616428454334, precision = 0.2153\n",
      "Iteration 1: loss = 19.664420637135343, precision = 0.35615\n",
      "Iteration 2: loss = 21.46253068367141, precision = 0.3226333333333333\n",
      "Iteration 3: loss = 16.46540327076622, precision = 0.44433333333333336\n",
      "Iteration 4: loss = 13.671973397877416, precision = 0.5257\n",
      "Iteration 5: loss = 10.446583973608686, precision = 0.6412666666666667\n",
      "Iteration 6: loss = 10.385393499014597, precision = 0.6318\n",
      "Iteration 7: loss = 9.952769649583145, precision = 0.6385833333333333\n",
      "Iteration 8: loss = 8.707673246200532, precision = 0.68045\n",
      "Iteration 9: loss = 7.6557457792337935, precision = 0.7225666666666667\n",
      "Iteration 10: loss = 6.670371319896059, precision = 0.7581166666666667\n",
      "Iteration 11: loss = 5.559248651415558, precision = 0.7953\n",
      "Iteration 12: loss = 4.700359751314763, precision = 0.8252666666666667\n",
      "Iteration 13: loss = 4.371973527625345, precision = 0.83765\n",
      "Iteration 14: loss = 4.330248815439994, precision = 0.8383666666666667\n",
      "Iteration 15: loss = 4.29116008151474, precision = 0.83905\n",
      "Iteration 16: loss = 4.287527615741816, precision = 0.8412166666666666\n",
      "Iteration 17: loss = 4.375515734873008, precision = 0.8401666666666666\n",
      "Iteration 18: loss = 4.404182463418974, precision = 0.8413666666666667\n",
      "Iteration 19: loss = 4.305607696522166, precision = 0.8446166666666667\n",
      "Iteration 20: loss = 4.217550968266281, precision = 0.8482\n",
      "Iteration 21: loss = 4.145628305822595, precision = 0.8500833333333333\n",
      "Iteration 22: loss = 4.035676802363263, precision = 0.8548\n",
      "Iteration 23: loss = 3.8684749192173924, precision = 0.8607333333333334\n",
      "Iteration 24: loss = 3.6793133438742545, precision = 0.8659166666666667\n",
      "Iteration 25: loss = 3.494405691204342, precision = 0.8709166666666667\n",
      "Iteration 26: loss = 3.3250606852501003, precision = 0.87735\n",
      "Iteration 27: loss = 3.1721754447757644, precision = 0.88235\n",
      "Iteration 28: loss = 3.0547835010554203, precision = 0.8858166666666667\n",
      "Iteration 29: loss = 2.966412512797983, precision = 0.8887333333333334\n",
      "Iteration 30: loss = 2.9003807972371893, precision = 0.8907\n",
      "Iteration 31: loss = 2.856130986086439, precision = 0.8918333333333334\n",
      "Iteration 32: loss = 2.8307397362252873, precision = 0.8917666666666667\n",
      "Iteration 33: loss = 2.811940011605976, precision = 0.89225\n",
      "Iteration 34: loss = 2.78991316726387, precision = 0.8927\n",
      "Iteration 35: loss = 2.752895633575918, precision = 0.8932166666666667\n",
      "Iteration 36: loss = 2.7080292680762312, precision = 0.89505\n",
      "Iteration 37: loss = 2.670325503703144, precision = 0.8960166666666667\n",
      "Iteration 38: loss = 2.644927706209637, precision = 0.8969333333333334\n",
      "Iteration 39: loss = 2.6241146892456815, precision = 0.8970833333333333\n",
      "Iteration 40: loss = 2.586869701920446, precision = 0.8986666666666666\n",
      "Iteration 41: loss = 2.535347222784116, precision = 0.89975\n",
      "Iteration 42: loss = 2.4916763982658554, precision = 0.9003333333333333\n",
      "Iteration 43: loss = 2.463809712552938, precision = 0.90075\n",
      "Iteration 44: loss = 2.4248987982406245, precision = 0.9016833333333333\n",
      "Iteration 45: loss = 2.368530202308442, precision = 0.9036\n",
      "Iteration 46: loss = 2.322056685394703, precision = 0.90525\n",
      "Iteration 47: loss = 2.299781921902217, precision = 0.9061333333333333\n",
      "Iteration 48: loss = 2.2770683322019596, precision = 0.9064166666666666\n",
      "Iteration 49: loss = 2.243854086894404, precision = 0.9072666666666667\n",
      "Iteration 50: loss = 2.2188040119344143, precision = 0.9080833333333334\n",
      "Iteration 51: loss = 2.206321385601851, precision = 0.9081833333333333\n",
      "Iteration 52: loss = 2.187869352335001, precision = 0.9086833333333333\n",
      "Iteration 53: loss = 2.1584056743354676, precision = 0.9098333333333334\n",
      "Iteration 54: loss = 2.1311766590441903, precision = 0.9107\n",
      "Iteration 55: loss = 2.110826035972237, precision = 0.9112666666666667\n",
      "Iteration 56: loss = 2.0886659486701444, precision = 0.9117166666666666\n",
      "Iteration 57: loss = 2.0667729139718576, precision = 0.9119333333333334\n",
      "Iteration 58: loss = 2.049549842218254, precision = 0.9118333333333334\n",
      "Iteration 59: loss = 2.0336969454560734, precision = 0.9118333333333334\n",
      "Iteration 60: loss = 2.0148336111294873, precision = 0.9126833333333333\n",
      "Iteration 61: loss = 1.996305465295727, precision = 0.9129833333333334\n",
      "Iteration 62: loss = 1.9767097484910578, precision = 0.9136666666666666\n",
      "Iteration 63: loss = 1.9550605456306482, precision = 0.9142333333333333\n",
      "Iteration 64: loss = 1.9341899080252734, precision = 0.9149833333333334\n",
      "Iteration 65: loss = 1.9111263534231364, precision = 0.91515\n",
      "Iteration 66: loss = 1.8884266900653106, precision = 0.9157666666666666\n",
      "Iteration 67: loss = 1.8708760143888026, precision = 0.91595\n",
      "Iteration 68: loss = 1.8539658466490416, precision = 0.9163666666666667\n",
      "Iteration 69: loss = 1.8354165044726296, precision = 0.9171833333333334\n",
      "Iteration 70: loss = 1.8196450908649067, precision = 0.9172333333333333\n",
      "Iteration 71: loss = 1.8023027667939855, precision = 0.9175833333333333\n",
      "Iteration 72: loss = 1.7817667342795744, precision = 0.9182666666666667\n",
      "Iteration 73: loss = 1.7641422103220887, precision = 0.9179166666666667\n",
      "Iteration 74: loss = 1.74669491039105, precision = 0.9178\n",
      "Iteration 75: loss = 1.728706791844967, precision = 0.9179666666666667\n",
      "Iteration 76: loss = 1.713368005275171, precision = 0.9184\n",
      "Iteration 77: loss = 1.6971142486353175, precision = 0.91835\n",
      "Iteration 78: loss = 1.680165785875929, precision = 0.91835\n",
      "Iteration 79: loss = 1.6636387019046355, precision = 0.9186833333333333\n",
      "Iteration 80: loss = 1.6471727965118212, precision = 0.9191\n",
      "Iteration 81: loss = 1.6321769783373214, precision = 0.9194333333333333\n",
      "Iteration 82: loss = 1.6178231612118839, precision = 0.9195333333333333\n",
      "Iteration 83: loss = 1.6031201876566323, precision = 0.9195833333333333\n",
      "Iteration 84: loss = 1.5863635855957858, precision = 0.9197833333333333\n",
      "Iteration 85: loss = 1.5691274035123488, precision = 0.91955\n",
      "Iteration 86: loss = 1.5535585682697972, precision = 0.9196166666666666\n",
      "Iteration 87: loss = 1.5391562856145544, precision = 0.9200166666666667\n",
      "Iteration 88: loss = 1.5270375603401278, precision = 0.9200166666666667\n",
      "Iteration 89: loss = 1.513762272163672, precision = 0.9201\n",
      "Iteration 90: loss = 1.5006132495103186, precision = 0.9198\n",
      "Iteration 91: loss = 1.4876269427923474, precision = 0.9202\n",
      "Iteration 92: loss = 1.4746946984967642, precision = 0.9205166666666666\n",
      "Iteration 93: loss = 1.461859664225406, precision = 0.9206166666666666\n",
      "Iteration 94: loss = 1.448727705390086, precision = 0.92075\n",
      "Iteration 95: loss = 1.43596442772023, precision = 0.92055\n",
      "Iteration 96: loss = 1.4238509897632081, precision = 0.9205833333333333\n",
      "Iteration 97: loss = 1.4119224594369018, precision = 0.9207333333333333\n",
      "Iteration 98: loss = 1.4002049700700863, precision = 0.9206166666666666\n",
      "Iteration 99: loss = 1.3887028056304285, precision = 0.9206166666666666\n",
      "Iteration 100: loss = 1.3776916519460118, precision = 0.9206\n",
      "Iteration 101: loss = 1.3665548083407908, precision = 0.9207833333333333\n",
      "Iteration 102: loss = 1.3554711721848853, precision = 0.9205\n",
      "Iteration 103: loss = 1.3441912759192511, precision = 0.9207666666666666\n",
      "Iteration 104: loss = 1.332817926491204, precision = 0.9209\n",
      "Iteration 105: loss = 1.3216904123210664, precision = 0.9211166666666667\n",
      "Iteration 106: loss = 1.3111859877628347, precision = 0.9211333333333334\n",
      "Iteration 107: loss = 1.3010127758730348, precision = 0.9209166666666667\n",
      "Iteration 108: loss = 1.2905525510814522, precision = 0.9210666666666667\n",
      "Iteration 109: loss = 1.2803589228660102, precision = 0.9212\n",
      "Iteration 110: loss = 1.2701483521011567, precision = 0.92145\n",
      "Iteration 111: loss = 1.2593917297825257, precision = 0.9215333333333333\n",
      "Iteration 112: loss = 1.2486928224347067, precision = 0.9214666666666667\n",
      "Iteration 113: loss = 1.238280704399472, precision = 0.9216833333333333\n",
      "Iteration 114: loss = 1.227695196213323, precision = 0.92185\n",
      "Iteration 115: loss = 1.217281255864413, precision = 0.9216666666666666\n",
      "Iteration 116: loss = 1.2074380115823706, precision = 0.92185\n",
      "Iteration 117: loss = 1.1978016099933642, precision = 0.9215833333333333\n",
      "Iteration 118: loss = 1.1880976684888966, precision = 0.9219\n",
      "Iteration 119: loss = 1.1786190222791337, precision = 0.9220333333333334\n",
      "Iteration 120: loss = 1.169397864861834, precision = 0.9218166666666666\n",
      "Iteration 121: loss = 1.1601002965897864, precision = 0.9217166666666666\n",
      "Iteration 122: loss = 1.1506138469469858, precision = 0.9214\n",
      "Iteration 123: loss = 1.1412586400881635, precision = 0.9215666666666666\n",
      "Iteration 124: loss = 1.1317430680340637, precision = 0.92135\n",
      "Iteration 125: loss = 1.1223661955018385, precision = 0.9212666666666667\n",
      "Iteration 126: loss = 1.1132538432262538, precision = 0.9211833333333334\n",
      "Iteration 127: loss = 1.1042586335781446, precision = 0.9211\n",
      "Iteration 128: loss = 1.0958551868154418, precision = 0.9213166666666667\n",
      "Iteration 129: loss = 1.0875553623578071, precision = 0.9212\n",
      "Iteration 130: loss = 1.0795148918509994, precision = 0.9215666666666666\n",
      "Iteration 131: loss = 1.0712818029759288, precision = 0.9217666666666666\n",
      "Iteration 132: loss = 1.06341389873152, precision = 0.9219333333333334\n",
      "Iteration 133: loss = 1.0552437197946836, precision = 0.9221833333333334\n",
      "Iteration 134: loss = 1.0489368679459483, precision = 0.9221833333333334\n",
      "Iteration 135: loss = 1.0435499064121456, precision = 0.9215\n",
      "Iteration 136: loss = 1.0453097506083548, precision = 0.92085\n",
      "Iteration 137: loss = 1.0525567878275626, precision = 0.9180833333333334\n",
      "Iteration 138: loss = 1.085279889867288, precision = 0.9125833333333333\n",
      "Iteration 139: loss = 1.1570550975207872, precision = 0.9060333333333334\n",
      "Iteration 140: loss = 1.2708549142284788, precision = 0.891\n",
      "Iteration 141: loss = 1.5424174039947725, precision = 0.8776666666666667\n",
      "Iteration 142: loss = 1.9768283663361697, precision = 0.85775\n",
      "Iteration 143: loss = 2.2011227163348592, precision = 0.8618166666666667\n",
      "Iteration 144: loss = 2.72657028463922, precision = 0.8410833333333333\n",
      "Iteration 145: loss = 3.737449187523312, precision = 0.8309333333333333\n",
      "Iteration 146: loss = 3.023870452962493, precision = 0.8554166666666667\n",
      "Iteration 147: loss = 1.7056667028187869, precision = 0.9000166666666667\n",
      "Iteration 148: loss = 3.099912482056905, precision = 0.8442333333333333\n",
      "Iteration 149: loss = 1.639546836596805, precision = 0.90335\n",
      "Iteration 150: loss = 2.267997368826599, precision = 0.87875\n",
      "Iteration 151: loss = 2.374921479100944, precision = 0.8730833333333333\n",
      "Iteration 152: loss = 1.8945995712900265, precision = 0.8943166666666666\n",
      "Iteration 153: loss = 1.9330135104611448, precision = 0.8932166666666667\n",
      "Iteration 154: loss = 1.9824708230380994, precision = 0.8960833333333333\n",
      "Iteration 155: loss = 1.79810741953136, precision = 0.9023666666666667\n",
      "Iteration 156: loss = 1.762216416840477, precision = 0.90535\n",
      "Iteration 157: loss = 1.9599927770140537, precision = 0.8911\n",
      "Iteration 158: loss = 1.6760703679750002, precision = 0.90795\n",
      "Iteration 159: loss = 1.5211210677509686, precision = 0.9164166666666667\n",
      "Iteration 160: loss = 1.741538340547337, precision = 0.9003166666666667\n",
      "Iteration 161: loss = 1.6514102647931026, precision = 0.9114\n",
      "Iteration 162: loss = 1.4533290208854321, precision = 0.9202666666666667\n",
      "Iteration 163: loss = 1.4482438670484994, precision = 0.9175833333333333\n",
      "Iteration 164: loss = 1.5781325876509014, precision = 0.90935\n",
      "Iteration 165: loss = 1.4585546548107595, precision = 0.9145\n",
      "Iteration 166: loss = 1.3504727932479896, precision = 0.92255\n",
      "Iteration 167: loss = 1.4133707073728752, precision = 0.9161\n",
      "Iteration 168: loss = 1.3713999129891639, precision = 0.9181\n",
      "Iteration 169: loss = 1.3226064620181994, precision = 0.91985\n",
      "Iteration 170: loss = 1.3289803888508984, precision = 0.9189333333333334\n",
      "Iteration 171: loss = 1.2590837352462088, precision = 0.9204666666666667\n",
      "Iteration 172: loss = 1.2199895138293704, precision = 0.9237833333333333\n",
      "Iteration 173: loss = 1.2890310988750613, precision = 0.9183666666666667\n",
      "Iteration 174: loss = 1.1981940490661982, precision = 0.9195166666666666\n",
      "Iteration 175: loss = 1.1581249140923955, precision = 0.9244833333333333\n",
      "Iteration 176: loss = 1.1666491989149104, precision = 0.9226166666666666\n",
      "Iteration 177: loss = 1.1408395018765876, precision = 0.9225166666666667\n",
      "Iteration 178: loss = 1.1535984241063624, precision = 0.9169166666666667\n",
      "Iteration 179: loss = 1.131294233280943, precision = 0.9179833333333334\n",
      "Iteration 180: loss = 1.0854361273344035, precision = 0.9222166666666667\n",
      "Iteration 181: loss = 1.0707015856753097, precision = 0.92395\n",
      "Iteration 182: loss = 1.0294904375878957, precision = 0.9260333333333334\n",
      "Iteration 183: loss = 1.0482870759072431, precision = 0.9214\n",
      "Iteration 184: loss = 1.041154044437618, precision = 0.9225166666666667\n",
      "Iteration 185: loss = 0.9907606292367603, precision = 0.9251166666666667\n",
      "Iteration 186: loss = 0.9683343537379175, precision = 0.9268\n",
      "Iteration 187: loss = 0.9511445597528668, precision = 0.9276666666666666\n",
      "Iteration 188: loss = 0.9629566428045018, precision = 0.92375\n",
      "Iteration 189: loss = 0.9355488147484352, precision = 0.9247\n",
      "Iteration 190: loss = 0.9700832620589517, precision = 0.9167833333333333\n",
      "Iteration 191: loss = 1.1222713762478578, precision = 0.8962\n",
      "Iteration 192: loss = 2.4292052568006484, precision = 0.8428666666666667\n",
      "Iteration 193: loss = 4.354916822479824, precision = 0.7682666666666667\n",
      "Iteration 194: loss = 4.280250844236876, precision = 0.814\n",
      "Iteration 195: loss = 3.206523068425794, precision = 0.8291166666666666\n",
      "Iteration 196: loss = 3.464107076416196, precision = 0.8128833333333333\n",
      "Iteration 197: loss = 2.752276001352631, precision = 0.8472\n",
      "Iteration 198: loss = 2.1655385507925193, precision = 0.8745833333333334\n",
      "Iteration 199: loss = 2.9125356003186615, precision = 0.8491166666666666\n"
     ]
    }
   ],
   "source": [
    "W = adam(loss_func, W, num_steps=200, precision=0.00001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
