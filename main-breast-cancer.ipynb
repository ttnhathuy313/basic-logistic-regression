{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on `main-titanic-vectorized.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 31), (569,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('./data/data.csv')\n",
    "data = data.dropna(axis=1)\n",
    "n_columns = len(data.columns)\n",
    "\n",
    "# Normalize data\n",
    "for col_idx in range(2, n_columns):\n",
    "    data.iloc[:, col_idx] = (data.iloc[:, col_idx] - data.iloc[:, col_idx].min()) / (data.iloc[:, col_idx].max() - data.iloc[:, col_idx].min())\n",
    "\n",
    "X = data.iloc[:, 2:n_columns]\n",
    "y = data['diagnosis']\n",
    "X = np.array(X)\n",
    "X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "y = np.array(y)\n",
    "y = np.where(y == 'M', 1, 0)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "np.random.seed(0)\n",
    "indices = np.random.permutation(X.shape[0])\n",
    "n_train = int(X.shape[0] * 0.8)\n",
    "train_idx, test_idx = indices[:n_train], indices[n_train:]\n",
    "X_train, X_test = X[train_idx, :], X[test_idx, :]\n",
    "y_train, y_test = y[train_idx], y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "class LossFunction:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def loss(self, a : np.ndarray):\n",
    "        prob = sigmoid(np.dot(self.X, a))\n",
    "        values = - self.y * np.log(prob) - (1 - self.y) * np.log(1 - prob)\n",
    "        return np.nansum(values)\n",
    "\n",
    "    def gradient(self, a : np.ndarray):\n",
    "        prob = sigmoid(np.dot(self.X, a))\n",
    "        sub_coefficient = -(self.y - prob) \n",
    "        return np.dot(self.X.T, sub_coefficient)\n",
    "    \n",
    "    def precision(self, a : np.ndarray):\n",
    "        prob = sigmoid(np.dot(self.X, a))\n",
    "        prob = np.array(prob >= 0.5, dtype=np.int32)\n",
    "        return np.sum(prob == self.y) / self.y.shape[0]\n",
    "\n",
    "loss_func = LossFunction(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss = 315.3819671547751, precision = 0.36923076923076925\n",
      "Iteration 1: loss = 309.08937159031245, precision = 0.6351648351648351\n",
      "Iteration 2: loss = 303.40784199196673, precision = 0.6527472527472528\n",
      "Iteration 3: loss = 298.09108824029846, precision = 0.6791208791208792\n",
      "Iteration 4: loss = 293.0329964129459, precision = 0.7142857142857143\n",
      "Iteration 5: loss = 288.18547635115397, precision = 0.7626373626373626\n",
      "Iteration 6: loss = 283.5244890248445, precision = 0.778021978021978\n",
      "Iteration 7: loss = 279.03602567940436, precision = 0.7934065934065934\n",
      "Iteration 8: loss = 274.71032801656247, precision = 0.8285714285714286\n",
      "Iteration 9: loss = 270.53951063080365, precision = 0.8483516483516483\n",
      "Iteration 10: loss = 266.51658665887453, precision = 0.8505494505494505\n",
      "Iteration 11: loss = 262.6350707399034, precision = 0.8615384615384616\n",
      "Iteration 12: loss = 258.88881832566085, precision = 0.865934065934066\n",
      "Iteration 13: loss = 255.27196084418875, precision = 0.8681318681318682\n",
      "Iteration 14: loss = 251.7788790220974, precision = 0.8703296703296703\n",
      "Iteration 15: loss = 248.40419082116395, precision = 0.8747252747252747\n",
      "Iteration 16: loss = 245.1427444973623, precision = 0.8791208791208791\n",
      "Iteration 17: loss = 241.989613052067, precision = 0.8791208791208791\n",
      "Iteration 18: loss = 238.94008869242202, precision = 0.8791208791208791\n",
      "Iteration 19: loss = 235.98967686201863, precision = 0.8857142857142857\n",
      "Iteration 20: loss = 233.13408977241988, precision = 0.8857142857142857\n",
      "Iteration 21: loss = 230.3692395013071, precision = 0.8901098901098901\n",
      "Iteration 22: loss = 227.69123076393896, precision = 0.8945054945054945\n",
      "Iteration 23: loss = 225.09635346848845, precision = 0.8967032967032967\n",
      "Iteration 24: loss = 222.58107515603672, precision = 0.8967032967032967\n",
      "Iteration 25: loss = 220.14203341202517, precision = 0.8989010989010989\n",
      "Iteration 26: loss = 217.77602832156765, precision = 0.9010989010989011\n",
      "Iteration 27: loss = 215.48001502763418, precision = 0.9032967032967033\n",
      "Iteration 28: loss = 213.2510964392074, precision = 0.9054945054945055\n",
      "Iteration 29: loss = 211.08651612616802, precision = 0.9054945054945055\n",
      "Iteration 30: loss = 208.98365142881298, precision = 0.9076923076923077\n",
      "Iteration 31: loss = 206.94000680241777, precision = 0.9098901098901099\n",
      "Iteration 32: loss = 204.95320741097538, precision = 0.9098901098901099\n",
      "Iteration 33: loss = 203.02099297903118, precision = 0.9098901098901099\n",
      "Iteration 34: loss = 201.1412119062437, precision = 0.9098901098901099\n",
      "Iteration 35: loss = 199.31181564581263, precision = 0.9098901098901099\n",
      "Iteration 36: loss = 197.53085334510592, precision = 0.9120879120879121\n",
      "Iteration 37: loss = 195.79646674459264, precision = 0.9120879120879121\n",
      "Iteration 38: loss = 194.1068853294483, precision = 0.9120879120879121\n",
      "Iteration 39: loss = 192.46042172687515, precision = 0.9120879120879121\n",
      "Iteration 40: loss = 190.85546734119467, precision = 0.9142857142857143\n",
      "Iteration 41: loss = 189.2904882180702, precision = 0.9142857142857143\n",
      "Iteration 42: loss = 187.76402112875172, precision = 0.9142857142857143\n",
      "Iteration 43: loss = 186.27466986495867, precision = 0.9164835164835164\n",
      "Iteration 44: loss = 184.8211017348949, precision = 0.9164835164835164\n",
      "Iteration 45: loss = 183.40204425089323, precision = 0.9164835164835164\n",
      "Iteration 46: loss = 182.01628199928476, precision = 0.9164835164835164\n",
      "Iteration 47: loss = 180.66265368326663, precision = 0.9164835164835164\n",
      "Iteration 48: loss = 179.3400493297744, precision = 0.9164835164835164\n",
      "Iteration 49: loss = 178.0474076516438, precision = 0.9164835164835164\n",
      "Iteration 50: loss = 176.78371355665695, precision = 0.9164835164835164\n",
      "Iteration 51: loss = 175.54799579539707, precision = 0.9164835164835164\n",
      "Iteration 52: loss = 174.33932474018138, precision = 0.9164835164835164\n",
      "Iteration 53: loss = 173.15681028769262, precision = 0.9186813186813186\n",
      "Iteration 54: loss = 171.9995998782806, precision = 0.9186813186813186\n",
      "Iteration 55: loss = 170.86687662525782, precision = 0.9186813186813186\n",
      "Iteration 56: loss = 169.75785754785468, precision = 0.9186813186813186\n",
      "Iteration 57: loss = 168.67179190183762, precision = 0.9186813186813186\n",
      "Iteration 58: loss = 167.60795960211914, precision = 0.9186813186813186\n",
      "Iteration 59: loss = 166.56566973200228, precision = 0.9186813186813186\n",
      "Iteration 60: loss = 165.5442591340062, precision = 0.9186813186813186\n",
      "Iteration 61: loss = 164.5430910775067, precision = 0.9186813186813186\n",
      "Iteration 62: loss = 163.5615539987036, precision = 0.9186813186813186\n",
      "Iteration 63: loss = 162.59906030868885, precision = 0.9186813186813186\n",
      "Iteration 64: loss = 161.65504526563808, precision = 0.9164835164835164\n",
      "Iteration 65: loss = 160.72896590738682, precision = 0.9164835164835164\n",
      "Iteration 66: loss = 159.82030004087403, precision = 0.9164835164835164\n",
      "Iteration 67: loss = 158.9285452851483, precision = 0.9164835164835164\n",
      "Iteration 68: loss = 158.0532181648319, precision = 0.9164835164835164\n",
      "Iteration 69: loss = 157.1938532511236, precision = 0.9164835164835164\n",
      "Iteration 70: loss = 156.35000234760082, precision = 0.9164835164835164\n",
      "Iteration 71: loss = 155.52123371824547, precision = 0.9164835164835164\n",
      "Iteration 72: loss = 154.70713135527646, precision = 0.9186813186813186\n",
      "Iteration 73: loss = 153.90729428451726, precision = 0.9186813186813186\n",
      "Iteration 74: loss = 153.1213359061651, precision = 0.9186813186813186\n",
      "Iteration 75: loss = 152.34888336895852, precision = 0.9186813186813186\n",
      "Iteration 76: loss = 151.58957697586123, precision = 0.9208791208791208\n",
      "Iteration 77: loss = 150.84306961949267, precision = 0.9208791208791208\n",
      "Iteration 78: loss = 150.10902624564577, precision = 0.9208791208791208\n",
      "Iteration 79: loss = 149.38712334332857, precision = 0.9208791208791208\n",
      "Iteration 80: loss = 148.6770484598636, precision = 0.9208791208791208\n",
      "Iteration 81: loss = 147.97849973966487, precision = 0.9208791208791208\n",
      "Iteration 82: loss = 147.29118548539543, precision = 0.9208791208791208\n",
      "Iteration 83: loss = 146.61482374028634, precision = 0.9208791208791208\n",
      "Iteration 84: loss = 145.94914189046867, precision = 0.9208791208791208\n",
      "Iteration 85: loss = 145.29387628624053, precision = 0.9208791208791208\n",
      "Iteration 86: loss = 144.64877188125212, precision = 0.9208791208791208\n",
      "Iteration 87: loss = 144.01358188865328, precision = 0.9208791208791208\n",
      "Iteration 88: loss = 143.38806745330328, precision = 0.9208791208791208\n",
      "Iteration 89: loss = 142.77199733919537, precision = 0.9208791208791208\n",
      "Iteration 90: loss = 142.1651476312967, precision = 0.9208791208791208\n",
      "Iteration 91: loss = 141.56730145105183, precision = 0.9208791208791208\n",
      "Iteration 92: loss = 140.97824868484108, precision = 0.9208791208791208\n",
      "Iteration 93: loss = 140.39778572472358, precision = 0.9230769230769231\n",
      "Iteration 94: loss = 139.8257152208361, precision = 0.9230769230769231\n",
      "Iteration 95: loss = 139.26184584485208, precision = 0.9230769230769231\n",
      "Iteration 96: loss = 138.7059920639397, precision = 0.9230769230769231\n",
      "Iteration 97: loss = 138.15797392468968, precision = 0.9230769230769231\n",
      "Iteration 98: loss = 137.6176168465122, precision = 0.9230769230769231\n",
      "Iteration 99: loss = 137.08475142403125, precision = 0.9230769230769231\n",
      "Iteration 100: loss = 136.55921323802988, precision = 0.9230769230769231\n",
      "Iteration 101: loss = 136.04084267452475, precision = 0.9230769230769231\n",
      "Iteration 102: loss = 135.52948475157206, precision = 0.9230769230769231\n",
      "Iteration 103: loss = 135.02498895342714, precision = 0.9230769230769231\n",
      "Iteration 104: loss = 134.52720907170232, precision = 0.9230769230769231\n",
      "Iteration 105: loss = 134.0360030531855, precision = 0.9230769230769231\n",
      "Iteration 106: loss = 133.55123285400057, precision = 0.9230769230769231\n",
      "Iteration 107: loss = 133.07276429980723, precision = 0.9230769230769231\n",
      "Iteration 108: loss = 132.60046695175515, precision = 0.9230769230769231\n",
      "Iteration 109: loss = 132.1342139779207, precision = 0.9230769230769231\n",
      "Iteration 110: loss = 131.67388202997063, precision = 0.9230769230769231\n",
      "Iteration 111: loss = 131.2193511248081, precision = 0.9230769230769231\n",
      "Iteration 112: loss = 130.7705045309723, precision = 0.9230769230769231\n",
      "Iteration 113: loss = 130.3272286595715, precision = 0.9230769230769231\n",
      "Iteration 114: loss = 129.88941295954294, precision = 0.9230769230769231\n",
      "Iteration 115: loss = 129.45694981704287, precision = 0.9230769230769231\n",
      "Iteration 116: loss = 129.02973445877936, precision = 0.9252747252747253\n",
      "Iteration 117: loss = 128.6076648591115, precision = 0.9252747252747253\n",
      "Iteration 118: loss = 128.19064165074582, precision = 0.9274725274725275\n",
      "Iteration 119: loss = 127.77856803887042, precision = 0.9274725274725275\n",
      "Iteration 120: loss = 127.37134971857517, precision = 0.9274725274725275\n",
      "Iteration 121: loss = 126.96889479541264, precision = 0.9296703296703297\n",
      "Iteration 122: loss = 126.57111370896361, precision = 0.9296703296703297\n",
      "Iteration 123: loss = 126.17791915927567, precision = 0.9296703296703297\n",
      "Iteration 124: loss = 125.78922603605146, precision = 0.9296703296703297\n",
      "Iteration 125: loss = 125.40495135046777, precision = 0.9296703296703297\n",
      "Iteration 126: loss = 125.02501416951355, precision = 0.9296703296703297\n",
      "Iteration 127: loss = 124.64933555273956, precision = 0.9296703296703297\n",
      "Iteration 128: loss = 124.27783849131772, precision = 0.9296703296703297\n",
      "Iteration 129: loss = 123.91044784931319, precision = 0.9296703296703297\n",
      "Iteration 130: loss = 123.54709030707652, precision = 0.9296703296703297\n",
      "Iteration 131: loss = 123.18769430666806, precision = 0.9296703296703297\n",
      "Iteration 132: loss = 122.83218999923012, precision = 0.9296703296703297\n",
      "Iteration 133: loss = 122.48050919422717, precision = 0.9318681318681319\n",
      "Iteration 134: loss = 122.13258531047771, precision = 0.9318681318681319\n",
      "Iteration 135: loss = 121.78835332890444, precision = 0.9318681318681319\n",
      "Iteration 136: loss = 121.44774974693395, precision = 0.9318681318681319\n",
      "Iteration 137: loss = 121.11071253447882, precision = 0.9318681318681319\n",
      "Iteration 138: loss = 120.77718109143925, precision = 0.9318681318681319\n",
      "Iteration 139: loss = 120.44709620666379, precision = 0.9318681318681319\n",
      "Iteration 140: loss = 120.12040001831076, precision = 0.9318681318681319\n",
      "Iteration 141: loss = 119.79703597555593, precision = 0.9340659340659341\n",
      "Iteration 142: loss = 119.47694880159342, precision = 0.9340659340659341\n",
      "Iteration 143: loss = 119.16008445787907, precision = 0.9340659340659341\n",
      "Iteration 144: loss = 118.84639010956869, precision = 0.9340659340659341\n",
      "Iteration 145: loss = 118.53581409210463, precision = 0.9340659340659341\n",
      "Iteration 146: loss = 118.22830587890692, precision = 0.9340659340659341\n",
      "Iteration 147: loss = 117.92381605012653, precision = 0.9340659340659341\n",
      "Iteration 148: loss = 117.6222962624206, precision = 0.9340659340659341\n",
      "Iteration 149: loss = 117.32369921971102, precision = 0.9340659340659341\n",
      "Iteration 150: loss = 117.02797864488927, precision = 0.9340659340659341\n",
      "Iteration 151: loss = 116.73508925243218, precision = 0.9340659340659341\n",
      "Iteration 152: loss = 116.44498672189482, precision = 0.9340659340659341\n",
      "Iteration 153: loss = 116.15762767224788, precision = 0.9340659340659341\n",
      "Iteration 154: loss = 115.87296963702855, precision = 0.9340659340659341\n",
      "Iteration 155: loss = 115.59097104027485, precision = 0.9340659340659341\n",
      "Iteration 156: loss = 115.3115911732154, precision = 0.9340659340659341\n",
      "Iteration 157: loss = 115.03479017168641, precision = 0.9340659340659341\n",
      "Iteration 158: loss = 114.76052899425028, precision = 0.9340659340659341\n",
      "Iteration 159: loss = 114.48876940099036, precision = 0.9340659340659341\n",
      "Iteration 160: loss = 114.21947393295758, precision = 0.9340659340659341\n",
      "Iteration 161: loss = 113.9526058922461, precision = 0.9318681318681319\n",
      "Iteration 162: loss = 113.68812932267505, precision = 0.9318681318681319\n",
      "Iteration 163: loss = 113.42600899105611, precision = 0.9318681318681319\n",
      "Iteration 164: loss = 113.166210369025, precision = 0.9318681318681319\n",
      "Iteration 165: loss = 112.90869961541833, precision = 0.9318681318681319\n",
      "Iteration 166: loss = 112.65344355917638, precision = 0.9318681318681319\n",
      "Iteration 167: loss = 112.40040968275366, precision = 0.9318681318681319\n",
      "Iteration 168: loss = 112.14956610601975, precision = 0.9318681318681319\n",
      "Iteration 169: loss = 111.90088157063394, precision = 0.9318681318681319\n",
      "Iteration 170: loss = 111.65432542487697, precision = 0.9318681318681319\n",
      "Iteration 171: loss = 111.409867608925, precision = 0.9318681318681319\n",
      "Iteration 172: loss = 111.1674786405503, precision = 0.9318681318681319\n",
      "Iteration 173: loss = 110.92712960123485, precision = 0.9318681318681319\n",
      "Iteration 174: loss = 110.68879212268257, precision = 0.9318681318681319\n",
      "Iteration 175: loss = 110.45243837371717, precision = 0.9340659340659341\n",
      "Iteration 176: loss = 110.21804104755293, precision = 0.9340659340659341\n",
      "Iteration 177: loss = 109.98557334942586, precision = 0.9340659340659341\n",
      "Iteration 178: loss = 109.7550089845735, precision = 0.9340659340659341\n",
      "Iteration 179: loss = 109.52632214655195, precision = 0.9340659340659341\n",
      "Iteration 180: loss = 109.29948750587948, precision = 0.9340659340659341\n",
      "Iteration 181: loss = 109.0744801989955, precision = 0.9340659340659341\n",
      "Iteration 182: loss = 108.85127581752532, precision = 0.9340659340659341\n",
      "Iteration 183: loss = 108.62985039784044, precision = 0.9340659340659341\n",
      "Iteration 184: loss = 108.41018041090544, precision = 0.9340659340659341\n",
      "Iteration 185: loss = 108.19224275240174, precision = 0.9340659340659341\n",
      "Iteration 186: loss = 107.97601473311994, precision = 0.9340659340659341\n",
      "Iteration 187: loss = 107.76147406961202, precision = 0.9340659340659341\n",
      "Iteration 188: loss = 107.54859887509542, precision = 0.9340659340659341\n",
      "Iteration 189: loss = 107.33736765060092, precision = 0.9340659340659341\n",
      "Iteration 190: loss = 107.12775927635698, precision = 0.9340659340659341\n",
      "Iteration 191: loss = 106.91975300340279, precision = 0.9340659340659341\n",
      "Iteration 192: loss = 106.7133284454237, precision = 0.9340659340659341\n",
      "Iteration 193: loss = 106.50846557080139, precision = 0.9340659340659341\n",
      "Iteration 194: loss = 106.30514469487278, precision = 0.9340659340659341\n",
      "Iteration 195: loss = 106.103346472391, precision = 0.9340659340659341\n",
      "Iteration 196: loss = 105.90305189018258, precision = 0.9340659340659341\n",
      "Iteration 197: loss = 105.70424225999459, precision = 0.9340659340659341\n",
      "Iteration 198: loss = 105.50689921152642, precision = 0.9340659340659341\n",
      "Iteration 199: loss = 105.31100468564023, precision = 0.9340659340659341\n",
      "Iteration 200: loss = 105.11654092774516, precision = 0.9340659340659341\n",
      "Iteration 201: loss = 104.92349048134983, precision = 0.9340659340659341\n",
      "Iteration 202: loss = 104.73183618177833, precision = 0.9340659340659341\n",
      "Iteration 203: loss = 104.54156115004486, precision = 0.9340659340659341\n",
      "Iteration 204: loss = 104.35264878688226, precision = 0.9340659340659341\n",
      "Iteration 205: loss = 104.16508276692012, precision = 0.9340659340659341\n",
      "Iteration 206: loss = 103.97884703300794, precision = 0.9340659340659341\n",
      "Iteration 207: loss = 103.79392579067922, precision = 0.9340659340659341\n",
      "Iteration 208: loss = 103.61030350275249, precision = 0.9340659340659341\n",
      "Iteration 209: loss = 103.4279648840652, precision = 0.9340659340659341\n",
      "Iteration 210: loss = 103.24689489633677, precision = 0.9340659340659341\n",
      "Iteration 211: loss = 103.06707874315703, precision = 0.9362637362637363\n",
      "Iteration 212: loss = 102.88850186509669, precision = 0.9384615384615385\n",
      "Iteration 213: loss = 102.7111499349361, precision = 0.9384615384615385\n",
      "Iteration 214: loss = 102.5350088530092, precision = 0.9384615384615385\n",
      "Iteration 215: loss = 102.36006474265935, precision = 0.9384615384615385\n",
      "Iteration 216: loss = 102.18630394580384, precision = 0.9384615384615385\n",
      "Iteration 217: loss = 102.01371301860434, precision = 0.9384615384615385\n",
      "Iteration 218: loss = 101.84227872723989, precision = 0.9384615384615385\n",
      "Iteration 219: loss = 101.67198804377993, precision = 0.9384615384615385\n",
      "Iteration 220: loss = 101.50282814215475, precision = 0.9384615384615385\n",
      "Iteration 221: loss = 101.33478639422023, precision = 0.9384615384615385\n",
      "Iteration 222: loss = 101.16785036591467, precision = 0.9384615384615385\n",
      "Iteration 223: loss = 101.00200781350506, precision = 0.9384615384615385\n",
      "Iteration 224: loss = 100.8372466799205, precision = 0.9384615384615385\n",
      "Iteration 225: loss = 100.67355509117017, precision = 0.9384615384615385\n",
      "Iteration 226: loss = 100.51092135284387, precision = 0.9384615384615385\n",
      "Iteration 227: loss = 100.34933394669268, precision = 0.9384615384615385\n",
      "Iteration 228: loss = 100.18878152728782, precision = 0.9384615384615385\n",
      "Iteration 229: loss = 100.0292529187553, precision = 0.9384615384615385\n",
      "Iteration 230: loss = 99.87073711158499, precision = 0.9384615384615385\n",
      "Iteration 231: loss = 99.71322325951111, precision = 0.9384615384615385\n",
      "Iteration 232: loss = 99.55670067646355, precision = 0.9384615384615385\n",
      "Iteration 233: loss = 99.40115883358708, precision = 0.9384615384615385\n",
      "Iteration 234: loss = 99.24658735632713, precision = 0.9384615384615385\n",
      "Iteration 235: loss = 99.09297602158065, precision = 0.9384615384615385\n",
      "Iteration 236: loss = 98.94031475490976, precision = 0.9384615384615385\n",
      "Iteration 237: loss = 98.78859362781722, precision = 0.9384615384615385\n",
      "Iteration 238: loss = 98.63780285508153, precision = 0.9384615384615385\n",
      "Iteration 239: loss = 98.48793279215074, precision = 0.9384615384615385\n",
      "Iteration 240: loss = 98.33897393259286, precision = 0.9384615384615385\n",
      "Iteration 241: loss = 98.19091690560192, precision = 0.9384615384615385\n",
      "Iteration 242: loss = 98.04375247355811, precision = 0.9384615384615385\n",
      "Iteration 243: loss = 97.89747152964044, precision = 0.9384615384615385\n",
      "Iteration 244: loss = 97.7520650954909, precision = 0.9384615384615385\n",
      "Iteration 245: loss = 97.6075243189287, precision = 0.9384615384615385\n",
      "Iteration 246: loss = 97.46384047171325, precision = 0.9384615384615385\n",
      "Iteration 247: loss = 97.32100494735491, precision = 0.9384615384615385\n",
      "Iteration 248: loss = 97.17900925897199, precision = 0.9384615384615385\n",
      "Iteration 249: loss = 97.0378450371932, precision = 0.9384615384615385\n",
      "Iteration 250: loss = 96.89750402810418, precision = 0.9384615384615385\n",
      "Iteration 251: loss = 96.75797809123718, precision = 0.9384615384615385\n",
      "Iteration 252: loss = 96.61925919760279, precision = 0.9384615384615385\n",
      "Iteration 253: loss = 96.48133942776262, precision = 0.9384615384615385\n",
      "Iteration 254: loss = 96.34421096994201, precision = 0.9384615384615385\n",
      "Iteration 255: loss = 96.20786611818187, precision = 0.9384615384615385\n",
      "Iteration 256: loss = 96.07229727052864, precision = 0.9384615384615385\n",
      "Iteration 257: loss = 95.93749692726126, precision = 0.9384615384615385\n",
      "Iteration 258: loss = 95.80345768915471, precision = 0.9384615384615385\n",
      "Iteration 259: loss = 95.67017225577885, precision = 0.9384615384615385\n",
      "Iteration 260: loss = 95.53763342383179, precision = 0.9384615384615385\n",
      "Iteration 261: loss = 95.40583408550728, precision = 0.9384615384615385\n",
      "Iteration 262: loss = 95.2747672268948, precision = 0.9384615384615385\n",
      "Iteration 263: loss = 95.14442592641207, precision = 0.9384615384615385\n",
      "Iteration 264: loss = 95.01480335326889, precision = 0.9384615384615385\n",
      "Iteration 265: loss = 94.88589276596181, precision = 0.9384615384615385\n",
      "Iteration 266: loss = 94.7576875107986, precision = 0.9384615384615385\n",
      "Iteration 267: loss = 94.63018102045234, precision = 0.9384615384615385\n",
      "Iteration 268: loss = 94.50336681254379, precision = 0.9384615384615385\n",
      "Iteration 269: loss = 94.37723848825198, precision = 0.9384615384615385\n",
      "Iteration 270: loss = 94.25178973095208, precision = 0.9384615384615385\n",
      "Iteration 271: loss = 94.1270143048798, precision = 0.9384615384615385\n",
      "Iteration 272: loss = 94.00290605382207, precision = 0.9384615384615385\n",
      "Iteration 273: loss = 93.87945889983322, precision = 0.9384615384615385\n",
      "Iteration 274: loss = 93.75666684197591, precision = 0.9384615384615385\n",
      "Iteration 275: loss = 93.63452395508654, precision = 0.9384615384615385\n",
      "Iteration 276: loss = 93.5130243885644, precision = 0.9384615384615385\n",
      "Iteration 277: loss = 93.39216236518428, precision = 0.9384615384615385\n",
      "Iteration 278: loss = 93.27193217993144, precision = 0.9384615384615385\n",
      "Iteration 279: loss = 93.15232819885924, precision = 0.9406593406593406\n",
      "Iteration 280: loss = 93.03334485796825, precision = 0.9406593406593406\n",
      "Iteration 281: loss = 92.91497666210674, precision = 0.9406593406593406\n",
      "Iteration 282: loss = 92.79721818389186, precision = 0.9406593406593406\n",
      "Iteration 283: loss = 92.68006406265133, precision = 0.9406593406593406\n",
      "Iteration 284: loss = 92.5635090033848, precision = 0.9406593406593406\n",
      "Iteration 285: loss = 92.4475477757449, precision = 0.9406593406593406\n",
      "Iteration 286: loss = 92.33217521303706, precision = 0.9406593406593406\n",
      "Iteration 287: loss = 92.21738621123819, precision = 0.9406593406593406\n",
      "Iteration 288: loss = 92.10317572803338, precision = 0.9406593406593406\n",
      "Iteration 289: loss = 91.98953878187054, precision = 0.9406593406593406\n",
      "Iteration 290: loss = 91.87647045103239, precision = 0.9406593406593406\n",
      "Iteration 291: loss = 91.76396587272566, precision = 0.9406593406593406\n",
      "Iteration 292: loss = 91.65202024218667, precision = 0.9406593406593406\n",
      "Iteration 293: loss = 91.54062881180357, precision = 0.9406593406593406\n",
      "Iteration 294: loss = 91.42978689025445, precision = 0.9406593406593406\n",
      "Iteration 295: loss = 91.31948984166104, precision = 0.9406593406593406\n",
      "Iteration 296: loss = 91.20973308475783, precision = 0.9406593406593406\n",
      "Iteration 297: loss = 91.10051209207606, precision = 0.9406593406593406\n",
      "Iteration 298: loss = 90.9918223891425, precision = 0.9406593406593406\n",
      "Iteration 299: loss = 90.88365955369254, precision = 0.9406593406593406\n",
      "Iteration 300: loss = 90.77601921489739, precision = 0.9406593406593406\n",
      "Iteration 301: loss = 90.66889705260505, precision = 0.9406593406593406\n",
      "Iteration 302: loss = 90.56228879659469, precision = 0.9406593406593406\n",
      "Iteration 303: loss = 90.45619022584438, precision = 0.9406593406593406\n",
      "Iteration 304: loss = 90.3505971678116, precision = 0.9406593406593406\n",
      "Iteration 305: loss = 90.24550549772663, precision = 0.9406593406593406\n",
      "Iteration 306: loss = 90.14091113789806, precision = 0.9406593406593406\n",
      "Iteration 307: loss = 90.03681005703079, precision = 0.9406593406593406\n",
      "Iteration 308: loss = 89.93319826955567, precision = 0.9406593406593406\n",
      "Iteration 309: loss = 89.83007183497088, precision = 0.9406593406593406\n",
      "Iteration 310: loss = 89.72742685719486, precision = 0.9406593406593406\n",
      "Iteration 311: loss = 89.6252594839303, precision = 0.9406593406593406\n",
      "Iteration 312: loss = 89.52356590603912, precision = 0.9406593406593406\n",
      "Iteration 313: loss = 89.42234235692827, precision = 0.9406593406593406\n",
      "Iteration 314: loss = 89.32158511194595, precision = 0.9406593406593406\n",
      "Iteration 315: loss = 89.22129048778834, precision = 0.9406593406593406\n",
      "Iteration 316: loss = 89.12145484191622, precision = 0.9406593406593406\n",
      "Iteration 317: loss = 89.02207457198168, precision = 0.9406593406593406\n",
      "Iteration 318: loss = 88.92314611526447, precision = 0.9406593406593406\n",
      "Iteration 319: loss = 88.82466594811793, precision = 0.9406593406593406\n",
      "Iteration 320: loss = 88.72663058542419, precision = 0.9406593406593406\n",
      "Iteration 321: loss = 88.6290365800586, precision = 0.9428571428571428\n",
      "Iteration 322: loss = 88.53188052236304, precision = 0.9428571428571428\n",
      "Iteration 323: loss = 88.43515903962822, precision = 0.9428571428571428\n",
      "Iteration 324: loss = 88.33886879558435, precision = 0.9428571428571428\n",
      "Iteration 325: loss = 88.24300648990047, precision = 0.9428571428571428\n",
      "Iteration 326: loss = 88.14756885769201, precision = 0.9428571428571428\n",
      "Iteration 327: loss = 88.05255266903644, precision = 0.9428571428571428\n",
      "Iteration 328: loss = 87.95795472849713, precision = 0.9428571428571428\n",
      "Iteration 329: loss = 87.86377187465456, precision = 0.9428571428571428\n",
      "Iteration 330: loss = 87.77000097964574, precision = 0.9428571428571428\n",
      "Iteration 331: loss = 87.67663894871092, precision = 0.9428571428571428\n",
      "Iteration 332: loss = 87.58368271974751, precision = 0.9428571428571428\n",
      "Iteration 333: loss = 87.49112926287161, precision = 0.945054945054945\n",
      "Iteration 334: loss = 87.3989755799864, precision = 0.945054945054945\n",
      "Iteration 335: loss = 87.30721870435754, precision = 0.945054945054945\n",
      "Iteration 336: loss = 87.21585570019548, precision = 0.945054945054945\n",
      "Iteration 337: loss = 87.12488366224444, precision = 0.945054945054945\n",
      "Iteration 338: loss = 87.03429971537798, precision = 0.945054945054945\n",
      "Iteration 339: loss = 86.94410101420107, precision = 0.945054945054945\n",
      "Iteration 340: loss = 86.85428474265834, precision = 0.945054945054945\n",
      "Iteration 341: loss = 86.76484811364887, precision = 0.945054945054945\n",
      "Iteration 342: loss = 86.6757883686468, precision = 0.945054945054945\n",
      "Iteration 343: loss = 86.5871027773281, precision = 0.945054945054945\n",
      "Iteration 344: loss = 86.49878863720306, precision = 0.945054945054945\n",
      "Iteration 345: loss = 86.41084327325478, precision = 0.945054945054945\n",
      "Iteration 346: loss = 86.32326403758321, precision = 0.945054945054945\n",
      "Iteration 347: loss = 86.23604830905475, precision = 0.945054945054945\n",
      "Iteration 348: loss = 86.14919349295728, precision = 0.945054945054945\n",
      "Iteration 349: loss = 86.06269702066075, precision = 0.945054945054945\n",
      "Iteration 350: loss = 85.97655634928265, precision = 0.945054945054945\n",
      "Iteration 351: loss = 85.89076896135924, precision = 0.945054945054945\n",
      "Iteration 352: loss = 85.80533236452129, precision = 0.945054945054945\n",
      "Iteration 353: loss = 85.72024409117503, precision = 0.945054945054945\n",
      "Iteration 354: loss = 85.63550169818822, precision = 0.9428571428571428\n",
      "Iteration 355: loss = 85.55110276658056, precision = 0.9428571428571428\n",
      "Iteration 356: loss = 85.46704490121925, precision = 0.9428571428571428\n",
      "Iteration 357: loss = 85.38332573051909, precision = 0.9428571428571428\n",
      "Iteration 358: loss = 85.29994290614684, precision = 0.9428571428571428\n",
      "Iteration 359: loss = 85.21689410273049, precision = 0.9428571428571428\n",
      "Iteration 360: loss = 85.13417701757272, precision = 0.9428571428571428\n",
      "Iteration 361: loss = 85.05178937036857, precision = 0.9428571428571428\n",
      "Iteration 362: loss = 84.96972890292757, precision = 0.9428571428571428\n",
      "Iteration 363: loss = 84.88799337889996, precision = 0.9428571428571428\n",
      "Iteration 364: loss = 84.80658058350691, precision = 0.9428571428571428\n",
      "Iteration 365: loss = 84.72548832327493, precision = 0.9428571428571428\n",
      "Iteration 366: loss = 84.64471442577411, precision = 0.9428571428571428\n",
      "Iteration 367: loss = 84.56425673936022, precision = 0.9428571428571428\n",
      "Iteration 368: loss = 84.48411313292087, precision = 0.9428571428571428\n",
      "Iteration 369: loss = 84.40428149562509, precision = 0.9428571428571428\n",
      "Iteration 370: loss = 84.32475973667678, precision = 0.9428571428571428\n",
      "Iteration 371: loss = 84.24554578507184, precision = 0.9428571428571428\n",
      "Iteration 372: loss = 84.16663758935869, precision = 0.9428571428571428\n",
      "Iteration 373: loss = 84.0880331174024, precision = 0.9428571428571428\n",
      "Iteration 374: loss = 84.00973035615232, precision = 0.9428571428571428\n",
      "Iteration 375: loss = 83.93172731141291, precision = 0.9428571428571428\n",
      "Iteration 376: loss = 83.85402200761807, precision = 0.9428571428571428\n",
      "Iteration 377: loss = 83.77661248760862, precision = 0.9428571428571428\n",
      "Iteration 378: loss = 83.69949681241305, precision = 0.9428571428571428\n",
      "Iteration 379: loss = 83.62267306103146, precision = 0.9428571428571428\n",
      "Iteration 380: loss = 83.54613933022253, precision = 0.9428571428571428\n",
      "Iteration 381: loss = 83.46989373429366, precision = 0.9428571428571428\n",
      "Iteration 382: loss = 83.39393440489378, precision = 0.9428571428571428\n",
      "Iteration 383: loss = 83.31825949080971, precision = 0.9428571428571428\n",
      "Iteration 384: loss = 83.24286715776483, precision = 0.9428571428571428\n",
      "Iteration 385: loss = 83.16775558822097, precision = 0.9428571428571428\n",
      "Iteration 386: loss = 83.09292298118288, precision = 0.9428571428571428\n",
      "Iteration 387: loss = 83.01836755200569, precision = 0.9428571428571428\n",
      "Iteration 388: loss = 82.94408753220489, precision = 0.9428571428571428\n",
      "Iteration 389: loss = 82.87008116926907, precision = 0.9428571428571428\n",
      "Iteration 390: loss = 82.79634672647526, precision = 0.9428571428571428\n",
      "Iteration 391: loss = 82.72288248270692, precision = 0.9428571428571428\n",
      "Iteration 392: loss = 82.6496867322743, precision = 0.9428571428571428\n",
      "Iteration 393: loss = 82.57675778473752, precision = 0.9428571428571428\n",
      "Iteration 394: loss = 82.50409396473198, precision = 0.9428571428571428\n",
      "Iteration 395: loss = 82.43169361179615, precision = 0.9428571428571428\n",
      "Iteration 396: loss = 82.35955508020191, precision = 0.9428571428571428\n",
      "Iteration 397: loss = 82.28767673878704, precision = 0.9428571428571428\n",
      "Iteration 398: loss = 82.2160569707901, precision = 0.9428571428571428\n",
      "Iteration 399: loss = 82.14469417368761, precision = 0.9428571428571428\n",
      "Iteration 400: loss = 82.07358675903339, precision = 0.9428571428571428\n",
      "Iteration 401: loss = 82.0027331523002, precision = 0.9428571428571428\n",
      "Iteration 402: loss = 81.93213179272331, precision = 0.9428571428571428\n",
      "Iteration 403: loss = 81.86178113314654, precision = 0.9428571428571428\n",
      "Iteration 404: loss = 81.79167963987013, precision = 0.9428571428571428\n",
      "Iteration 405: loss = 81.7218257925006, precision = 0.9428571428571428\n",
      "Iteration 406: loss = 81.652218083803, precision = 0.9428571428571428\n",
      "Iteration 407: loss = 81.58285501955484, precision = 0.9428571428571428\n",
      "Iteration 408: loss = 81.51373511840204, precision = 0.9428571428571428\n",
      "Iteration 409: loss = 81.44485691171684, precision = 0.9472527472527472\n",
      "Iteration 410: loss = 81.37621894345763, precision = 0.9472527472527472\n",
      "Iteration 411: loss = 81.30781977003068, precision = 0.9472527472527472\n",
      "Iteration 412: loss = 81.23965796015361, precision = 0.9472527472527472\n",
      "Iteration 413: loss = 81.17173209472072, precision = 0.9472527472527472\n",
      "Iteration 414: loss = 81.10404076667018, precision = 0.9472527472527472\n",
      "Iteration 415: loss = 81.03658258085284, precision = 0.9472527472527472\n",
      "Iteration 416: loss = 80.96935615390284, precision = 0.9472527472527472\n",
      "Iteration 417: loss = 80.90236011411002, precision = 0.9472527472527472\n",
      "Iteration 418: loss = 80.83559310129378, precision = 0.9494505494505494\n",
      "Iteration 419: loss = 80.76905376667881, precision = 0.9494505494505494\n",
      "Iteration 420: loss = 80.70274077277234, precision = 0.9494505494505494\n",
      "Iteration 421: loss = 80.63665279324297, precision = 0.9494505494505494\n",
      "Iteration 422: loss = 80.57078851280122, precision = 0.9494505494505494\n",
      "Iteration 423: loss = 80.5051466270813, precision = 0.9494505494505494\n",
      "Iteration 424: loss = 80.43972584252498, precision = 0.9494505494505494\n",
      "Iteration 425: loss = 80.37452487626632, precision = 0.9494505494505494\n",
      "Iteration 426: loss = 80.30954245601833, precision = 0.9494505494505494\n",
      "Iteration 427: loss = 80.24477731996092, precision = 0.9494505494505494\n",
      "Iteration 428: loss = 80.18022821663028, precision = 0.9494505494505494\n",
      "Iteration 429: loss = 80.11589390480978, precision = 0.9494505494505494\n",
      "Iteration 430: loss = 80.05177315342216, precision = 0.9494505494505494\n",
      "Iteration 431: loss = 79.98786474142312, precision = 0.9494505494505494\n",
      "Iteration 432: loss = 79.92416745769629, precision = 0.9494505494505494\n",
      "Iteration 433: loss = 79.86068010094951, precision = 0.9494505494505494\n",
      "Iteration 434: loss = 79.79740147961249, precision = 0.9494505494505494\n",
      "Iteration 435: loss = 79.73433041173558, precision = 0.9494505494505494\n",
      "Iteration 436: loss = 79.67146572489017, precision = 0.9494505494505494\n",
      "Iteration 437: loss = 79.6088062560698, precision = 0.9494505494505494\n",
      "Iteration 438: loss = 79.54635085159317, precision = 0.9494505494505494\n",
      "Iteration 439: loss = 79.48409836700779, precision = 0.9494505494505494\n",
      "Iteration 440: loss = 79.4220476669952, precision = 0.9494505494505494\n",
      "Iteration 441: loss = 79.36019762527715, precision = 0.9516483516483516\n",
      "Iteration 442: loss = 79.29854712452313, precision = 0.9516483516483516\n",
      "Iteration 443: loss = 79.23709505625892, precision = 0.9538461538461539\n",
      "Iteration 444: loss = 79.17584032077636, precision = 0.9538461538461539\n",
      "Iteration 445: loss = 79.11478182704406, precision = 0.9538461538461539\n",
      "Iteration 446: loss = 79.05391849261957, precision = 0.9538461538461539\n",
      "Iteration 447: loss = 78.99324924356219, precision = 0.9538461538461539\n",
      "Iteration 448: loss = 78.93277301434722, precision = 0.9538461538461539\n",
      "Iteration 449: loss = 78.87248874778102, precision = 0.9538461538461539\n",
      "Iteration 450: loss = 78.81239539491727, precision = 0.9538461538461539\n",
      "Iteration 451: loss = 78.7524919149741, precision = 0.9538461538461539\n",
      "Iteration 452: loss = 78.69277727525241, precision = 0.9538461538461539\n",
      "Iteration 453: loss = 78.63325045105502, precision = 0.9538461538461539\n",
      "Iteration 454: loss = 78.57391042560693, precision = 0.9538461538461539\n",
      "Iteration 455: loss = 78.51475618997634, precision = 0.9538461538461539\n",
      "Iteration 456: loss = 78.45578674299705, precision = 0.9538461538461539\n",
      "Iteration 457: loss = 78.39700109119116, precision = 0.9538461538461539\n",
      "Iteration 458: loss = 78.33839824869338, precision = 0.9538461538461539\n",
      "Iteration 459: loss = 78.27997723717567, precision = 0.9538461538461539\n",
      "Iteration 460: loss = 78.22173708577324, precision = 0.9538461538461539\n",
      "Iteration 461: loss = 78.16367683101112, precision = 0.9538461538461539\n",
      "Iteration 462: loss = 78.10579551673166, precision = 0.9538461538461539\n",
      "Iteration 463: loss = 78.04809219402314, precision = 0.9538461538461539\n",
      "Iteration 464: loss = 77.99056592114891, precision = 0.9538461538461539\n",
      "Iteration 465: loss = 77.93321576347748, precision = 0.9538461538461539\n",
      "Iteration 466: loss = 77.87604079341355, precision = 0.9538461538461539\n",
      "Iteration 467: loss = 77.81904009032974, precision = 0.9538461538461539\n",
      "Iteration 468: loss = 77.76221274049904, precision = 0.9538461538461539\n",
      "Iteration 469: loss = 77.70555783702835, precision = 0.9538461538461539\n",
      "Iteration 470: loss = 77.64907447979243, precision = 0.9538461538461539\n",
      "Iteration 471: loss = 77.59276177536904, precision = 0.9538461538461539\n",
      "Iteration 472: loss = 77.53661883697424, precision = 0.9538461538461539\n",
      "Iteration 473: loss = 77.48064478439922, precision = 0.9538461538461539\n",
      "Iteration 474: loss = 77.42483874394716, precision = 0.9538461538461539\n",
      "Iteration 475: loss = 77.36919984837124, precision = 0.9538461538461539\n",
      "Iteration 476: loss = 77.31372723681326, precision = 0.9538461538461539\n",
      "Iteration 477: loss = 77.25842005474291, precision = 0.9538461538461539\n",
      "Iteration 478: loss = 77.20327745389784, precision = 0.9538461538461539\n",
      "Iteration 479: loss = 77.14829859222425, precision = 0.9538461538461539\n",
      "Iteration 480: loss = 77.09348263381852, precision = 0.9538461538461539\n",
      "Iteration 481: loss = 77.038828748869, precision = 0.9538461538461539\n",
      "Iteration 482: loss = 76.98433611359894, precision = 0.9538461538461539\n",
      "Iteration 483: loss = 76.93000391020979, precision = 0.9538461538461539\n",
      "Iteration 484: loss = 76.87583132682522, precision = 0.9538461538461539\n",
      "Iteration 485: loss = 76.82181755743585, precision = 0.9538461538461539\n",
      "Iteration 486: loss = 76.76796180184452, precision = 0.9538461538461539\n",
      "Iteration 487: loss = 76.71426326561215, precision = 0.9538461538461539\n",
      "Iteration 488: loss = 76.66072116000434, precision = 0.9538461538461539\n",
      "Iteration 489: loss = 76.60733470193847, precision = 0.9538461538461539\n",
      "Iteration 490: loss = 76.55410311393148, precision = 0.9538461538461539\n",
      "Iteration 491: loss = 76.50102562404805, precision = 0.9538461538461539\n",
      "Iteration 492: loss = 76.44810146584976, precision = 0.9538461538461539\n",
      "Iteration 493: loss = 76.39532987834421, precision = 0.9538461538461539\n",
      "Iteration 494: loss = 76.34271010593541, precision = 0.9538461538461539\n",
      "Iteration 495: loss = 76.29024139837409, precision = 0.9538461538461539\n",
      "Iteration 496: loss = 76.23792301070898, precision = 0.9538461538461539\n",
      "Iteration 497: loss = 76.18575420323853, precision = 0.9538461538461539\n",
      "Iteration 498: loss = 76.13373424146306, precision = 0.9538461538461539\n",
      "Iteration 499: loss = 76.08186239603754, precision = 0.9538461538461539\n",
      "Iteration 500: loss = 76.03013794272482, precision = 0.9538461538461539\n",
      "Iteration 501: loss = 75.97856016234955, precision = 0.9538461538461539\n",
      "Iteration 502: loss = 75.92712834075232, precision = 0.9538461538461539\n",
      "Iteration 503: loss = 75.87584176874458, precision = 0.9538461538461539\n",
      "Iteration 504: loss = 75.82469974206384, precision = 0.9538461538461539\n",
      "Iteration 505: loss = 75.77370156132963, precision = 0.9516483516483516\n",
      "Iteration 506: loss = 75.72284653199951, precision = 0.9516483516483516\n",
      "Iteration 507: loss = 75.6721339643261, precision = 0.9516483516483516\n",
      "Iteration 508: loss = 75.62156317331406, precision = 0.9516483516483516\n",
      "Iteration 509: loss = 75.57113347867792, precision = 0.9516483516483516\n",
      "Iteration 510: loss = 75.52084420480018, precision = 0.9516483516483516\n",
      "Iteration 511: loss = 75.47069468068992, precision = 0.9516483516483516\n",
      "Iteration 512: loss = 75.42068423994178, precision = 0.9516483516483516\n",
      "Iteration 513: loss = 75.37081222069557, precision = 0.9516483516483516\n",
      "Iteration 514: loss = 75.32107796559603, precision = 0.9516483516483516\n",
      "Iteration 515: loss = 75.27148082175331, precision = 0.9516483516483516\n",
      "Iteration 516: loss = 75.22202014070376, precision = 0.9516483516483516\n",
      "Iteration 517: loss = 75.17269527837092, precision = 0.9516483516483516\n",
      "Iteration 518: loss = 75.12350559502742, precision = 0.9516483516483516\n",
      "Iteration 519: loss = 75.07445045525682, precision = 0.9516483516483516\n",
      "Iteration 520: loss = 75.02552922791604, precision = 0.9516483516483516\n",
      "Iteration 521: loss = 74.97674128609823, precision = 0.9516483516483516\n",
      "Iteration 522: loss = 74.92808600709604, precision = 0.9516483516483516\n",
      "Iteration 523: loss = 74.87956277236506, precision = 0.9516483516483516\n",
      "Iteration 524: loss = 74.83117096748802, precision = 0.9516483516483516\n",
      "Iteration 525: loss = 74.78290998213893, precision = 0.9516483516483516\n",
      "Iteration 526: loss = 74.73477921004809, precision = 0.9516483516483516\n",
      "Iteration 527: loss = 74.68677804896697, precision = 0.9516483516483516\n",
      "Iteration 528: loss = 74.63890590063382, precision = 0.9516483516483516\n",
      "Iteration 529: loss = 74.59116217073947, precision = 0.9516483516483516\n",
      "Iteration 530: loss = 74.54354626889356, precision = 0.9516483516483516\n",
      "Iteration 531: loss = 74.49605760859106, precision = 0.9516483516483516\n",
      "Iteration 532: loss = 74.44869560717919, precision = 0.9516483516483516\n",
      "Iteration 533: loss = 74.40145968582468, precision = 0.9516483516483516\n",
      "Iteration 534: loss = 74.35434926948126, precision = 0.9516483516483516\n",
      "Iteration 535: loss = 74.30736378685774, precision = 0.9516483516483516\n",
      "Iteration 536: loss = 74.26050267038609, precision = 0.9516483516483516\n",
      "Iteration 537: loss = 74.21376535619018, precision = 0.9516483516483516\n",
      "Iteration 538: loss = 74.16715128405451, precision = 0.9516483516483516\n",
      "Iteration 539: loss = 74.12065989739361, precision = 0.9516483516483516\n",
      "Iteration 540: loss = 74.07429064322145, precision = 0.9516483516483516\n",
      "Iteration 541: loss = 74.02804297212131, precision = 0.9516483516483516\n",
      "Iteration 542: loss = 73.98191633821597, precision = 0.9516483516483516\n",
      "Iteration 543: loss = 73.93591019913818, precision = 0.9516483516483516\n",
      "Iteration 544: loss = 73.89002401600132, precision = 0.9516483516483516\n",
      "Iteration 545: loss = 73.84425725337064, precision = 0.9516483516483516\n",
      "Iteration 546: loss = 73.79860937923445, precision = 0.9538461538461539\n",
      "Iteration 547: loss = 73.75307986497576, precision = 0.9538461538461539\n",
      "Iteration 548: loss = 73.7076681853444, precision = 0.9538461538461539\n",
      "Iteration 549: loss = 73.66237381842902, precision = 0.9538461538461539\n",
      "Iteration 550: loss = 73.61719624562967, precision = 0.9538461538461539\n",
      "Iteration 551: loss = 73.57213495163057, precision = 0.9538461538461539\n",
      "Iteration 552: loss = 73.52718942437315, precision = 0.9538461538461539\n",
      "Iteration 553: loss = 73.48235915502937, precision = 0.9538461538461539\n",
      "Iteration 554: loss = 73.43764363797524, precision = 0.9538461538461539\n",
      "Iteration 555: loss = 73.39304237076476, precision = 0.9538461538461539\n",
      "Iteration 556: loss = 73.34855485410395, precision = 0.9538461538461539\n",
      "Iteration 557: loss = 73.30418059182526, precision = 0.9538461538461539\n",
      "Iteration 558: loss = 73.2599190908621, precision = 0.9538461538461539\n",
      "Iteration 559: loss = 73.21576986122383, precision = 0.9538461538461539\n",
      "Iteration 560: loss = 73.17173241597081, precision = 0.9538461538461539\n",
      "Iteration 561: loss = 73.12780627118971, precision = 0.9538461538461539\n",
      "Iteration 562: loss = 73.08399094596922, precision = 0.9538461538461539\n",
      "Iteration 563: loss = 73.04028596237583, precision = 0.9538461538461539\n",
      "Iteration 564: loss = 72.99669084542995, precision = 0.9538461538461539\n",
      "Iteration 565: loss = 72.95320512308217, precision = 0.9538461538461539\n",
      "Iteration 566: loss = 72.90982832618984, precision = 0.9538461538461539\n",
      "Iteration 567: loss = 72.86655998849392, precision = 0.9538461538461539\n",
      "Iteration 568: loss = 72.82339964659582, precision = 0.9538461538461539\n",
      "Iteration 569: loss = 72.78034683993481, precision = 0.9538461538461539\n",
      "Iteration 570: loss = 72.73740111076532, precision = 0.9538461538461539\n",
      "Iteration 571: loss = 72.69456200413474, precision = 0.9560439560439561\n",
      "Iteration 572: loss = 72.65182906786114, precision = 0.9582417582417583\n",
      "Iteration 573: loss = 72.6092018525116, precision = 0.9582417582417583\n",
      "Iteration 574: loss = 72.56667991138028, precision = 0.9582417582417583\n",
      "Iteration 575: loss = 72.52426280046708, precision = 0.9582417582417583\n",
      "Iteration 576: loss = 72.48195007845636, precision = 0.9582417582417583\n",
      "Iteration 577: loss = 72.43974130669585, precision = 0.9582417582417583\n",
      "Iteration 578: loss = 72.39763604917574, precision = 0.9582417582417583\n",
      "Iteration 579: loss = 72.3556338725081, precision = 0.9582417582417583\n",
      "Iteration 580: loss = 72.31373434590628, precision = 0.9582417582417583\n",
      "Iteration 581: loss = 72.27193704116488, precision = 0.9582417582417583\n",
      "Iteration 582: loss = 72.23024153263927, precision = 0.9582417582417583\n",
      "Iteration 583: loss = 72.18864739722615, precision = 0.9582417582417583\n",
      "Iteration 584: loss = 72.14715421434354, precision = 0.9582417582417583\n",
      "Iteration 585: loss = 72.10576156591141, precision = 0.9582417582417583\n",
      "Iteration 586: loss = 72.06446903633223, precision = 0.9582417582417583\n",
      "Iteration 587: loss = 72.02327621247193, precision = 0.9582417582417583\n",
      "Iteration 588: loss = 71.98218268364093, precision = 0.9582417582417583\n",
      "Iteration 589: loss = 71.94118804157523, precision = 0.9604395604395605\n",
      "Iteration 590: loss = 71.900291880418, precision = 0.9604395604395605\n",
      "Iteration 591: loss = 71.85949379670095, precision = 0.9604395604395605\n",
      "Iteration 592: loss = 71.81879338932616, precision = 0.9604395604395605\n",
      "Iteration 593: loss = 71.77819025954796, precision = 0.9604395604395605\n",
      "Iteration 594: loss = 71.73768401095506, precision = 0.9604395604395605\n",
      "Iteration 595: loss = 71.69727424945268, precision = 0.9604395604395605\n",
      "Iteration 596: loss = 71.65696058324511, precision = 0.9604395604395605\n",
      "Iteration 597: loss = 71.61674262281812, precision = 0.9604395604395605\n",
      "Iteration 598: loss = 71.57661998092178, precision = 0.9604395604395605\n",
      "Iteration 599: loss = 71.53659227255338, precision = 0.9604395604395605\n",
      "Iteration 600: loss = 71.49665911494041, precision = 0.9604395604395605\n",
      "Iteration 601: loss = 71.4568201275238, precision = 0.9604395604395605\n",
      "Iteration 602: loss = 71.4170749319413, precision = 0.9604395604395605\n",
      "Iteration 603: loss = 71.37742315201092, precision = 0.9604395604395605\n",
      "Iteration 604: loss = 71.3378644137147, precision = 0.9604395604395605\n",
      "Iteration 605: loss = 71.29839834518248, precision = 0.9604395604395605\n",
      "Iteration 606: loss = 71.2590245766758, precision = 0.9604395604395605\n",
      "Iteration 607: loss = 71.21974274057209, precision = 0.9604395604395605\n",
      "Iteration 608: loss = 71.18055247134899, precision = 0.9604395604395605\n",
      "Iteration 609: loss = 71.14145340556851, precision = 0.9604395604395605\n",
      "Iteration 610: loss = 71.10244518186181, precision = 0.9604395604395605\n",
      "Iteration 611: loss = 71.06352744091382, precision = 0.9604395604395605\n",
      "Iteration 612: loss = 71.02469982544798, precision = 0.9604395604395605\n",
      "Iteration 613: loss = 70.98596198021121, precision = 0.9604395604395605\n",
      "Iteration 614: loss = 70.94731355195916, precision = 0.9604395604395605\n",
      "Iteration 615: loss = 70.9087541894412, precision = 0.9604395604395605\n",
      "Iteration 616: loss = 70.87028354338594, precision = 0.9604395604395605\n",
      "Iteration 617: loss = 70.83190126648671, precision = 0.9604395604395605\n",
      "Iteration 618: loss = 70.79360701338712, precision = 0.9604395604395605\n",
      "Iteration 619: loss = 70.75540044066683, precision = 0.9604395604395605\n",
      "Iteration 620: loss = 70.7172812068275, precision = 0.9604395604395605\n",
      "Iteration 621: loss = 70.67924897227861, precision = 0.9604395604395605\n",
      "Iteration 622: loss = 70.64130339932387, precision = 0.9604395604395605\n",
      "Iteration 623: loss = 70.60344415214719, precision = 0.9604395604395605\n",
      "Iteration 624: loss = 70.56567089679926, precision = 0.9604395604395605\n",
      "Iteration 625: loss = 70.52798330118391, precision = 0.9604395604395605\n",
      "Iteration 626: loss = 70.49038103504479, precision = 0.9604395604395605\n",
      "Iteration 627: loss = 70.45286376995216, precision = 0.9604395604395605\n",
      "Iteration 628: loss = 70.41543117928956, precision = 0.9604395604395605\n",
      "Iteration 629: loss = 70.37808293824098, precision = 0.9604395604395605\n",
      "Iteration 630: loss = 70.34081872377783, precision = 0.9604395604395605\n",
      "Iteration 631: loss = 70.30363821464616, precision = 0.9604395604395605\n",
      "Iteration 632: loss = 70.26654109135396, precision = 0.9604395604395605\n",
      "Iteration 633: loss = 70.22952703615854, precision = 0.9604395604395605\n",
      "Iteration 634: loss = 70.19259573305419, precision = 0.9604395604395605\n",
      "Iteration 635: loss = 70.15574686775969, precision = 0.9604395604395605\n",
      "Iteration 636: loss = 70.11898012770607, precision = 0.9604395604395605\n",
      "Iteration 637: loss = 70.08229520202451, precision = 0.9604395604395605\n",
      "Iteration 638: loss = 70.04569178153429, precision = 0.9604395604395605\n",
      "Iteration 639: loss = 70.00916955873078, precision = 0.9604395604395605\n",
      "Iteration 640: loss = 69.97272822777373, precision = 0.9604395604395605\n",
      "Iteration 641: loss = 69.93636748447545, precision = 0.9604395604395605\n",
      "Iteration 642: loss = 69.90008702628916, precision = 0.9604395604395605\n",
      "Iteration 643: loss = 69.86388655229757, precision = 0.9604395604395605\n",
      "Iteration 644: loss = 69.82776576320133, precision = 0.9604395604395605\n",
      "Iteration 645: loss = 69.79172436130773, precision = 0.9604395604395605\n",
      "Iteration 646: loss = 69.75576205051951, precision = 0.9604395604395605\n",
      "Iteration 647: loss = 69.71987853632373, precision = 0.9604395604395605\n",
      "Iteration 648: loss = 69.6840735257806, precision = 0.9604395604395605\n",
      "Iteration 649: loss = 69.64834672751269, precision = 0.9604395604395605\n",
      "Iteration 650: loss = 69.61269785169392, precision = 0.9604395604395605\n",
      "Iteration 651: loss = 69.57712661003897, precision = 0.9604395604395605\n",
      "Iteration 652: loss = 69.5416327157925, precision = 0.9604395604395605\n",
      "Iteration 653: loss = 69.5062158837185, precision = 0.9604395604395605\n",
      "Iteration 654: loss = 69.47087583009, precision = 0.9604395604395605\n",
      "Iteration 655: loss = 69.43561227267844, precision = 0.9604395604395605\n",
      "Iteration 656: loss = 69.40042493074357, precision = 0.9604395604395605\n",
      "Iteration 657: loss = 69.36531352502303, precision = 0.9604395604395605\n",
      "Iteration 658: loss = 69.33027777772236, precision = 0.9604395604395605\n",
      "Iteration 659: loss = 69.29531741250482, precision = 0.9604395604395605\n",
      "Iteration 660: loss = 69.26043215448152, precision = 0.9604395604395605\n",
      "Iteration 661: loss = 69.22562173020148, precision = 0.9604395604395605\n",
      "Iteration 662: loss = 69.19088586764184, precision = 0.9604395604395605\n",
      "Iteration 663: loss = 69.1562242961981, precision = 0.9604395604395605\n",
      "Iteration 664: loss = 69.12163674667455, precision = 0.9604395604395605\n",
      "Iteration 665: loss = 69.08712295127465, precision = 0.9604395604395605\n",
      "Iteration 666: loss = 69.05268264359157, precision = 0.9604395604395605\n",
      "Iteration 667: loss = 69.01831555859873, precision = 0.9604395604395605\n",
      "Iteration 668: loss = 68.98402143264063, precision = 0.9604395604395605\n",
      "Iteration 669: loss = 68.9498000034234, precision = 0.9604395604395605\n",
      "Iteration 670: loss = 68.91565101000579, precision = 0.9604395604395605\n",
      "Iteration 671: loss = 68.88157419278997, precision = 0.9604395604395605\n",
      "Iteration 672: loss = 68.84756929351256, precision = 0.9604395604395605\n",
      "Iteration 673: loss = 68.81363605523575, precision = 0.9604395604395605\n",
      "Iteration 674: loss = 68.77977422233823, precision = 0.9604395604395605\n",
      "Iteration 675: loss = 68.74598354050656, precision = 0.9604395604395605\n",
      "Iteration 676: loss = 68.71226375672646, precision = 0.9604395604395605\n",
      "Iteration 677: loss = 68.67861461927399, precision = 0.9604395604395605\n",
      "Iteration 678: loss = 68.6450358777071, precision = 0.9604395604395605\n",
      "Iteration 679: loss = 68.61152728285714, precision = 0.9604395604395605\n",
      "Iteration 680: loss = 68.57808858682026, precision = 0.9604395604395605\n",
      "Iteration 681: loss = 68.54471954294917, precision = 0.9604395604395605\n",
      "Iteration 682: loss = 68.51141990584478, precision = 0.9604395604395605\n",
      "Iteration 683: loss = 68.47818943134797, precision = 0.9604395604395605\n",
      "Iteration 684: loss = 68.44502787653141, precision = 0.9604395604395605\n",
      "Iteration 685: loss = 68.41193499969147, precision = 0.9604395604395605\n",
      "Iteration 686: loss = 68.3789105603401, precision = 0.9604395604395605\n",
      "Iteration 687: loss = 68.345954319197, precision = 0.9604395604395605\n",
      "Iteration 688: loss = 68.31306603818155, precision = 0.9604395604395605\n",
      "Iteration 689: loss = 68.28024548040503, precision = 0.9604395604395605\n",
      "Iteration 690: loss = 68.24749241016286, precision = 0.9604395604395605\n",
      "Iteration 691: loss = 68.21480659292685, precision = 0.9604395604395605\n",
      "Iteration 692: loss = 68.18218779533748, precision = 0.9604395604395605\n",
      "Iteration 693: loss = 68.14963578519644, precision = 0.9604395604395605\n",
      "Iteration 694: loss = 68.11715033145892, precision = 0.9604395604395605\n",
      "Iteration 695: loss = 68.08473120422624, precision = 0.9604395604395605\n",
      "Iteration 696: loss = 68.05237817473841, precision = 0.9604395604395605\n",
      "Iteration 697: loss = 68.02009101536673, precision = 0.9604395604395605\n",
      "Iteration 698: loss = 67.98786949960657, precision = 0.9604395604395605\n",
      "Iteration 699: loss = 67.95571340207002, precision = 0.9604395604395605\n",
      "Iteration 700: loss = 67.92362249847875, precision = 0.9604395604395605\n",
      "Iteration 701: loss = 67.89159656565693, precision = 0.9604395604395605\n",
      "Iteration 702: loss = 67.85963538152401, precision = 0.9604395604395605\n",
      "Iteration 703: loss = 67.8277387250879, precision = 0.9604395604395605\n",
      "Iteration 704: loss = 67.79590637643781, precision = 0.9604395604395605\n",
      "Iteration 705: loss = 67.76413811673754, precision = 0.9604395604395605\n",
      "Iteration 706: loss = 67.73243372821841, precision = 0.9604395604395605\n",
      "Iteration 707: loss = 67.70079299417264, precision = 0.9604395604395605\n",
      "Iteration 708: loss = 67.66921569894646, precision = 0.9604395604395605\n",
      "Iteration 709: loss = 67.63770162793355, precision = 0.9604395604395605\n",
      "Iteration 710: loss = 67.6062505675683, precision = 0.9604395604395605\n",
      "Iteration 711: loss = 67.57486230531923, precision = 0.9604395604395605\n",
      "Iteration 712: loss = 67.5435366296825, precision = 0.9604395604395605\n",
      "Iteration 713: loss = 67.51227333017536, precision = 0.9604395604395605\n",
      "Iteration 714: loss = 67.4810721973298, precision = 0.9604395604395605\n",
      "Iteration 715: loss = 67.449933022686, precision = 0.9604395604395605\n",
      "Iteration 716: loss = 67.41885559878617, precision = 0.9604395604395605\n",
      "Iteration 717: loss = 67.3878397191682, precision = 0.9604395604395605\n",
      "Iteration 718: loss = 67.35688517835933, precision = 0.9604395604395605\n",
      "Iteration 719: loss = 67.32599177187005, precision = 0.9604395604395605\n",
      "Iteration 720: loss = 67.29515929618799, precision = 0.9604395604395605\n",
      "Iteration 721: loss = 67.26438754877165, precision = 0.9604395604395605\n",
      "Iteration 722: loss = 67.23367632804454, precision = 0.9604395604395605\n",
      "Iteration 723: loss = 67.20302543338909, precision = 0.9604395604395605\n",
      "Iteration 724: loss = 67.17243466514066, precision = 0.9604395604395605\n",
      "Iteration 725: loss = 67.1419038245817, precision = 0.9604395604395605\n",
      "Iteration 726: loss = 67.1114327139358, precision = 0.9604395604395605\n",
      "Iteration 727: loss = 67.08102113636193, precision = 0.9604395604395605\n",
      "Iteration 728: loss = 67.05066889594862, precision = 0.9604395604395605\n",
      "Iteration 729: loss = 67.02037579770827, precision = 0.9604395604395605\n",
      "Iteration 730: loss = 66.9901416475714, precision = 0.9604395604395605\n",
      "Iteration 731: loss = 66.95996625238105, precision = 0.9604395604395605\n",
      "Iteration 732: loss = 66.92984941988715, precision = 0.9604395604395605\n",
      "Iteration 733: loss = 66.89979095874088, precision = 0.9604395604395605\n",
      "Iteration 734: loss = 66.86979067848938, precision = 0.9604395604395605\n",
      "Iteration 735: loss = 66.83984838957, precision = 0.9604395604395605\n",
      "Iteration 736: loss = 66.80996390330495, precision = 0.9604395604395605\n",
      "Iteration 737: loss = 66.78013703189603, precision = 0.9604395604395605\n",
      "Iteration 738: loss = 66.75036758841912, precision = 0.9604395604395605\n",
      "Iteration 739: loss = 66.72065538681889, precision = 0.9604395604395605\n",
      "Iteration 740: loss = 66.69100024190357, precision = 0.9604395604395605\n",
      "Iteration 741: loss = 66.66140196933966, precision = 0.9604395604395605\n",
      "Iteration 742: loss = 66.6318603856468, precision = 0.9604395604395605\n",
      "Iteration 743: loss = 66.60237530819249, precision = 0.9604395604395605\n",
      "Iteration 744: loss = 66.57294655518713, precision = 0.9604395604395605\n",
      "Iteration 745: loss = 66.54357394567876, precision = 0.9604395604395605\n",
      "Iteration 746: loss = 66.51425729954816, precision = 0.9604395604395605\n",
      "Iteration 747: loss = 66.48499643750374, precision = 0.9604395604395605\n",
      "Iteration 748: loss = 66.45579118107668, precision = 0.9604395604395605\n",
      "Iteration 749: loss = 66.42664135261583, precision = 0.9604395604395605\n",
      "Iteration 750: loss = 66.39754677528296, precision = 0.9604395604395605\n",
      "Iteration 751: loss = 66.36850727304787, precision = 0.9604395604395605\n",
      "Iteration 752: loss = 66.33952267068345, precision = 0.9604395604395605\n",
      "Iteration 753: loss = 66.31059279376105, precision = 0.9604395604395605\n",
      "Iteration 754: loss = 66.28171746864558, precision = 0.9604395604395605\n",
      "Iteration 755: loss = 66.25289652249097, precision = 0.9604395604395605\n",
      "Iteration 756: loss = 66.22412978323524, precision = 0.9604395604395605\n",
      "Iteration 757: loss = 66.19541707959604, precision = 0.9604395604395605\n",
      "Iteration 758: loss = 66.16675824106598, precision = 0.9604395604395605\n",
      "Iteration 759: loss = 66.13815309790799, precision = 0.9604395604395605\n",
      "Iteration 760: loss = 66.10960148115089, precision = 0.9604395604395605\n",
      "Iteration 761: loss = 66.08110322258466, precision = 0.9604395604395605\n",
      "Iteration 762: loss = 66.05265815475622, precision = 0.9604395604395605\n",
      "Iteration 763: loss = 66.0242661109648, precision = 0.9604395604395605\n",
      "Iteration 764: loss = 65.99592692525754, precision = 0.9604395604395605\n",
      "Iteration 765: loss = 65.96764043242516, precision = 0.9604395604395605\n",
      "Iteration 766: loss = 65.93940646799751, precision = 0.9604395604395605\n",
      "Iteration 767: loss = 65.9112248682394, precision = 0.9604395604395605\n",
      "Iteration 768: loss = 65.88309547014615, precision = 0.9604395604395605\n",
      "Iteration 769: loss = 65.85501811143935, precision = 0.9604395604395605\n",
      "Iteration 770: loss = 65.82699263056271, precision = 0.9604395604395605\n",
      "Iteration 771: loss = 65.79901886667784, precision = 0.9604395604395605\n",
      "Iteration 772: loss = 65.77109665965997, precision = 0.9604395604395605\n",
      "Iteration 773: loss = 65.74322585009396, precision = 0.9604395604395605\n",
      "Iteration 774: loss = 65.71540627927004, precision = 0.9604395604395605\n",
      "Iteration 775: loss = 65.68763778917986, precision = 0.9604395604395605\n",
      "Iteration 776: loss = 65.65992022251231, precision = 0.9604395604395605\n",
      "Iteration 777: loss = 65.63225342264965, precision = 0.9604395604395605\n",
      "Iteration 778: loss = 65.60463723366333, precision = 0.9604395604395605\n",
      "Iteration 779: loss = 65.57707150031015, precision = 0.9604395604395605\n",
      "Iteration 780: loss = 65.5495560680283, precision = 0.9604395604395605\n",
      "Iteration 781: loss = 65.5220907829334, precision = 0.9604395604395605\n",
      "Iteration 782: loss = 65.49467549181468, precision = 0.9604395604395605\n",
      "Iteration 783: loss = 65.46731004213105, precision = 0.9604395604395605\n",
      "Iteration 784: loss = 65.4399942820073, precision = 0.9604395604395605\n",
      "Iteration 785: loss = 65.41272806023036, precision = 0.9604395604395605\n",
      "Iteration 786: loss = 65.38551122624546, precision = 0.9604395604395605\n",
      "Iteration 787: loss = 65.35834363015232, precision = 0.9604395604395605\n",
      "Iteration 788: loss = 65.33122512270157, precision = 0.9626373626373627\n",
      "Iteration 789: loss = 65.30415555529092, precision = 0.9626373626373627\n",
      "Iteration 790: loss = 65.27713477996159, precision = 0.9626373626373627\n",
      "Iteration 791: loss = 65.25016264939464, precision = 0.9626373626373627\n",
      "Iteration 792: loss = 65.22323901690723, precision = 0.9626373626373627\n",
      "Iteration 793: loss = 65.1963637364492, precision = 0.9626373626373627\n",
      "Iteration 794: loss = 65.16953666259948, precision = 0.9626373626373627\n",
      "Iteration 795: loss = 65.14275765056236, precision = 0.9626373626373627\n",
      "Iteration 796: loss = 65.1160265561642, precision = 0.9626373626373627\n",
      "Iteration 797: loss = 65.08934323584974, precision = 0.9626373626373627\n",
      "Iteration 798: loss = 65.06270754667884, precision = 0.9626373626373627\n",
      "Iteration 799: loss = 65.0361193463228, precision = 0.9626373626373627\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(loss_func, starting_point, learning_rate = 0.001, num_steps = 800, precision=0.00001):\n",
    "    cur_point = starting_point\n",
    "    for i in range(num_steps):\n",
    "        grad = loss_func.gradient(cur_point)\n",
    "        print(\"Iteration {}: loss = {}, precision = {}\".format(i, loss_func.loss(cur_point), loss_func.precision(cur_point)))\n",
    "        cur_point = cur_point - learning_rate * grad\n",
    "        if np.linalg.norm(grad) < precision:\n",
    "            break\n",
    "    return cur_point\n",
    "\n",
    "optimal = gradient_descent(loss_func, np.zeros(X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test precision: 0.9912280701754386\n"
     ]
    }
   ],
   "source": [
    "# test on X_test\n",
    "\n",
    "loss_func = LossFunction(X_test, y_test)\n",
    "print(\"Test precision: {}\".format(loss_func.precision(optimal)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
